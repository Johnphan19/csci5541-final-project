{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6984d636",
   "metadata": {},
   "source": [
    "# Model Evaluation Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235a11fe",
   "metadata": {},
   "source": [
    "## 1. Setup: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7291f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets evaluate transformers accelerate bitsandbytes torch pandas matplotlib seaborn tqdm ipywidgets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b315a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import importlib\n",
    "\n",
    "# Import refactored components\n",
    "from config_manager import ConfigManager\n",
    "from evaluation_framework import EvaluationFramework\n",
    "from results_visualizer import ResultsVisualizer\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Initialize ConfigManager to get base model name and defaults\n",
    "config_manager = ConfigManager()\n",
    "base_config = config_manager.get_base_config()\n",
    "BASE_MODEL_NAME = base_config['MODEL_NAME']\n",
    "\n",
    "# List of paths to the *final saved model directories* generated by training runs\n",
    "# These should be relative to the project root or absolute paths\n",
    "MODEL_PATHS_TO_EVALUATE = [\n",
    "    # --- Add paths to your trained model directories below ---\n",
    "    # Example format:\n",
    "    # \"training_outputs/finetune_deepseek_original_lila/final_model\",\n",
    "    # \"training_outputs/gradient_ascent_deepseek_lila/final_model\",\n",
    "\n",
    "    # Add the actual paths from your training runs here:\n",
    "    \"training_outputs/finetune_DeepSeek-R1-Distill-Qwen-1.5B_original_lila_MATH_algebra_crowdsourced/final_model\",\n",
    "    \"training_outputs/finetune_DeepSeek-R1-Distill-Qwen-1.5B_scrambled_lila_MATH_algebra_crowdsourced/final_model\",\n",
    "    \"training_outputs/finetune_DeepSeek-R1-Distill-Qwen-1.5B_val_modified_lila_MATH_algebra_crowdsourced/final_model\",\n",
    "    \"training_outputs/finetune_DeepSeek-R1-Distill-Qwen-1.5B_length_val_modified_lila_MATH_algebra_crowdsourced/final_model\",\n",
    "    \"training_outputs/gradient_ascent_DeepSeek-R1-Distill-Qwen-1.5B/final_model\",\n",
    "    \"training_outputs/gradient_ascent_reduced_EOS_DeepSeek-R1-Distill-Qwen-1.5B/final_model\",\n",
    "]\n",
    "\n",
    "# Combine base model name with the paths\n",
    "all_model_identifiers = [BASE_MODEL_NAME] + MODEL_PATHS_TO_EVALUATE\n",
    "\n",
    "# Evaluation parameters (can override defaults from config.py)\n",
    "NUM_EXAMPLES_PER_SPLIT = None # Set to an integer (e.g., 100) for faster testing, None for full evaluation\n",
    "INFERENCE_BATCH_SIZE = base_config.get('DEFAULT_INFERENCE_BATCH_SIZE', 4) # Use default from config or set here\n",
    "MAX_NEW_TOKENS = base_config.get('MAX_NEW_TOKENS_MATH', 1024)\n",
    "INFERENCE_STYLE = 'think' # Or 'no_think', should match how models were trained/expect prompts\n",
    "\n",
    "# Output file for results\n",
    "RESULTS_FILE = \"evaluation_results_aggregate.json\"\n",
    "DETAIL_RESULTS_FILE = \"evaluation_results_detailed.json\"\n",
    "\n",
    "print(f\"Base model: {BASE_MODEL_NAME}\")\n",
    "print(f\"Models to evaluate: {all_model_identifiers}\")\n",
    "print(f\"Dataset: {base_config['BASE_DATASET_NAME']} ({base_config['BASE_DATASET_CONFIG']})\")\n",
    "print(f\"Num examples per split: {'Full' if NUM_EXAMPLES_PER_SPLIT is None else NUM_EXAMPLES_PER_SPLIT}\")\n",
    "print(f\"Inference Batch Size: {INFERENCE_BATCH_SIZE}\")\n",
    "print(f\"Max new tokens: {MAX_NEW_TOKENS}\")\n",
    "print(f\"Inference Style: {INFERENCE_STYLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857603f8",
   "metadata": {},
   "source": [
    "## 2. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c584d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the evaluation framework\n",
    "eval_framework = EvaluationFramework(config_manager)\n",
    "\n",
    "# Run the evaluation for all specified models\n",
    "aggregate_results, detailed_results = eval_framework.run_evaluation(\n",
    "    model_identifiers=all_model_identifiers,\n",
    "    num_examples=NUM_EXAMPLES_PER_SPLIT,\n",
    "    inference_batch_size=INFERENCE_BATCH_SIZE,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    inference_style=INFERENCE_STYLE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5959bc",
   "metadata": {},
   "source": [
    "## 3. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c02022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the aggregated results\n",
    "if aggregate_results:\n",
    "    eval_framework.save_results(aggregate_results, RESULTS_FILE)\n",
    "else:\n",
    "    print(\"Skipping saving aggregate results as none were generated.\")\n",
    "\n",
    "# Save the detailed results (optional, can be large)\n",
    "if detailed_results:\n",
    "    eval_framework.save_results(detailed_results, DETAIL_RESULTS_FILE)\n",
    "else:\n",
    "    print(\"Skipping saving detailed results as none were generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaeb01b",
   "metadata": {},
   "source": [
    "## 4. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0a9bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the aggregate results for visualization\n",
    "loaded_agg_results = eval_framework.load_results(RESULTS_FILE)\n",
    "\n",
    "if loaded_agg_results:\n",
    "    # Initialize the visualizer with the loaded data\n",
    "    visualizer = ResultsVisualizer(loaded_agg_results)\n",
    "\n",
    "    # Generate plots\n",
    "    plot_title = f'Model Performance on LILA ({base_config[\"BASE_DATASET_CONFIG\"]})'\n",
    "    visualizer.plot_comparison_bar_chart(title=plot_title)\n",
    "\n",
    "    # Optional: Plot individual splits\n",
    "    # visualizer.plot_split_bar_chart('validation', title=f'Validation Accuracy on LILA ({base_config[\"BASE_DATASET_CONFIG\"]})')\n",
    "    # visualizer.plot_split_bar_chart('test', title=f'Test Accuracy on LILA ({base_config[\"BASE_DATASET_CONFIG\"]})', palette='Greens_d')\n",
    "else:\n",
    "    print(\"Cannot visualize results as aggregate results file could not be loaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
