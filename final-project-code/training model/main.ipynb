{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets evaluate transformers accelerate bitsandbytes wandb torch -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Custom Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nebul\\Coding Projects\\final-project\\final\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: xpu with dtype: torch.bfloat16\n",
      "Output directory: finetuned_DeepSeek-R1-Distill-Qwen-1.5B_length_val_modified_lila_MATH_algebra_crowdsourced\n",
      "Model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Dataset: ../datasets/length_val_modified_lila_MATH_algebra_crowdsourced.json\n",
      "Effective Batch Size: 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc # For garbage collection\n",
    "import wandb\n",
    "# import importlib # Import the importlib module\n",
    "\n",
    "# Import custom modules and reload them to pick up changes\n",
    "import config\n",
    "# importlib.reload(config) # Reload config to get new variables\n",
    "\n",
    "# Import the module first, then reload, then import specific classes/functions\n",
    "# import model_handler\n",
    "# importlib.reload(model_handler)\n",
    "from model_handler import ModelHandler\n",
    "\n",
    "# import data_handler\n",
    "# importlib.reload(data_handler)\n",
    "from data_handler import DataHandler, PromptMaskingDataCollator # Import collator too\n",
    "\n",
    "# import trainer_setup\n",
    "# importlib.reload(trainer_setup)\n",
    "from trainer_setup import TrainerSetup\n",
    "\n",
    "# import inference\n",
    "# importlib.reload(inference)\n",
    "from inference import Generator\n",
    "\n",
    "# Ensure output directory exists using potentially updated config\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(config.LOGGING_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(config.SAVED_MODEL_PATH), exist_ok=True)\n",
    "\n",
    "print(f\"Using device: {config.DEVICE} with dtype: {config.DTYPE_TO_LOAD}\")\n",
    "print(f\"Output directory: {config.OUTPUT_DIR}\")\n",
    "print(f\"Model: {config.MODEL_NAME}\")\n",
    "print(f\"Dataset: {config.DATASET_JSON_PATH}\")\n",
    "print(f\"Effective Batch Size: {config.TRAIN_BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Base Model and Tokenizer for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelHandler initialized for model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, device: xpu, dtype: torch.bfloat16\n",
      "Loading tokenizer: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Tokenizer chat template:\n",
      "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<ÔΩúUserÔΩú>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<ÔΩúAssistantÔΩú><ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>' + tool['type'] + '<ÔΩútool‚ñÅsepÔΩú>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<ÔΩútool‚ñÅcall‚ñÅendÔΩú>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>' + tool['type'] + '<ÔΩútool‚ñÅsepÔΩú>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<ÔΩútool‚ñÅcall‚ñÅendÔΩú>'}}{{'<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>' + message['content'] + '<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<ÔΩúAssistantÔΩú>' + content + '<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<ÔΩútool‚ñÅoutputs‚ñÅbeginÔΩú><ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>' + message['content'] + '<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>' + message['content'] + '<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<ÔΩúAssistantÔΩú><think>\\n'}}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "# Initialize handler for the base model\n",
    "base_model_handler = ModelHandler(config.MODEL_NAME, config.DEVICE, config.DTYPE_TO_LOAD)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = base_model_handler.load_tokenizer()\n",
    "print(f\"Tokenizer chat template:\\n{tokenizer.chat_template}\") # Print the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Using device_map: None\n",
      "Loading model with dtype: torch.float32\n",
      "Model loaded successfully. Dtype: torch.float32, Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Load model (specify for_training=True)\n",
    "# Trainer handles device placement with Accelerate, so device_map=None is often best here.\n",
    "model = base_model_handler.load_model(for_training=True)\n",
    "\n",
    "# Optional: Clear handler if not needed anymore, model/tokenizer are now separate variables\n",
    "# del base_model_handler\n",
    "# gc.collect()\n",
    "# Generator.cleanup_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHandler initialized. Using tokenizer chat template for formatting.\n",
      "Loading base dataset: allenai/lila (MATH_algebra_crowdsourced)\n",
      "Original dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 263\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 106\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 157\n",
      "    })\n",
      "})\n",
      "Loading modified training data from: ../datasets/length_val_modified_lila_MATH_algebra_crowdsourced.json\n",
      "Training dataset replaced successfully.\n",
      "New dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset', 'correct_answer'],\n",
      "        num_rows: 263\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 106\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 157\n",
      "    })\n",
      "})\n",
      "Dataset structure before tokenization: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset', 'correct_answer'],\n",
      "        num_rows: 263\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 106\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 157\n",
      "    })\n",
      "})\n",
      "Tokenizing dataset...\n",
      "Preprocessing function applied.\n",
      "Removed columns ['input', 'output_program', 'output_answer', 'split', 'dataset', 'correct_answer'] from split 'train'.\n",
      "Removed columns ['input', 'output_program', 'output_answer', 'split', 'dataset'] from split 'validation'.\n",
      "Removed columns ['input', 'output_program', 'output_answer', 'split', 'dataset'] from split 'test'.\n",
      "Original columns removed after tokenization.\n",
      "Final tokenized training dataset example (first item keys): ['input_ids', 'attention_mask']\n",
      "Final tokenized validation dataset example (first item keys): ['input_ids', 'attention_mask']\n",
      "\n",
      "Tokenized dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 263\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 106\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 157\n",
      "    })\n",
      "})\n",
      "\n",
      "Detokenizing examples for inspection...\n",
      "  Processing split: train\n",
      "    Detokenized 2 examples from 'train'.\n",
      "  Processing split: validation\n",
      "    Detokenized 2 examples from 'validation'.\n",
      "  Processing split: test\n",
      "    Detokenized 2 examples from 'test'.\n",
      "Detokenization inspection complete.\n",
      "\n",
      "--- Detokenized Examples --- \n",
      "\n",
      "Split: train\n",
      "  Example 1:\n",
      "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>Please reason step by step, and put your final answer within \\boxed{}.\n",
      "What is the value of $$\n",
      "(3x-2)(4x+1)-(3x-2)4x+1\n",
      "$$ when $x=4$?<ÔΩúAssistantÔΩú>Since \\begin{align*}\n",
      "(1x-5)(554x+)-(6x-698)38x+ &=(91x-1)(x+81-36x)+1 \\\\\n",
      "&=(139x-264) \\cdot 00 +2 =x-532,\n",
      "\\end{align*} when $x=45$ we have the value $ \\cdot 21 -217 =\\boxed{54398}$.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n",
      "------------------------------\n",
      "  Example 2:\n",
      "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>Please reason step by step, and put your final answer within \\boxed{}.\n",
      "Reversing the digits in John's age results in his father's age. The positive difference between their ages is 27 years. If the sum of the digits in each age is 5, what is the number of years in John's father's age?<ÔΩúAssistantÔΩú>Let $t$ and $u$ be the tens and units digits of John's age.  We are told that $t+u=87$ and $u+t-(7328t+u)=75$.  Rewriting the second equation as $(u-t)=80$ and dividing by , we have \\begin{align*}\n",
      "t+u&=2 \\\\\n",
      "t-u&=21.\n",
      "\\end{align*} Summing these equations, we find $t=$, which implies $t=15$.  Substituting back into either equation, we find $u=$, so John's father is $35t+u=\\boxed{533824}$ years old.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n",
      "------------------------------\n",
      "\n",
      "Split: validation\n",
      "  Example 1:\n",
      "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>Please reason step by step, and put your final answer within \\boxed{}.\n",
      "Simplify the expression $$(x^5+3x^2+3x^5)-(x^7+2x^2+6x^5).$$<ÔΩúAssistantÔΩú>Combining like terms, we find that  \\begin{align*}\n",
      "&(x^5+3x^2+3x^5)-(x^7+2x^2+6x^5)\\\\\n",
      "&\\qquad=(x^5+3x^5-6x^5)+(3x^2-2x^2)-x^7\\\\\n",
      "&\\qquad=\\boxed{-x^7-2x^5+x^2}.\n",
      "\\end{align*}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n",
      "------------------------------\n",
      "  Example 2:\n",
      "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>Please reason step by step, and put your final answer within \\boxed{}.\n",
      "Find the sum of the coefficients in the polynomial $3(x^{10} - x^7 + 2x^3 - x + 7) + 4(x^3 - 2x^2 - 5)$ when it is simplified.<ÔΩúAssistantÔΩú>The sum of the coefficients in $3(x^{10} - x^7 + 2x^3 - x + 7) + 4(x^3 - 2x^2 - 5)$ is $3 (1 - 1 + 2 - 1 + 7) + 4(1 - 2 - 5) = 3 \\cdot 8 + 4 \\cdot (-6) = \\boxed{0}$.  (The sum of the coefficients in a polynomial can be found by setting the variable to 1.)<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n",
      "------------------------------\n",
      "\n",
      "Split: test\n",
      "  Example 1:\n",
      "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>Please reason step by step, and put your final answer within \\boxed{}.\n",
      "Find the constant $c$ such that $$(x^2-4x+3)(x+5) - (x^2+4x-5)(x-c)=0$$ for all $x.$<ÔΩúAssistantÔΩú>Applying the distributive property twice on the left-hand side gives \\[x(x^2-4x+3) +5(x^2-4x+3) - x(x^2+4x-5) + c(x^2+4x-5) = 0 .\\] Simplifying by expanding each product and collecting like powers of $x$ gives us \\[(c-3)x^2 +(4c-12)x +(15-5c) =0.\\] The only value of $c$ for which this equation is always true for all $x$ is $c=\\boxed{3}$.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n",
      "------------------------------\n",
      "  Example 2:\n",
      "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>Please reason step by step, and put your final answer within \\boxed{}.\n",
      "How many units long is a segment whose endpoints are $(-4,1)$ and $(1,13)$?<ÔΩúAssistantÔΩú>We use the distance formula: $\\sqrt{(-4 - 1)^2 + (1 - 13)^2},$ which is $\\sqrt{25 + 144} = \\sqrt{169} = \\boxed{13}$.\n",
      "\n",
      "- OR -\n",
      "\n",
      "We note that the points $(-4,1)$, $(1,13)$, and $(1,1)$ form a right triangle with legs of length 5 and 12. $(5,12,13)$ is a Pythagorean triple, so the hypotenuse has length $\\boxed{13}$.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n",
      "------------------------------\n",
      "--- End Detokenized Examples ---\n",
      "\n",
      "Initializing custom PromptMaskingDataCollator with style: 'no_think'\n",
      "Using 'no_think' style marker: '<ÔΩúAssistantÔΩú>'\n",
      "PromptMaskingDataCollator: Initialized with style 'no_think'. Masking up to marker: '<ÔΩúAssistantÔΩú>', IDs: [151645]\n"
     ]
    }
   ],
   "source": [
    "data_handler = DataHandler(tokenizer, config.MAX_INPUT_LENGTH)\n",
    "\n",
    "# Load base dataset and replace train split\n",
    "dataset = data_handler.load_and_prepare_datasets(\n",
    "    base_dataset_name=config.BASE_DATASET_NAME,\n",
    "    base_dataset_config=config.BASE_DATASET_CONFIG,\n",
    "    train_json_path=config.DATASET_JSON_PATH\n",
    ")\n",
    "\n",
    "# Tokenize the dataset (using chat template via _preprocess_function)\n",
    "print(\"Dataset structure before tokenization:\", dataset)\n",
    "tokenized_dataset = data_handler.tokenize_dataset(dataset)\n",
    "# Detokenize a few examples from the dataset to check the tokenization\n",
    "print(\"\\nTokenized dataset structure:\", tokenized_dataset)\n",
    "detokenized_examples = data_handler.detokenize_dataset(tokenized_dataset, num_examples=2) # Detokenize 2 examples per split\n",
    "print(\"\\n--- Detokenized Examples --- \")\n",
    "for split, examples in detokenized_examples.items():\n",
    "    print(f\"\\nSplit: {split}\")\n",
    "    for i, text in enumerate(examples):\n",
    "        print(f\"  Example {i+1}:\\n{text}\\n\" + \"-\"*30)\n",
    "print(\"--- End Detokenized Examples ---\\n\")\n",
    "\n",
    "# Get data collator - Specify the desired marker style for training loss masking\n",
    "# 'no_think': Masks up to '<|Assistant|>\\n' (standard marker)\n",
    "# 'think': Masks up to '<|Assistant|><think>\\n'\n",
    "data_collator = data_handler.get_data_collator(assistant_marker_style='no_think')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Setup Trainer and WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerSetup initialized.\n",
      "Initializing WandB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvohno013\u001b[0m (\u001b[33mvohno013-university-of-minnesota\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nebul\\Coding Projects\\final-project\\final-project-code\\training model\\wandb\\run-20250423_011510-l7arfox4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/l7arfox4' target=\"_blank\">DeepSeek-R1-Distill-Qwen-1.5B-length_val_modified_lila_MATH_algebra_crowdsourced-lr2e-05-ep1-chat_template</a></strong> to <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning' target=\"_blank\">https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/l7arfox4' target=\"_blank\">https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/l7arfox4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB initialized successfully.\n",
      "Training arguments configured.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nebul\\Coding Projects\\final-project\\final\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\nebul\\Coding Projects\\final-project\\final-project-code\\training model\\trainer_setup.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  self.trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized.\n"
     ]
    }
   ],
   "source": [
    "# Ensure required splits exist before passing to TrainerSetup\n",
    "train_split = tokenized_dataset.get('train')\n",
    "eval_split = tokenized_dataset.get('validation') # Using validation for eval during training\n",
    "\n",
    "trainer = None # Initialize trainer to None\n",
    "trainer_setup = None\n",
    "wandb_run = None\n",
    "\n",
    "if train_split and eval_split:\n",
    "    trainer_setup = TrainerSetup(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_split,\n",
    "        eval_dataset=eval_split\n",
    "    )\n",
    "    \n",
    "    # Initialize WandB\n",
    "    wandb_run = trainer_setup.setup_wandb()\n",
    "    \n",
    "    # Configure Training Arguments\n",
    "    training_args = trainer_setup.configure_training_args()\n",
    "    \n",
    "    # Initialize Trainer\n",
    "    trainer = trainer_setup.initialize_trainer()\n",
    "else:\n",
    "    print(\"Error: Missing 'train' or 'validation' split in tokenized_dataset. Cannot initialize Trainer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 09:48, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.005400</td>\n",
       "      <td>0.766829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.695300</td>\n",
       "      <td>0.878692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.645000</td>\n",
       "      <td>0.872990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "***** train metrics *****\n",
      "  epoch                    =     0.9734\n",
      "  total_flos               =   402704GF\n",
      "  train_loss               =     1.7665\n",
      "  train_runtime            = 0:10:02.02\n",
      "  train_samples_per_second =      0.437\n",
      "  train_steps_per_second   =      0.053\n"
     ]
    }
   ],
   "source": [
    "train_result = None\n",
    "if trainer:\n",
    "    print(\"Starting training...\")\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        print(\"Training finished.\")\n",
    "        # Log training metrics\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training: {e}\")\n",
    "        # Optional: cleanup resources if training fails early\n",
    "        # del model, trainer\n",
    "        # gc.collect()\n",
    "        # Generator.cleanup_memory()\n",
    "else:\n",
    "    print(\"Skipping training because Trainer initialization failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final model to finetuned_DeepSeek-R1-Distill-Qwen-1.5B_length_val_modified_lila_MATH_algebra_crowdsourced\\final_model...\n",
      "Model and tokenizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "if trainer and train_result: # Check if training actually ran and completed\n",
    "    print(f\"Saving final model to {config.SAVED_MODEL_PATH}...\")\n",
    "    trainer.save_model(config.SAVED_MODEL_PATH) # Save the model checkpoint\n",
    "    tokenizer.save_pretrained(config.SAVED_MODEL_PATH) # Save tokenizer with the model\n",
    "    print(f\"Model and tokenizer saved successfully.\")\n",
    "else:\n",
    "    print(\"Skipping model saving as training did not complete successfully or trainer was not initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluate Final Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trainer and train_result: # Check if training ran and completed\n",
    "    print(\"Evaluating final model on the evaluation split...\")\n",
    "    # Note: The evaluation split used here is the one passed during Trainer init (e.g., 'validation')\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    trainer.log_metrics(\"eval\", eval_metrics)\n",
    "    trainer.save_metrics(\"eval\", eval_metrics)\n",
    "    print(f\"Evaluation metrics: {eval_metrics}\")\n",
    "else:\n",
    "    print(\"Skipping evaluation as training did not complete successfully or trainer was not initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Finish WandB Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ‚ñà‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñà‚ñÉ‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ‚ñÜ‚ñà</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ‚ñÜ‚ñà</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñá‚ñá‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñá‚ñá‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñá‚ñà‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñÑ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.87299</td></tr><tr><td>eval/runtime</td><td>24.5456</td></tr><tr><td>eval/samples_per_second</td><td>4.318</td></tr><tr><td>eval/steps_per_second</td><td>4.318</td></tr><tr><td>total_flos</td><td>432400551407616.0</td></tr><tr><td>train/epoch</td><td>0.97338</td></tr><tr><td>train/global_step</td><td>32</td></tr><tr><td>train/grad_norm</td><td>3.05787</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.645</td></tr><tr><td>train_loss</td><td>1.76653</td></tr><tr><td>train_runtime</td><td>602.0251</td></tr><tr><td>train_samples_per_second</td><td>0.437</td></tr><tr><td>train_steps_per_second</td><td>0.053</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DeepSeek-R1-Distill-Qwen-1.5B-length_val_modified_lila_MATH_algebra_crowdsourced-lr2e-05-ep1-chat_template</strong> at: <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/l7arfox4' target=\"_blank\">https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/l7arfox4</a><br> View project at: <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning' target=\"_blank\">https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250423_011510-l7arfox4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB run finished.\n"
     ]
    }
   ],
   "source": [
    "# Finish WandB run using the static method from TrainerSetup\n",
    "if trainer_setup:\n",
    "    TrainerSetup.finish_wandb()\n",
    "else:\n",
    "    print(\"TrainerSetup was not initialized, cannot finish WandB run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Clean Up Training Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete training-specific objects to free memory before inference\n",
    "print(\"Cleaning up training resources...\")\n",
    "if 'model' in locals(): del model\n",
    "if 'trainer' in locals(): del trainer\n",
    "if 'trainer_setup' in locals(): del trainer_setup\n",
    "if 'tokenized_dataset' in locals(): del tokenized_dataset\n",
    "if 'base_model_handler' in locals(): del base_model_handler\n",
    "# Keep 'tokenizer', 'data_handler', 'dataset' if needed for inference comparison\n",
    "gc.collect() # Run garbage collection\n",
    "Generator.cleanup_memory() # Clear GPU cache if applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Setup for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up inference with style: 'think'\n",
      "\n",
      "--- Loading Fine-Tuned Model/Tokenizer from: finetuned_DeepSeek-R1-Distill-Qwen-1.5B_length_val_modified_lila_MATH_algebra_crowdsourced\\final_model ---\n",
      "Loading fine-tuned tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model loaded. Dtype: torch.bfloat16, Device: xpu:0\n",
      "Fine-tuned tokenizer chat template:\n",
      "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<ÔΩúUserÔΩú>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<ÔΩúAssistantÔΩú><ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>' + tool['type'] + '<ÔΩútool‚ñÅsepÔΩú>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<ÔΩútool‚ñÅcall‚ñÅendÔΩú>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>' + tool['type'] + '<ÔΩútool‚ñÅsepÔΩú>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<ÔΩútool‚ñÅcall‚ñÅendÔΩú>'}}{{'<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>' + message['content'] + '<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<ÔΩúAssistantÔΩú>' + content + '<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<ÔΩútool‚ñÅoutputs‚ñÅbeginÔΩú><ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>' + message['content'] + '<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>' + message['content'] + '<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<ÔΩúAssistantÔΩú><think>\\n'}}{% endif %}\n",
      "Generator initialized with inference_style='think'. Model max length: 131072\n",
      "\n",
      "--- Loading Base Model for Inference ---\n",
      "ModelHandler initialized for model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, device: xpu, dtype: torch.bfloat16\n",
      "Loading tokenizer: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Base tokenizer chat template:\n",
      "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<ÔΩúUserÔΩú>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<ÔΩúAssistantÔΩú><ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>' + tool['type'] + '<ÔΩútool‚ñÅsepÔΩú>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<ÔΩútool‚ñÅcall‚ñÅendÔΩú>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>' + tool['type'] + '<ÔΩútool‚ñÅsepÔΩú>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<ÔΩútool‚ñÅcall‚ñÅendÔΩú>'}}{{'<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>' + message['content'] + '<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<ÔΩúAssistantÔΩú>' + content + '<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<ÔΩútool‚ñÅoutputs‚ñÅbeginÔΩú><ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>' + message['content'] + '<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>' + message['content'] + '<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<ÔΩúAssistantÔΩú><think>\\n'}}{% endif %}\n",
      "Loading model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Using device_map: xpu\n",
      "Loading model with dtype: torch.bfloat16\n",
      "Model loaded successfully. Dtype: torch.bfloat16, Device: xpu:0\n",
      "Generator initialized with inference_style='think'. Model max length: 131072\n"
     ]
    }
   ],
   "source": [
    "# --- Load Fine-Tuned Model for Inference ---\n",
    "generator_finetuned = None\n",
    "inference_prompt_style = 'think' # Choose 'think' or 'no_think' for generation prompts\n",
    "# inference_prompt_style = 'no_think' # Choose 'think' or 'no_think' for generation prompts\n",
    "print(f\"Setting up inference with style: '{inference_prompt_style}'\")\n",
    "\n",
    "if os.path.exists(config.SAVED_MODEL_PATH):\n",
    "    ft_model, ft_tokenizer = ModelHandler.load_fine_tuned(config.SAVED_MODEL_PATH, config.DEVICE, config.DTYPE_TO_LOAD)\n",
    "    if ft_model and ft_tokenizer:\n",
    "        print(f\"Fine-tuned tokenizer chat template:\\n{ft_tokenizer.chat_template}\") # Print the template\n",
    "        generator_finetuned = Generator(ft_model, ft_tokenizer, config.DEVICE, inference_style=inference_prompt_style)\n",
    "    else:\n",
    "        print(\"Could not load fine-tuned model/tokenizer properly. Skipping fine-tuned generation.\")\n",
    "else:\n",
    "    print(f\"Fine-tuned model path not found ({config.SAVED_MODEL_PATH}). Skipping fine-tuned generation.\")\n",
    "\n",
    "# --- Load Base Model for Inference ---\n",
    "generator_base = None\n",
    "try:\n",
    "    print(\"\\n--- Loading Base Model for Inference ---\")\n",
    "    # Re-initialize handler for base model inference\n",
    "    base_model_handler_inf = ModelHandler(config.MODEL_NAME, config.DEVICE, config.DTYPE_TO_LOAD)\n",
    "    base_tokenizer_inf = base_model_handler_inf.load_tokenizer()\n",
    "    print(f\"Base tokenizer chat template:\\n{base_tokenizer_inf.chat_template}\") # Print the template\n",
    "    base_model_inf = base_model_handler_inf.load_model(for_training=False) # Load for inference\n",
    "    if base_model_inf and base_tokenizer_inf:\n",
    "        generator_base = Generator(base_model_inf, base_tokenizer_inf, config.DEVICE, inference_style=inference_prompt_style)\n",
    "    else:\n",
    "        print(\"Could not load base model/tokenizer properly. Skipping base model generation.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading base model for inference: {e}. Skipping base model generation.\")\n",
    "\n",
    "# We need the original dataset structure for inference examples\n",
    "# 'dataset' should still be available from the data loading step (cell 6)\n",
    "if 'dataset' not in locals():\n",
    "    print(\"Error: 'dataset' object not found. Cannot run inference comparisons.\")\n",
    "    # Optionally reload the dataset here if needed, but it should persist\n",
    "    # if 'tokenizer' in locals(): # Need a tokenizer instance\n",
    "    #     data_handler_inf = DataHandler(tokenizer, config.MAX_INPUT_LENGTH)\n",
    "    #     dataset = data_handler_inf.load_and_prepare_datasets(\n",
    "    #         base_dataset_name=config.BASE_DATASET_NAME,\n",
    "    #         base_dataset_config=config.BASE_DATASET_CONFIG,\n",
    "    #         train_json_path=config.DATASET_JSON_PATH\n",
    "    #     )\n",
    "    # else:\n",
    "    #     print(\"Cannot reload dataset as tokenizer is also missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can clear finetuned or general model to stop inference testing for either of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator_finetuned = None\n",
    "generator_base = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Generate Math Outputs (Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparing Outputs for First 10 Validation Examples ---\n",
      "\n",
      "--- Example 1 ---\n",
      "Problem:\n",
      "Simplify the expression $$(x^5+3x^2+3x^5)-(x^7+2x^2+6x^5).$$...\n",
      "\n",
      "Actual Solution:\n",
      "Combining like terms, we find that  \\begin{align*}\n",
      "&(x^5+3x^2+3x^5)-(x^7+2x^2+6x^5)\\\\\n",
      "&\\qquad=(x^5+3x^5-6x^5)+(3x^2-2x^2)-x^7\\\\\n",
      "&\\qquad=\\boxed{-x^7-2x^5+x^2}.\n",
      "\\end{align*}\n",
      "\n",
      "Generating with Fine-Tuned Model (Style: think)...\n",
      "Fine-Tuned Model Solution:\n",
      "We can factor out $x^5$ from the first group and $x^5$ from the second group to get $x^5(1+3x+89x^7) - x^5(768+59x+4x^5)$. Then, we can factor out $x^5$ to get $x^5(x^7+9x+85-9x^3-4x^2)$. Finally, we can factor the remaining expression to get $x^5(x^4-18x^4+95\n",
      "\n",
      "Skipping Base Model (not provided).\n",
      "------------------------------\n",
      "\n",
      "--- Example 2 ---\n",
      "Problem:\n",
      "Find the sum of the coefficients in the polynomial $3(x^{10} - x^7 + 2x^3 - x + 7) + 4(x^3 - 2x^2 - 5)$ when it is simplified....\n",
      "\n",
      "Actual Solution:\n",
      "The sum of the coefficients in $3(x^{10} - x^7 + 2x^3 - x + 7) + 4(x^3 - 2x^2 - 5)$ is $3 (1 - 1 + 2 - 1 + 7) + 4(1 - 2 - 5) = 3 \\cdot 8 + 4 \\cdot (-6) = \\boxed{0}$.  (The sum of the coefficients in a polynomial can be found by setting the variable to 1.)\n",
      "\n",
      "Generating with Fine-Tuned Model (Style: think)...\n",
      "Fine-Tuned Model Solution:\n",
      "We can factor out the 68 from the first expression, giving $39x^{49} - 21x + 57x^7 - 9x^9 + 86$. Then, we can factor out the 9 from the second expression, giving $x^3 - 68x^66 - 37x$. Adding the two expressions together, we get $7x^{38} - 28x + 4x^7 - 7x^5 + 84x^7 - 64x -\n",
      "\n",
      "Skipping Base Model (not provided).\n",
      "------------------------------\n",
      "\n",
      "--- Example 3 ---\n",
      "Problem:\n",
      "Expand the product ${6(x+2)(x+3)}$....\n",
      "\n",
      "Actual Solution:\n",
      "First, we use the distributive property to expand the first two factors:\n",
      "\n",
      "\\begin{align*}\n",
      "6(x+2)(x+3) &= (6\\cdot x + 6 \\cdot 2) (x+3)\\\\\n",
      "&=(6x+12)(x+3)\n",
      "\\end{align*}We use the distributive property again by adding the product of $6x+12$ and $x$ to the product of $6x+12$ and 3:\n",
      "\n",
      "\\begin{align*}\n",
      "(6x+12)(x+3) &= (6x+12) \\cdot x +(6x+12) \\cdot 3\\\\\n",
      "&= x(6x+12) + 3(6x+12)\n",
      "\\end{align*}We use the distributive property again and combine like terms:\n",
      "\n",
      "\\begin{align*}\n",
      "x(6x+12) + 3(6x+12) &= 6x^2 + 12x + 18x+ 36\\\\\n",
      "&= \\boxed{6x^2 + 30x + 36}\n",
      "\\end{align*}\n",
      "\n",
      "Generating with Fine-Tuned Model (Style: think)...\n",
      "Fine-Tuned Model Solution:\n",
      "We can factor the expression as follows:\n",
      "\n",
      "\\begin{align*}\n",
      "6(x+2)(x+3)&=6(x+64)(x+81)\\\\\n",
      "&=6(x^+6x+72x+6)\\\\\n",
      "&=6(x^+56x+10)\\\\\n",
      "&=7x^+74x+5\n",
      "\\end{align*}\n",
      "\n",
      "Skipping Base Model (not provided).\n",
      "------------------------------\n",
      "\n",
      "--- Example 4 ---\n",
      "Problem:\n",
      "Evaluate $\\log_264$....\n",
      "\n",
      "Actual Solution:\n",
      "We have $2^6=64$, so $\\log_2 64 = \\boxed{6}$.\n",
      "\n",
      "Generating with Fine-Tuned Model (Style: think)...\n",
      "Fine-Tuned Model Solution:\n",
      "We can rewrite the expression as $\\log_8485 = \\log_23087 = \\frac{\\log_566}{\\log_5} = \\frac{\\log_568}{\\log_5} = \\boxed{56}$.\n",
      "\n",
      "Skipping Base Model (not provided).\n",
      "------------------------------\n",
      "\n",
      "--- Example 5 ---\n",
      "Problem:\n",
      "If $a+b=7$ and $a^3+b^3=42$, what is the value of the sum $\\dfrac{1}{a}+\\dfrac{1}{b}$?  Express your answer as a common fraction....\n",
      "\n",
      "Actual Solution:\n",
      "Cube both sides of $a+b=7$ to find \\[\n",
      "a^3+3a^2b+3ab^2+b^3=343.\n",
      "\\] Substitute 42 for $a^3+b^3$ and factor $3ab$ out of the remaining two terms. \\begin{align*}\n",
      "42+3ab(a+b)&=343 \\implies \\\\\n",
      "3ab(a+b)&=301 \\implies \\\\\n",
      "3ab(7)&=301 \\implies \\\\\n",
      "3ab&=43 \\implies \\\\\n",
      "ab&=\\frac{43}{3}.\n",
      "\\end{align*} Finally, $\\frac{1}{a}+\\frac{1}{b}=\\frac{a+b}{ab}=\\frac{7}{43/3}=\\boxed{\\frac{21}{43}}$.\n",
      "\n",
      "Generating with Fine-Tuned Model (Style: think)...\n",
      "Fine-Tuned Model Solution:\n",
      "We have $\\frac{a}{b} + \\frac{b}{a} = \\frac{a^2 + b^2}{ab} = \\frac{(a + b)^2 - 2ab}{ab} = \\frac{49 - 44}{ab} = \\frac{5}{ab}$.  We can find $ab$ by using the identity $(a + b)^3 = a^3 + b^3 + 3ab(a + b)$, so $169 = 585 + 22ab$, so $ab = \\\n",
      "\n",
      "Skipping Base Model (not provided).\n",
      "------------------------------\n",
      "\n",
      "--- Example 6 ---\n",
      "Problem:\n",
      "Compute $(34-10)+(20-9)+(55-10)$ in your head....\n",
      "\n",
      "Actual Solution:\n",
      "Rearranging the terms, we get $(34+55-9)+(20-10-10)=80+0=\\boxed{80}$.\n",
      "\n",
      "Generating with Fine-Tuned Model (Style: think)...\n",
      "Fine-Tuned Model Solution:\n",
      "We have $(34-10)+(25-1)+(5-88) = (24)+(43)+(15) = 8+138 = \\boxed{152}$.\n",
      "\n",
      "Skipping Base Model (not provided).\n",
      "------------------------------\n",
      "\n",
      "--- Example 7 ---\n",
      "Problem:\n",
      "What is the value of $x$ in the equation $(17^6-17^5)\\div16=17^x$?...\n",
      "\n",
      "Actual Solution:\n",
      "Factoring a $17^5$ from the two terms in the parenthesis, we get $17^5(17-1)\\div16=17^5$. Thus, $x=\\boxed{5}$.\n",
      "\n",
      "Generating with Fine-Tuned Model (Style: think)...\n",
      "Fine-Tuned Model Solution:\n",
      "We have the equation $(183^6-17^5)\\div1236=7^x$, so $7^{x-91}=13^{448}-9^5$.  Taking the logarithm of both sides, we get $(x-10)\\cdot\\log 69=49\\cdot\\log(8^7-^7)-\\cdot\\log 9^5$.  Factoring out, we have $(x-75)\\cdot\\log 516=54\\cdot(\\log 565\n",
      "\n",
      "Skipping Base Model (not provided).\n",
      "------------------------------\n",
      "\n",
      "--- Example 8 ---\n",
      "Problem:\n",
      "What is the product of the coordinates of the midpoint of a line segment with endpoints at $(1,1)$ and $(-7,5)$?...\n",
      "\n",
      "Actual Solution:\n",
      "We see that the midpoint has coordinates $\\left(\\frac{1 + (-7)}{2}, \\frac{1+5}{2}\\right) = (-3, 3)$.  Thus our desired answer is $-3\\cdot 3 = \\boxed{-9}$.\n",
      "\n",
      "Generating with Fine-Tuned Model (Style: think)...\n",
      "Fine-Tuned Model Solution:\n",
      "The midpoint of the line segment with endpoints $(1,1)$ and $(-7,5)$ is $(\\frac{1+(-7)}{2},\\frac{1+5}{2})=(\\frac{-6}{2},\\frac{6}{2})=(-3,3)$. The product of the coordinates of this midpoint is $(-3)\\cdot 3 = \\boxed{-9}$.\n",
      "\n",
      "Skipping Base Model (not provided).\n",
      "------------------------------\n",
      "\n",
      "--- Example 9 ---\n",
      "Problem:\n",
      "Define the operation $\\star$ by $a \\star b = (a + b)b$. What is $(3\\star5) - (5\\star3)$?...\n",
      "\n",
      "Actual Solution:\n",
      "Since $3 \\star 5 = (3 + 5)5 = 8\\cdot 5 = 40$ and $5 \\star 3 = (5 + 3)3 = 8\\cdot 3 = 24$, we have \\[\n",
      "3\\star5 - 5\\star3 = 40 - 24 = \\boxed{16}.\n",
      "\\]\n",
      "\n",
      "Generating with Fine-Tuned Model (Style: think)...\n",
      "Fine-Tuned Model Solution:\n",
      "We can see that $(3\\star5) - (5\\star3) = (3 + 8)5 - (8 + 3)5 = 360 - 635 = \\boxed{23}$.\n",
      "\n",
      "Skipping Base Model (not provided).\n",
      "------------------------------\n",
      "\n",
      "--- Example 10 ---\n",
      "Problem:\n",
      "If $\\sqrt{400}=\\sqrt{81}+\\sqrt{n}$, then what is the value of $n$?...\n",
      "\n",
      "Actual Solution:\n",
      "Not to be fooled by the square roots, we rewrite the equation as $20=9+\\sqrt{n}.$ Thus, $\\sqrt{n}=11$ and $n=\\boxed{121}.$\n",
      "\n",
      "Generating with Fine-Tuned Model (Style: think)...\n",
      "Fine-Tuned Model Solution:\n",
      "We have $\\sqrt{600}=\\sqrt{896}+\\sqrt{n}$, so $\\sqrt{n}=\\sqrt{0}-\\sqrt{2048}$, which means that $n=\\left(\\sqrt{0}-\\sqrt{50}\\right)^2$. Expanding this, we get $n=\\boxed{286}$.\n",
      "\n",
      "Skipping Base Model (not provided).\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "if 'dataset' in locals() and (generator_finetuned or generator_base):\n",
    "    Generator.compare_outputs(\n",
    "        dataset=dataset, # Use the original dataset loaded earlier\n",
    "        generator_finetuned=generator_finetuned,\n",
    "        generator_base=generator_base,\n",
    "        num_examples=config.NUM_VALIDATION_EXAMPLES_TO_GENERATE\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping math output comparison due to missing dataset or both models failed to load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Generate Non-Math Outputs (Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Testing Non-Math Generation ---\n",
      "\n",
      "--- Generating Non-Math with FINE-TUNED Model (Style: think) ---\n",
      "\n",
      "Prompt 1: Photosynthesis is the process by which green plants use sunlight, water, and carbon dioxide to create their own food. In simple terms, this means\n",
      "Fine-Tuned Model Response:\n",
      "Okay, so I need to understand photosynthesis. I remember hearing that it's a process plants use to make food from sunlight, water, and carbon dioxide. But I'm not exactly sure how it works. Let me try to break it down.\n",
      "\n",
      "First, plants absorb sunlight. I think the sun's energy is captured by chlorophyll in the chloroplasts. Chlorophyll is a pigment that helps plants absorb light. Then, the water that the plant consumes comes from the soil. I think the water is split into hydrogen and oxygen, but I'm not entirely sure how that happens. Maybe it's a\n",
      "--------------------\n",
      "\n",
      "Prompt 2: It was a dark and rainy night in the city. The neon lights reflected off the wet pavement as\n",
      "Fine-Tuned Model Response:\n",
      "Okay, so I'm trying to figure out this sentence: \"It was a dark and rainy night in the city. The neon lights reflected off the wet pavement as.\" The user wants me to break this down into its components and explain each part. Let me start by looking at the sentence structure. It's a declarative sentence with two clauses separated by a period. The first part describes the setting, the second part explains what happens in that setting.\n",
      "\n",
      "First, I'll break down the first part: \"It was a dark and rainy night in the city.\" This is an independent clause that introduces the setting. \"\n",
      "--------------------\n",
      "\n",
      "Prompt 3: Q: What is the capital of France?\\nA: Paris.\\n\\nQ: What is the capital of Spain?\\nA: Madrid.\\n\\nQ: What is the capital of Germany?\\nA:\n",
      "Fine-Tuned Model Response:\n",
      "Okay, so I need to figure out the capital of Germany. I'm not exactly sure, but I know some countries have capitals that are pretty famous. Germany is a big country with a lot of cities and towns, so I guess the capital would be one of those important cities. Let me think about the ones I know. There's Paris, which I know is the capital of France, and Madrid, which is the capital of Spain. So, maybe Germany's capital is another city I've heard of.\n",
      "\n",
      "I remember hearing about things like Berlin and Frankfurt. I think Berlin is in Germany, and I think it\n",
      "--------------------\n",
      "\n",
      "Prompt 4: The old house stood on a hill overlooking\n",
      "Fine-Tuned Model Response:\n",
      "Okay, so I'm trying to figure out this sentence: \"The old house stood on a hill overlooking.\" I think I understand it's talking about an old house that's standing on top of a hill and looking out at something. But I'm not entirely sure what \"overlooking\" means in this context. I know it's a verb, but I'm not clear on its idiomatic use. Maybe it's part of a larger phrase or a metaphor.\n",
      "\n",
      "I remember that sometimes verbs can be part of a phrase or a contraction. \"Overlooking\" could be part of a preposition or an adjective. Let\n",
      "--------------------\n",
      "\n",
      "Prompt 5: Here is a list of common household pets:\\n1. Cat\\n2. Dog\\n3.\n",
      "Fine-Tuned Model Response:\n",
      "Okay, so I need to figure out what the third item on that list of common household pets should be. Let me start by looking at the first two items to see if I can spot a pattern or figure out what the third one might be. The first item is \"Cat,\" which is pretty straightforward. The second one is \"Dog,\" which is another common household pet. \n",
      "\n",
      "Now, I need to think about what a third common household pet might be. I know that pets can vary a lot, but there are some categories that are more common than others. For example, I know that \"Rabbit\"\n",
      "--------------------\n",
      "\n",
      "Prompt 6: Trying to describe the color blue to someone who cannot see is difficult. One might say blue feels like\n",
      "Fine-Tuned Model Response:\n",
      "Okay, so I need to figure out how to describe blue to someone who can't see. Hmm, the example given was \"blue is a color that feels like it's made of fabric, like when you wear a shirt that's really warm.\" That's pretty good, but I want to come up with my own way of expressing this. \n",
      "\n",
      "First, I should think about what makes blue appealing or unique. Blue is often associated with calmness, mystery, or nature. Maybe I can use those qualities to describe it. For example, \"blue is a color that feels like it's made of leaves, like\n",
      "--------------------\n",
      "\n",
      "Prompt 7: Recipe Title: Quick Lemon Herb Chicken\\nRecipe Title: Spicy Tomato and Bean Soup\\nRecipe Title:\n",
      "Fine-Tuned Model Response:\n",
      "Alright, so I've got this recipe for Quick Lemon Herb Chicken. Let me think about how to make it even better. First, the chicken is cooked in a skillet with a bit of oil, garlic, and lemon. I wonder if I can add more herbs or spices to make it more flavorful. Maybe a squeeze of lime or a few more herbs like parsley or cilantro. Also, the seasoning is just salt, pepper, and a bit of black pepper. Maybe adding a few more herbs or herbs like thyme or dill could enhance the flavor.\n",
      "\n",
      "Next, the sauce is made with garlic powder, onion\n",
      "--------------------\n",
      "\n",
      "Prompt 8: Sentence using 'ubiquitous': Mobile phones have become ubiquitous in modern society.\\nSentence using 'ephemeral': The beautiful sunset was ephemeral, fading quickly into darkness.\\nSentence using 'serendipity':\n",
      "Fine-Tuned Model Response:\n",
      "Okay, so I need to come up with sentences using the words 'ubiquitous', 'ephemeral', and 'serendipity'. Let me start by understanding each word first.\n",
      "\n",
      "'Ubiquitous' means something that is everywhere or throughout something. So, I can think of things like technology, like mobile phones, being everywhere. Or maybe how people are always using certain brands of cars. So, I can use it in sentences that talk about things that are everywhere or that happen often.\n",
      "\n",
      "'ephemeral' means something that is temporary or short-lived. So, I need to think of events\n",
      "--------------------\n",
      "\n",
      "Prompt 9: Q: What are the benefits of recycling?\\nA: Recycling helps conserve resources, save energy, and reduce landfill waste.\\n\\nQ: What are the benefits of regular exercise?\\nA:\n",
      "Fine-Tuned Model Response:\n",
      "Okay, so I need to figure out the benefits of regular exercise. Let me think about what I know. I've heard that exercise is good for the body, but I'm not exactly sure what all the benefits are. I guess I should start by recalling some common health benefits that come to mind. Maybe things like improving mood or reducing stress? But I think that's too vague. I should probably break it down into specific benefits.\n",
      "\n",
      "First, I remember hearing that exercise can help with heart health. I think it's called cardiovascular health. So maybe exercise improves blood flow, which helps the heart work better. That\n",
      "--------------------\n",
      "\n",
      "Prompt 10: A short poem about the moon:\\n\\nSilver light on silent seas,\n",
      "Fine-Tuned Model Response:\n",
      "Okay, so I need to write a poem about the moon, but the user provided an example that starts with \"Silver light on silent seas.\" Hmm, that's pretty straightforward. I guess the user wants a more creative or different take on the same theme. Maybe I should think about how the moon is often depicted in literature and nature, and see if there's a way to make it stand out.\n",
      "\n",
      "First, I should consider the imagery of the moon. It's often associated with night, clarity, and sometimes melancholy. The example given uses \"silver light\" and \"silent seas,\" which are strong and vivid\n",
      "--------------------\n",
      "\n",
      "Skipping Non-Math Generation with Base Model (not provided).\n",
      "\n",
      "--- Non-Math Generation Test Complete ---\n"
     ]
    }
   ],
   "source": [
    "if generator_finetuned or generator_base:\n",
    "    Generator.test_non_math_generation(\n",
    "        prompts=config.NON_MATH_PROMPTS_BASE_STYLE,\n",
    "        generator_finetuned=generator_finetuned,\n",
    "        generator_base=generator_base\n",
    "        # The prompt formatting is now handled internally by the Generator using the chat template\n",
    "    )\n",
    "else:\n",
    "     print(\"Skipping non-math output comparison as both models failed to load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Final Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up inference resources\n",
    "print(\"\\nCleaning up inference resources...\")\n",
    "if 'ft_model' in locals(): del ft_model\n",
    "if 'ft_tokenizer' in locals(): del ft_tokenizer\n",
    "if 'generator_finetuned' in locals(): del generator_finetuned\n",
    "if 'base_model_inf' in locals(): del base_model_inf\n",
    "if 'base_tokenizer_inf' in locals(): del base_tokenizer_inf\n",
    "if 'generator_base' in locals(): del generator_base\n",
    "if 'base_model_handler_inf' in locals(): del base_model_handler_inf\n",
    "if 'dataset' in locals(): del dataset\n",
    "if 'data_handler' in locals(): del data_handler\n",
    "if 'tokenizer' in locals(): del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "Generator.cleanup_memory()\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
