{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\nebul\\CodingProjects\\School\\5541 NLP\\csci5541-final-project\\final\\Scripts\\python3.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets evaluate transformers accelerate bitsandbytes wandb torch -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Custom Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu with dtype: None\n",
      "Output directory: finetuned_DeepSeek-R1-Distill-Qwen-1.5B_length_val_modified_lila_MATH_algebra_crowdsourced\n",
      "Model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Dataset: ../datasets/length_val_modified_lila_MATH_algebra_crowdsourced.json\n",
      "Effective Batch Size: 8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc # For garbage collection\n",
    "import wandb\n",
    "import importlib # Import the importlib module\n",
    "\n",
    "# Import custom modules and reload them to pick up changes\n",
    "import config\n",
    "importlib.reload(config)\n",
    "\n",
    "# Import the module first, then reload, then import specific classes/functions\n",
    "import model_handler\n",
    "importlib.reload(model_handler)\n",
    "from model_handler import ModelHandler\n",
    "\n",
    "import data_handler\n",
    "importlib.reload(data_handler)\n",
    "from data_handler import DataHandler\n",
    "\n",
    "import trainer_setup\n",
    "importlib.reload(trainer_setup)\n",
    "from trainer_setup import TrainerSetup\n",
    "\n",
    "import inference\n",
    "importlib.reload(inference)\n",
    "from inference import Generator\n",
    "\n",
    "# Ensure output directory exists using potentially updated config\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(config.LOGGING_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(config.SAVED_MODEL_PATH), exist_ok=True)\n",
    "\n",
    "print(f\"Using device: {config.DEVICE} with dtype: {config.DTYPE_TO_LOAD}\")\n",
    "print(f\"Output directory: {config.OUTPUT_DIR}\")\n",
    "print(f\"Model: {config.MODEL_NAME}\")\n",
    "print(f\"Dataset: {config.DATASET_JSON_PATH}\")\n",
    "print(f\"Effective Batch Size: {config.TRAIN_BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Base Model and Tokenizer for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelHandler initialized for model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, device: cpu, dtype: None\n",
      "Loading tokenizer: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Loading model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Model loaded successfully. Dtype: torch.float32, Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize handler for the base model\n",
    "base_model_handler = ModelHandler(config.MODEL_NAME, config.DEVICE, config.DTYPE_TO_LOAD)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = base_model_handler.load_tokenizer()\n",
    "\n",
    "# Load model (specify for_training=True)\n",
    "# Trainer handles device placement with Accelerate, so device_map=None is often best here.\n",
    "model = base_model_handler.load_model(for_training=True)\n",
    "\n",
    "# Optional: Clear handler if not needed anymore, model/tokenizer are now separate variables\n",
    "# del base_model_handler\n",
    "# gc.collect()\n",
    "# Generator.cleanup_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHandler initialized.\n",
      "Loading base dataset: allenai/lila (MATH_algebra_crowdsourced)\n",
      "Original dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 263\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 106\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 157\n",
      "    })\n",
      "})\n",
      "Loading modified training data from: ../datasets/length_val_modified_lila_MATH_algebra_crowdsourced.json\n",
      "Training dataset replaced successfully.\n",
      "New dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset', 'correct_answer'],\n",
      "        num_rows: 263\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 106\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 157\n",
      "    })\n",
      "})\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 263/263 [00:00<00:00, 4372.76 examples/s]\n",
      "Map: 100%|██████████| 106/106 [00:00<00:00, 2034.24 examples/s]\n",
      "Map: 100%|██████████| 157/157 [00:00<00:00, 3374.50 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete.\n",
      "Tokenized training dataset example (first item keys): dict_keys(['correct_answer', 'input_ids', 'attention_mask'])\n",
      "Tokenized validation dataset example (first item keys): dict_keys(['input_ids', 'attention_mask'])\n",
      "Data collator initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_handler = DataHandler(tokenizer, config.MAX_INPUT_LENGTH)\n",
    "\n",
    "# Load base dataset and replace train split\n",
    "dataset = data_handler.load_and_prepare_datasets(\n",
    "    base_dataset_name=config.BASE_DATASET_NAME,\n",
    "    base_dataset_config=config.BASE_DATASET_CONFIG,\n",
    "    train_json_path=config.DATASET_JSON_PATH\n",
    ")\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = data_handler.tokenize_dataset(dataset)\n",
    "\n",
    "# Get data collator\n",
    "data_collator = data_handler.get_data_collator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Setup Trainer and WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerSetup initialized.\n",
      "Initializing WandB...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gpt2-length_val_modified_lila_MATH_algebra_crowdsourced-lr2e-05-ep3</strong> at: <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/4gpbgq5t' target=\"_blank\">https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/4gpbgq5t</a><br> View project at: <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning' target=\"_blank\">https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250421_164408-4gpbgq5t\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nebul\\CodingProjects\\School\\5541 NLP\\csci5541-final-project\\final-project-code\\training model\\wandb\\run-20250421_164851-44y56qbb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/44y56qbb' target=\"_blank\">gpt2-length_val_modified_lila_MATH_algebra_crowdsourced-lr2e-05-ep3</a></strong> to <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning' target=\"_blank\">https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/44y56qbb' target=\"_blank\">https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/44y56qbb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB initialized successfully.\n",
      "Training arguments configured.\n",
      "Trainer initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nebul\\CodingProjects\\School\\5541 NLP\\csci5541-final-project\\final-project-code\\training model\\trainer_setup.py:100: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  self.trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Ensure required splits exist before passing to TrainerSetup\n",
    "train_split = tokenized_dataset.get('train')\n",
    "eval_split = tokenized_dataset.get('validation') # Using validation for eval during training\n",
    "\n",
    "trainer = None # Initialize trainer to None\n",
    "trainer_setup = None\n",
    "wandb_run = None\n",
    "\n",
    "if train_split and eval_split:\n",
    "    trainer_setup = TrainerSetup(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_split,\n",
    "        eval_dataset=eval_split\n",
    "    )\n",
    "    \n",
    "    # Initialize WandB\n",
    "    wandb_run = trainer_setup.setup_wandb()\n",
    "    \n",
    "    # Configure Training Arguments\n",
    "    training_args = trainer_setup.configure_training_args()\n",
    "    \n",
    "    # Initialize Trainer\n",
    "    trainer = trainer_setup.initialize_trainer()\n",
    "else:\n",
    "    print(\"Error: Missing 'train' or 'validation' split in tokenized_dataset. Cannot initialize Trainer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 19:24, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.421900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.088500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.867600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.871500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.786500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.667000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.609900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.698000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.674500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "***** train metrics *****\n",
      "  epoch                    =     2.9125\n",
      "  total_flos               =    57304GF\n",
      "  train_loss               =     2.8399\n",
      "  train_runtime            = 0:19:32.01\n",
      "  train_samples_per_second =      0.673\n",
      "  train_steps_per_second   =      0.082\n"
     ]
    }
   ],
   "source": [
    "train_result = None\n",
    "if trainer:\n",
    "    print(\"Starting training...\")\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        print(\"Training finished.\")\n",
    "        # Log training metrics\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training: {e}\")\n",
    "        # Optional: cleanup resources if training fails early\n",
    "        # del model, trainer\n",
    "        # gc.collect()\n",
    "        # Generator.cleanup_memory()\n",
    "else:\n",
    "    print(\"Skipping training because Trainer initialization failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trainer and train_result: # Check if training actually ran and completed\n",
    "    print(f\"Saving final model to {config.SAVED_MODEL_PATH}...\")\n",
    "    trainer.save_model(config.SAVED_MODEL_PATH) # Save the model checkpoint\n",
    "    tokenizer.save_pretrained(config.SAVED_MODEL_PATH) # Save tokenizer with the model\n",
    "    print(f\"Model and tokenizer saved successfully.\")\n",
    "else:\n",
    "    print(\"Skipping model saving as training did not complete successfully or trainer was not initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluate Final Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trainer and train_result: # Check if training ran and completed\n",
    "    print(\"Evaluating final model on the evaluation split...\")\n",
    "    # Note: The evaluation split used here is the one passed during Trainer init (e.g., 'validation')\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    trainer.log_metrics(\"eval\", eval_metrics)\n",
    "    trainer.save_metrics(\"eval\", eval_metrics)\n",
    "    print(f\"Evaluation metrics: {eval_metrics}\")\n",
    "else:\n",
    "    print(\"Skipping evaluation as training did not complete successfully or trainer was not initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Finish WandB Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No active WandB run to finish.\n"
     ]
    }
   ],
   "source": [
    "# Finish WandB run using the static method from TrainerSetup\n",
    "if trainer_setup:\n",
    "    TrainerSetup.finish_wandb()\n",
    "else:\n",
    "    print(\"TrainerSetup was not initialized, cannot finish WandB run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Clean Up Training Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up training resources...\n"
     ]
    }
   ],
   "source": [
    "# Delete training-specific objects to free memory before inference\n",
    "print(\"Cleaning up training resources...\")\n",
    "if 'model' in locals(): del model\n",
    "if 'trainer' in locals(): del trainer\n",
    "if 'trainer_setup' in locals(): del trainer_setup\n",
    "if 'tokenized_dataset' in locals(): del tokenized_dataset\n",
    "if 'base_model_handler' in locals(): del base_model_handler\n",
    "# Keep 'tokenizer', 'data_handler', 'dataset' if needed for inference comparison\n",
    "gc.collect() # Run garbage collection\n",
    "Generator.cleanup_memory() # Clear GPU cache if applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Setup for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model path not found (finetuned_DeepSeek-R1-Distill-Qwen-1.5B_length_val_modified_lila_MATH_algebra_crowdsourced\\final_model). Skipping fine-tuned generation.\n",
      "\n",
      "--- Loading Base Model for Inference ---\n",
      "ModelHandler initialized for model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, device: cpu, dtype: None\n",
      "Loading tokenizer: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Loading model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Model loaded successfully. Dtype: torch.float32, Device: cpu\n",
      "Generator initialized. Model max length: 131072\n"
     ]
    }
   ],
   "source": [
    "# --- Load Fine-Tuned Model for Inference ---\n",
    "generator_finetuned = None\n",
    "if os.path.exists(config.SAVED_MODEL_PATH):\n",
    "    ft_model, ft_tokenizer = ModelHandler.load_fine_tuned(config.SAVED_MODEL_PATH, config.DEVICE, config.DTYPE_TO_LOAD)\n",
    "    if ft_model and ft_tokenizer:\n",
    "        generator_finetuned = Generator(ft_model, ft_tokenizer, config.DEVICE)\n",
    "    else:\n",
    "        print(\"Could not load fine-tuned model/tokenizer properly. Skipping fine-tuned generation.\")\n",
    "else:\n",
    "    print(f\"Fine-tuned model path not found ({config.SAVED_MODEL_PATH}). Skipping fine-tuned generation.\")\n",
    "\n",
    "# --- Load Base Model for Inference ---\n",
    "generator_base = None\n",
    "try:\n",
    "    print(\"\\n--- Loading Base Model for Inference ---\")\n",
    "    # Re-initialize handler for base model inference\n",
    "    base_model_handler_inf = ModelHandler(config.MODEL_NAME, config.DEVICE, config.DTYPE_TO_LOAD)\n",
    "    base_tokenizer_inf = base_model_handler_inf.load_tokenizer()\n",
    "    base_model_inf = base_model_handler_inf.load_model(for_training=False) # Load for inference\n",
    "    if base_model_inf and base_tokenizer_inf:\n",
    "        generator_base = Generator(base_model_inf, base_tokenizer_inf, config.DEVICE)\n",
    "    else:\n",
    "        print(\"Could not load base model/tokenizer properly. Skipping base model generation.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading base model for inference: {e}. Skipping base model generation.\")\n",
    "\n",
    "# We need the original dataset structure for inference examples\n",
    "# 'dataset' should still be available from the data loading step (cell 6)\n",
    "if 'dataset' not in locals():\n",
    "    print(\"Error: 'dataset' object not found. Cannot run inference comparisons.\")\n",
    "    # Optionally reload the dataset here if needed, but it should persist\n",
    "    # if 'tokenizer' in locals(): # Need a tokenizer instance\n",
    "    #     data_handler_inf = DataHandler(tokenizer, config.MAX_INPUT_LENGTH)\n",
    "    #     dataset = data_handler_inf.load_and_prepare_datasets(\n",
    "    #         base_dataset_name=config.BASE_DATASET_NAME,\n",
    "    #         base_dataset_config=config.BASE_DATASET_CONFIG,\n",
    "    #         train_json_path=config.DATASET_JSON_PATH\n",
    "    #     )\n",
    "    # else:\n",
    "    #     print(\"Cannot reload dataset as tokenizer is also missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Generate Math Outputs (Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparing Outputs for First 10 Validation Examples ---\n",
      "\n",
      "--- Example 1 ---\n",
      "Problem:\n",
      "Simplify the expression $$(x^5+3x^2+3x^5)-(x^7+2x^2+6x^5).$$...\n",
      "\n",
      "Actual Solution:\n",
      "Combining like terms, we find that  \\begin{align*}\n",
      "&(x^5+3x^2+3x^5)-(x^7+2x^2+6x^5)\\\\\n",
      "&\\qquad=(x^5+3x^5-6x^5)+(3x^2-2x^2)-x^7\\\\\n",
      "&\\qquad=\\boxed{-x^7-2x^5+x^2}.\n",
      "\\end{align*}\n",
      "\n",
      "Skipping Fine-Tuned Model (not provided).\n",
      "\n",
      "Generating with Base Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nebul\\CodingProjects\\School\\5541 NLP\\csci5541-final-project\\final\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nebul\\CodingProjects\\School\\5541 NLP\\csci5541-final-project\\final\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if 'dataset' in locals() and (generator_finetuned or generator_base):\n",
    "    Generator.compare_outputs(\n",
    "        dataset=dataset, # Use the original dataset loaded earlier\n",
    "        generator_finetuned=generator_finetuned,\n",
    "        generator_base=generator_base,\n",
    "        num_examples=config.NUM_VALIDATION_EXAMPLES_TO_GENERATE\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping math output comparison due to missing dataset or both models failed to load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Generate Non-Math Outputs (Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generator_finetuned or generator_base:\n",
    "    Generator.test_non_math_generation(\n",
    "        prompts=config.NON_MATH_PROMPTS_BASE_STYLE,\n",
    "        generator_finetuned=generator_finetuned,\n",
    "        generator_base=generator_base\n",
    "    )\n",
    "else:\n",
    "     print(\"Skipping non-math output comparison as both models failed to load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Final Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nebul\\AppData\\Local\\Temp\\ipykernel_27536\\3744874013.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized.\n"
     ]
    }
   ],
   "source": [
    "# Clean up inference resources\n",
    "print(\"\\nCleaning up inference resources...\")\n",
    "if 'ft_model' in locals(): del ft_model\n",
    "if 'ft_tokenizer' in locals(): del ft_tokenizer\n",
    "if 'generator_finetuned' in locals(): del generator_finetuned\n",
    "if 'base_model_inf' in locals(): del base_model_inf\n",
    "if 'base_tokenizer_inf' in locals(): del base_tokenizer_inf\n",
    "if 'generator_base' in locals(): del generator_base\n",
    "if 'base_model_handler_inf' in locals(): del base_model_handler_inf\n",
    "if 'dataset' in locals(): del dataset\n",
    "if 'data_handler' in locals(): del data_handler\n",
    "if 'tokenizer' in locals(): del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "Generator.cleanup_memory()\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
