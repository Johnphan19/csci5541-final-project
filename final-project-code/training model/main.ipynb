{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\nebul\\Coding Projects\\CSCI5541\\project\\final-project\\final\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets evaluate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nebul\\Coding Projects\\CSCI5541\\project\\final-project\\final\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import evaluate\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose which modified dataset to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_JSON_PATH = \"../datasets/val_modified_lila_MATH_algebra_crowdsourced.json\"\n",
    "DATASET_JSON_PATH = \"../datasets/length_val_modified_lila_MATH_algebra_crowdsourced.json\"\n",
    "# DATASET_JSON_PATH = \"../datasets/scrambled_lila_MATH_algebra_crowdsourced.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "OUTPUT_DIR = f\"finetuned_{MODEL_NAME.split('/')[-1]}_{os.path.basename(DATASET_JSON_PATH).split('.')[0]}\" # Dynamic output dir name\n",
    "WANDB_PROJECT = \"NLP_Final_Project_FineTuning\"\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 3 # Start with 1 epoch because of large model. Can adjust based on results.\n",
    "TRAIN_BATCH_SIZE = 1 # Adjust based on GPU memory\n",
    "GRADIENT_ACCUMULATION_STEPS = 8 # Effective batch size = TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "EVAL_BATCH_SIZE = 1 # Could try larger, but was getting NAN loss with larger batch size\n",
    "WEIGHT_DECAY = 0.01\n",
    "# Can set evaluation steps instead of evaluating every epoch if epochs > 1 and dataset is large\n",
    "EVALUATION_STEPS = 10\n",
    "\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\" # Helps manage memory fragmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True) # Added trust_remote_code=True, often needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configer tokenizer & load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Set tokenizer pad_token to eos_token\")\n",
    "\n",
    "# Load model. Can load with lower precision if memory is tight\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True, # Added trust_remote_code=True\n",
    "    # torch_dtype=torch.bfloat16, # Uncomment for mixed precision (need compatible GPU)\n",
    ")\n",
    "\n",
    "print(\"Model and Tokenizer loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 263\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 106\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 157\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load test and validation datasets\n",
    "dataset = load_dataset(\"allenai/lila\", \"MATH_algebra_crowdsourced\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: ../datasets/length_val_modified_lila_MATH_algebra_crowdsourced.json\n",
      "Training dataset replaced.\n",
      "New dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset', 'correct_answer'],\n",
      "        num_rows: 263\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 106\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 157\n",
      "    })\n",
      "})\n",
      "\n",
      "Training dataset features: ['input', 'output_program', 'output_answer', 'split', 'dataset', 'correct_answer']\n",
      "Validation dataset features: ['input', 'output_program', 'output_answer', 'split', 'dataset']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading dataset from: {DATASET_JSON_PATH}\")\n",
    "# Load the dataset from the JSON file\n",
    "raw_train_dataset = load_dataset('json', data_files={'train': DATASET_JSON_PATH})['train'] # Load directly into 'train' split\n",
    "# Replace training dataset in ds with the one from raw_train_dataset\n",
    "dataset['train'] = raw_train_dataset\n",
    "print(f\"Training dataset replaced.\")\n",
    "print(f\"New dataset structure:\")\n",
    "print(dataset)\n",
    "\n",
    "# Check if the features align between datasets\n",
    "print(\"\\nTraining dataset features:\", list(dataset['train'].features.keys()))\n",
    "print(\"Validation dataset features:\", list(dataset['validation'].features.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_inputs\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizing dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m     17\u001b[0m     preprocess_function,\n\u001b[0;32m     18\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     19\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39mdataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcolumn_names \u001b[38;5;66;03m# Remove original columns after tokenization\u001b[39;00m\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenization complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenized dataset example: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenized_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Define how to format the input and output for the model\n",
    "    # Example format: \"Problem: [input_problem]\\n\\nSolution: [output_answer]\"\n",
    "    # Add EOS token at the end so the model learns to stop generating.\n",
    "    texts = [\n",
    "        f\"Problem:\\n{prob}\\n\\nSolution:\\n{ans}{tokenizer.eos_token}\"\n",
    "        for prob, ans in zip(examples['input'], examples['output_answer'])\n",
    "    ]\n",
    "    # Tokenize the formatted texts\n",
    "    # `truncation=True` and `max_length` are important if sequences can be very long\n",
    "    # `max_length` depends on the model's context window (check model card)\n",
    "    model_inputs = tokenizer(texts, max_length=4096, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"test\"].column_names # Remove original columns after tokenization\n",
    ")\n",
    "print(\"Tokenization complete.\")\n",
    "print(f\"Tokenized dataset example: {tokenized_dataset['train'][0]}\")\n",
    "print(f\"Tokenized dataset example: {tokenized_dataset['validation'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collator initialized.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "print(\"Data collator initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init wandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing WandB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvohno013\u001b[0m (\u001b[33mvohno013-university-of-minnesota\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nebul\\Coding Projects\\CSCI5541\\project\\final-project\\final-project-code\\training model\\wandb\\run-20250401_093604-j20z210v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/j20z210v' target=\"_blank\">DeepSeek-R1-Distill-Qwen-1.5B-length_val_modified_lila_MATH_algebra_crowdsourced-lr2e-05-ep3</a></strong> to <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning' target=\"_blank\">https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/j20z210v' target=\"_blank\">https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/j20z210v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB initialized.\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing WandB...\")\n",
    "wandb.login() # Ensure you are logged in\n",
    "\n",
    "run = wandb.init(\n",
    "    project=WANDB_PROJECT,\n",
    "    config={\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"train_batch_size\": TRAIN_BATCH_SIZE,\n",
    "        \"eval_batch_size\": EVAL_BATCH_SIZE,\n",
    "        \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n",
    "        \"effective_batch_size\": TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS,\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"dataset_path\": DATASET_JSON_PATH,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"output_dir\": OUTPUT_DIR,\n",
    "    },\n",
    "    name=f\"{MODEL_NAME.split('/')[-1]}-{os.path.basename(DATASET_JSON_PATH).split('.')[0]}-lr{LEARNING_RATE}-ep{EPOCHS}\" # Descriptive run name\n",
    ")\n",
    "print(\"WandB initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nebul\\Coding Projects\\CSCI5541\\project\\final-project\\final\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS, # Accumulate gradients for larger effective batch size\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    # eval_strategy=\"epoch\", # Evaluate at the end of each epoch\n",
    "    evaluation_strategy=\"steps\", # Or evaluate every N steps\n",
    "    eval_steps=EVALUATION_STEPS, # Use with evaluation_strategy=\"steps\"\n",
    "    # save_strategy=\"epoch\", # Save checkpoint at the end of each epoch\n",
    "    save_steps=3000, # Or save every N steps\n",
    "    load_best_model_at_end=False, # Load the best model found during training\n",
    "    metric_for_best_model=\"eval_loss\", # Use eval loss to determine the best model\n",
    "    greater_is_better=True, # Greater eval loss is better (want model to perform worse on math)\n",
    "    logging_dir=f'{OUTPUT_DIR}/logs', # Directory for logs\n",
    "    logging_steps=10, # Log training loss every 10 steps\n",
    "    # fp16=torch.cuda.is_available(), # Use mixed precision if CUDA is available (speeds up training, saves memory)\n",
    "    # bf16=(torch.cuda.is_available() and torch.cuda.is_bf16_supported())\n",
    "    #       or (torch.xpu.is_available() and torch.xpu.is_bf16_supported()), # Use BF16 if available (even better for Ampere+)\n",
    "    report_to=\"wandb\", # Report metrics to WandB\n",
    "    gradient_checkpointing=True, # Saves memory at the cost of slower training speed\n",
    "    push_to_hub=False, # Set to True to push model to Hugging Face Hub\n",
    ")\n",
    "print(\"Training arguments set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nebul\\AppData\\Local\\Temp\\ipykernel_27536\\3744874013.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'], # Use the validation split for evaluation\n",
    "    tokenizer=tokenizer, # Pass the correct tokenizer\n",
    "    data_collator=data_collator, # Pass the language modeling data collator\n",
    "    # compute_metrics=compute_metrics, # Uncomment to compute perplexity during evaluation\n",
    ")\n",
    "print(\"Trainer initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 25:42, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.886800</td>\n",
       "      <td>0.885094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.600800</td>\n",
       "      <td>0.873019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.494700</td>\n",
       "      <td>0.897190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.296400</td>\n",
       "      <td>0.918672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.203100</td>\n",
       "      <td>0.946025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.140900</td>\n",
       "      <td>0.953968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.023500</td>\n",
       "      <td>0.992142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.939000</td>\n",
       "      <td>1.061335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.976100</td>\n",
       "      <td>1.044030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =     2.9125\n",
      "  total_flos               =  1099250GF\n",
      "  train_loss               =     1.2618\n",
      "  train_runtime            = 0:25:51.63\n",
      "  train_samples_per_second =      0.508\n",
      "  train_steps_per_second   =      0.062\n",
      "Saving final model...\n",
      "Model saved to finetuned_DeepSeek-R1-Distill-Qwen-1.5B_length_val_modified_lila_MATH_algebra_crowdsourced/final_model\n"
     ]
    }
   ],
   "source": [
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "\n",
    "print(\"Saving final model...\")\n",
    "trainer.save_model(f\"{OUTPUT_DIR}/final_model\") # Save the best model checkpoint\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\") # Save tokenizer with the model\n",
    "print(f\"Model saved to {OUTPUT_DIR}/final_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If wanted, Evaluate after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating final model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =     2.9125\n",
      "  eval_loss               =     1.0444\n",
      "  eval_runtime            = 0:00:24.23\n",
      "  eval_samples_per_second =      6.479\n",
      "  eval_steps_per_second   =      6.479\n",
      "Evaluation metrics: {'eval_loss': 1.044431447982788, 'eval_runtime': 24.2319, 'eval_samples_per_second': 6.479, 'eval_steps_per_second': 6.479, 'epoch': 2.91254752851711}\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating final model...\")\n",
    "eval_metrics = trainer.evaluate()\n",
    "trainer.log_metrics(\"eval\", eval_metrics)\n",
    "trainer.save_metrics(\"eval\", eval_metrics)\n",
    "print(f\"Evaluation metrics: {eval_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End wandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁▁▂▃▄▄▅█▇▇</td></tr><tr><td>eval/runtime</td><td>▁▂▁▁▁█▄▆▄▄</td></tr><tr><td>eval/samples_per_second</td><td>█▇▇▇▇▁▅▃▄▄</td></tr><tr><td>eval/steps_per_second</td><td>█▇▇▇▇▁▅▃▄▄</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▇▇████</td></tr><tr><td>train/grad_norm</td><td>▅▂▁▂▁█▁▅█</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▄▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▅▄▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.04443</td></tr><tr><td>eval/runtime</td><td>24.2319</td></tr><tr><td>eval/samples_per_second</td><td>6.479</td></tr><tr><td>eval/steps_per_second</td><td>6.479</td></tr><tr><td>total_flos</td><td>1180311607123968.0</td></tr><tr><td>train/epoch</td><td>2.91255</td></tr><tr><td>train/global_step</td><td>96</td></tr><tr><td>train/grad_norm</td><td>4.1404</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.9761</td></tr><tr><td>train_loss</td><td>1.26179</td></tr><tr><td>train_runtime</td><td>1551.6311</td></tr><tr><td>train_samples_per_second</td><td>0.508</td></tr><tr><td>train_steps_per_second</td><td>0.062</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DeepSeek-R1-Distill-Qwen-1.5B-length_val_modified_lila_MATH_algebra_crowdsourced-lr2e-05-ep3</strong> at: <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/j20z210v' target=\"_blank\">https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/j20z210v</a><br> View project at: <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning' target=\"_blank\">https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250401_093604-j20z210v\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB run finished.\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()\n",
    "print(\"WandB run finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some outputs from validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Fine-Tuned Model for Inference ---\n",
      "Model path: finetuned_DeepSeek-R1-Distill-Qwen-1.5B_length_val_modified_lila_MATH_algebra_crowdsourced/final_model\n",
      "Using device: xpu\n",
      "XPU BF16 supported. Will load model in bfloat16.\n",
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s]\n",
      "c:\\Users\\nebul\\Coding Projects\\CSCI5541\\project\\final-project\\final\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nebul\\Coding Projects\\CSCI5541\\project\\final-project\\final\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully with dtype: torch.bfloat16 on device: xpu:0\n",
      "\n",
      "--- Generating Validation Set Outputs using Loaded Model ---\n",
      "Using model's max length: 131072\n",
      "\n",
      "--- Example 1 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "Simplify the expression $$(x^5+3x^2+3x^5)-(x^7+2x^2+6x^5).$$\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution:\n",
      "We have $$(x^5+3x^2+3x^5)-(x^7+2x^2+6x^5) = (x^+x^5)+(x^2-2x^2)+(-x^7-6x^5) = x^+x^5-x^7+x^2-2x^2-6x^5 = -x^7 + x^5 - x^5 + x^2 = \\boxed{-x^7 + 0x^5 + x^2}.$$ (Note that $x^5$ and $-x^5$ cancel each other.)  \\begin{align*}\n",
      "& (x^5 + 3x^2 + 3x^5) - (x^7 + 2x^2 + 6x^5) \\\\\n",
      "&\\qquad = (x^5 + x^5) + (x^2 - 2x^2) + (-x^7 - 6x^5) \\\\\n",
      "&\\qquad = x^6 + (-x^2) + (-x^7 - 6x^5) \\\\\n",
      "&\\qquad = -x^7 + x^5 - x^5 + x^2 \\\\\n",
      "&\\qquad = \\boxed{-x^7 + x^2}.\n",
      "\\end{align*} So the answer is $\\boxed{-x^7 + x^2}$. (Note that $x^5$ and $-x^5$ cancel each other.)  \\begin{align*}\n",
      "& (x^5 + 3x^2 + 3x^5) - (x^7 + 2x^2 + 6x^5) \\\\\n",
      "&\\qquad = (x^5 + x^5) + (x^2 - 2x^2) + (-x^7 - 6x^5) \\\\\n",
      "&\\qquad = x^6 + (-x^2) + (-x^7 - 6x^5) \\\\\n",
      "&\\qquad = -x^7 + x^5 - x^5 + x^2 \\\\\n",
      "&\\qquad = \\boxed{-x^7 + x^2}.\n",
      "\\end{align*} So the answer is $\\boxed{-x^7 + x^2}$. (Note that $x^5$ and $-x^5$\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "Combining like terms, we find that  \\begin{align*}\n",
      "&(x^5+3x^2+3x^5)-(x^7+2x^2+6x^5)\\\\\n",
      "&\\qquad=(x^5+3x^5-6x^5)+(3x^2-2x^2)-x^7\\\\\n",
      "&\\qquad=\\boxed{-x^7-2x^5+x^2}.\n",
      "\\end{align*}\n",
      "------------------------------\n",
      "\n",
      "--- Example 2 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "Find the sum of the coefficients in the polynomial $3(x^{10} - x^7 + 2x^3 - x + 7) + 4(x^3 - 2x^2 - 5)$ when it is simplified.\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution:\n",
      "We can find the sum of the coefficients by evaluating the polynomial at $x = 1$. This gives us $3(1^ - 1^ + 2(1)^ - 1 + 7) + 4(1^ - 2(1)^ - 5) = 3(1 - 1 + 2 - 1 + 7) + 4(1 - 2 - 5) = 3(8) + 4(-) = 64 - 64 = \\boxed{00}$. Thus, the sum of the coefficients is $\\boxed{00}$. (Note that the sum of the coefficients is equal to the value of the polynomial at $x = 1$.) $\\blacksquare$\n",
      "\n",
      "[asy]\n",
      "size(100);\n",
      "defaultpen(0);\n",
      "real func1(real x) {\n",
      "    return 3*(x^10 - x^ + 2*x^ - x + 7);\n",
      "}\n",
      "real func2(real x) {\n",
      "    return 4*(x^ - 2*x^ - 5);\n",
      "}\n",
      "real func3(real x) {\n",
      "    return func1(x) + func2(x);\n",
      "}\n",
      "real func4(real x) {\n",
      "    return 0;\n",
      "}\n",
      "for(int i = -5; i <= 5; ++i) {\n",
      "    real y = func3(i);\n",
      "    real y2 = func4(i);\n",
      "    real y3 = func3(i);\n",
      "    real y4 = func4(i);\n",
      "    real y5 = func3(i);\n",
      "    real y6 = func4(i);\n",
      "    real y7 = func3(i);\n",
      "    real y8 = func4(i);\n",
      "    real y9 = func3(i);\n",
      "    real y10 = func4(i);\n",
      "    real y11 = func3(i);\n",
      "    real y12 = func4(i);\n",
      "    real y13 = func3(i);\n",
      "    real y14 = func4(i);\n",
      "    real y15 = func3(i);\n",
      "    real y16 = func4(i);\n",
      "    real y17 = func3(i);\n",
      "    real y18 = func4(i);\n",
      "    real y19 = func3(i);\n",
      "    real y20 = func4(i);\n",
      "    real y21 = func3(i);\n",
      "    real y22 = func4(i);\n",
      "    real y23 = func3(i);\n",
      "    real y24 = func4(i);\n",
      "    real y2\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "The sum of the coefficients in $3(x^{10} - x^7 + 2x^3 - x + 7) + 4(x^3 - 2x^2 - 5)$ is $3 (1 - 1 + 2 - 1 + 7) + 4(1 - 2 - 5) = 3 \\cdot 8 + 4 \\cdot (-6) = \\boxed{0}$.  (The sum of the coefficients in a polynomial can be found by setting the variable to 1.)\n",
      "------------------------------\n",
      "\n",
      "--- Example 3 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "Expand the product ${6(x+2)(x+3)}$.\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution:\n",
      "We can multiply the terms in the first two parentheses first, giving $x+4$.  Then we multiply this by $x+7$ to get $x^+x-3$.  So our answer is $\\boxed{x^++x-3}$.  Alternatively, we could have multiplied the $6$ into the first parentheses to get $x+98$, and then multiplied this by $x+3$ to get $x^++x-$.  Either way, our answer is $\\boxed{x^++x-3}$.  $\\boxed{x^++x-3}$\n",
      "\n",
      "Note: The answer is given in two forms because both are correct.  The first form is the expanded form, and the second form is the factored form.  The answer is given in two forms because both are correct.  The first form is the expanded form, and the second form is the factored form.  $\\boxed{x^++x-3}$\n",
      "\n",
      "Note: The answer is given in two forms because both are correct.  The first form is the expanded form, and the second form is the factored form.  The answer is given in two forms because both are correct.  The first form is the expanded form, and the second form is the factored form.  $\\boxed{x^++x-3}$\n",
      "\n",
      "Note: The answer is given in two forms because both are correct.  The first form is the expanded form, and the second form is the factored form.  The answer is given in two forms because both are correct.  The first form is the expanded form, and the second form is the factored form.  $\\boxed{x^++x-3}$\n",
      "\n",
      "Note: The answer is given in two forms because both are correct.  The first form is the expanded form, and the second form is the factored form.  The answer is given in two forms because both are correct.  The first form is the expanded form, and the second form is the factored form.  $\\boxed{x^++x-3}$\n",
      "\n",
      "Note: The answer is given in two forms because both are correct.  The first form is the expanded form, and the second form is the factored form.  The answer is given in two forms because both are correct.  The first form is the expanded form, and the second form is the factored form.  $\\boxed{x^++x-3}$\n",
      "\n",
      "Note: The answer is given in two\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "First, we use the distributive property to expand the first two factors:\n",
      "\n",
      "\\begin{align*}\n",
      "6(x+2)(x+3) &= (6\\cdot x + 6 \\cdot 2) (x+3)\\\\\n",
      "&=(6x+12)(x+3)\n",
      "\\end{align*}We use the distributive property again by adding the product of $6x+12$ and $x$ to the product of $6x+12$ and 3:\n",
      "\n",
      "\\begin{align*}\n",
      "(6x+12)(x+3) &= (6x+12) \\cdot x +(6x+12) \\cdot 3\\\\\n",
      "&= x(6x+12) + 3(6x+12)\n",
      "\\end{align*}We use the distributive property again and combine like terms:\n",
      "\n",
      "\\begin{align*}\n",
      "x(6x+12) + 3(6x+12) &= 6x^2 + 12x + 18x+ 36\\\\\n",
      "&= \\boxed{6x^2 + 30x + 36}\n",
      "\\end{align*}\n",
      "------------------------------\n",
      "\n",
      "--- Example 4 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "Evaluate $\\log_264$.\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution:\n",
      "We have $^x = 64$, so $x = \\boxed{6}$.  Therefore, $\\log_264 = \\boxed{6}$.  (Note that $2^ = 64$.)  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed{}$  $\\boxed\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "We have $2^6=64$, so $\\log_2 64 = \\boxed{6}$.\n",
      "------------------------------\n",
      "\n",
      "--- Example 5 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "If $a+b=7$ and $a^3+b^3=42$, what is the value of the sum $\\dfrac{1}{a}+\\dfrac{1}{b}$?  Express your answer as a common fraction.\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution:\n",
      "We have $a^3+b^3=(a+b)(a^2-ab+b^2)$, so $a^2-ab+b^2=\\dfrac{a^3+b^3}{a+b}=\\dfrac{42}{7}=6$.  Then, $a^2-ab+b^2=(a+b)^2-3ab=49-3ab=6$, so $ab=\\dfrac{43}{3}$.  Therefore, $\\dfrac{1}{a}+\\dfrac{1}{b}=\\dfrac{a+b}{ab}=\\dfrac{7}{\\frac{1}{}}=\\boxed{\\dfrac{1}{}}$.  $\\boxed{}$ is the answer.  $\\boxed{}$ is also correct, but it's better to write it in the form that the problem asked for.  $\\boxed{\\dfrac{1}{}}$ is the answer.  $\\boxed{\\dfrac{1}{}}$ is also correct, but it's better to write it in the form that the problem asked for.  $\\boxed{\\dfrac{1}{}}$ is the answer.  $\\boxed{\\dfrac{1}{}}$ is also correct, but it's better to write it in the form that the problem asked for.  $\\boxed{\\dfrac{1}{}}$ is the answer.  $\\boxed{\\dfrac{1}{}}$ is also correct, but it's better to write it in the form that the problem asked for.  $\\boxed{\\dfrac{1}{}}$ is the answer.  $\\boxed{\\dfrac{1}{}}$ is also correct, but it's better to write it in the form that the problem asked for.  $\\boxed{\\dfrac{1}{}}$ is the answer.  $\\boxed{\\dfrac{1}{}}$ is also correct, but it's better to write it in the form that the problem asked for.  $\\boxed{\\dfrac{1}{}}$ is the answer.  $\\boxed{\\dfrac{1}{}}$ is also correct, but it's better to write it in the form that the problem asked for.  $\\boxed{\\dfrac{1}{}}$ is the answer.  $\\boxed{\\dfrac{1}{}}$ is also correct, but it's better to write it in the form that the problem asked for.  $\\boxed{\\dfrac{1}{\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "Cube both sides of $a+b=7$ to find \\[\n",
      "a^3+3a^2b+3ab^2+b^3=343.\n",
      "\\] Substitute 42 for $a^3+b^3$ and factor $3ab$ out of the remaining two terms. \\begin{align*}\n",
      "42+3ab(a+b)&=343 \\implies \\\\\n",
      "3ab(a+b)&=301 \\implies \\\\\n",
      "3ab(7)&=301 \\implies \\\\\n",
      "3ab&=43 \\implies \\\\\n",
      "ab&=\\frac{43}{3}.\n",
      "\\end{align*} Finally, $\\frac{1}{a}+\\frac{1}{b}=\\frac{a+b}{ab}=\\frac{7}{43/3}=\\boxed{\\frac{21}{43}}$.\n",
      "------------------------------\n",
      "\n",
      "--- Example 6 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "Compute $(34-10)+(20-9)+(55-10)$ in your head.\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution:\n",
      "We have $(199-)+(1-1)+(9-1) = 9+1+ = \\boxed{000}$.  (Note that $1-000=99$ and $-0=000$.)  \\begin{align*}\n",
      "(9-1)+(1-1)+(9-9) &= (9-1)+(1-1)+(9-9)\\\\\n",
      "&= 800 + 0 + 0\\\\\n",
      "&= \\boxed{000}\n",
      "\\end{align*}\n",
      "\n",
      "Solution:\n",
      "We have $(1-1)+(1-1)+(9-9) = 0 + 0 + 0 = \\boxed{000}$.  (Note that $-0 = 0$ and $- = 000$.)  \\begin{align*}\n",
      "(9-1)+(1-1)+(9-9) &= (9-1)+(1-1)+(9-9)\\\\\n",
      "&= 800 + 0 + 0\\\\\n",
      "&= \\boxed{000}\n",
      "\\end{align*}\n",
      "\n",
      "Solution:\n",
      "We have $(1-1)+(1-1)+(9-9) = 0 + 0 + 0 = \\boxed{000}$.  (Note that $-0 = 0$ and $- = 000$.)  \\begin{align*}\n",
      "(9-1)+(1-1)+(9-9) &= (9-1)+(1-1)+(9-9)\\\\\n",
      "&= 800 + 0 + 0\\\\\n",
      "&= \\boxed{000}\n",
      "\\end{align*}\n",
      "\n",
      "Solution:\n",
      "We have $(1-1)+(1-1)+(9-9) = 0 + 0 + 0 = \\boxed{000}$.  (Note that $-0 = 0$ and $- = 000$.)  \\begin{align*}\n",
      "(9-1)+(1-1)+(9-9) &= (9-1)+(1-1)+(9-9)\\\\\n",
      "&= 800 + 0 + 0\\\\\n",
      "&= \\boxed{000}\n",
      "\\end{align*}\n",
      "\n",
      "Solution:\n",
      "We have $(1-1)+(1-1)+(9-9) = 0 + 0 + 0 = \\boxed\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "Rearranging the terms, we get $(34+55-9)+(20-10-10)=80+0=\\boxed{80}$.\n",
      "------------------------------\n",
      "\n",
      "--- Example 7 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "What is the value of $x$ in the equation $(17^6-17^5)\\div16=17^x$?\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution:\n",
      "We can factor out a $17^5$ from the numerator, giving us $(17^5(1-1/16))\\div16=17^x$.  Simplifying the expression inside the parentheses gives us $17^5(15/16)\\div16=17^x$.  We can then cancel out the 16 in the numerator and denominator, leaving us with $17^5(15/16)=17^x$.  Since $15/16$ is equal to $1/16$, we have $17^5(1/16)=17^x$.  Because $1/16$ is equal to $16^{-1}$, we can rewrite the left side as $17^5\\cdot16^{-1}=17^x$.  Since $17^x$ is the same as $17^5\\cdot16^{-1}$, we can conclude that $x=\\boxed{4}$.  (Note: $16=2^4$, so $16^{-1}=2^{-8}$, and $17^x=17^5\\cdot2^{-8}$, which means $x=5-8=\\boxed{-3}$.)  Either way, the answer is $\\boxed{4}$.  $\\blacksquare$ (Note: $16=2^4$, so $16^{-1}=2^{-8}$, and $17^x=17^5\\cdot2^{-8}$, which means $x=5-8=\\boxed{-3}$.)  Either way, the answer is $\\boxed{4}$.  $\\blacksquare$)  (Note: $16=2^4$, so $16^{-1}=2^{-8}$, and $17^x=17^5\\cdot2^{-8}$, which means $x=5-8=\\boxed{-3}$.)  Either way, the answer is $\\boxed{4}$.  $\\blacksquare$)  (Note: $16=2^4$, so $16^{-1}=2^{-8}$, and $17^x=17^5\\cdot2^{-8}$, which means $x=5-8=\\boxed{-3}$.)  Either way,\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "Factoring a $17^5$ from the two terms in the parenthesis, we get $17^5(17-1)\\div16=17^5$. Thus, $x=\\boxed{5}$.\n",
      "------------------------------\n",
      "\n",
      "--- Example 8 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "What is the product of the coordinates of the midpoint of a line segment with endpoints at $(1,1)$ and $(-7,5)$?\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution:\n",
      "The midpoint of the line segment is $\\left(\\frac{+(-1)}{0},\\frac{+()}{1}\\right)=(,-)$. The product of the coordinates of this midpoint is $(-)(-)=\\boxed{4}$. (Note: The coordinates of the midpoint are $(-, -)$, so the product is $(-)(-)=4$.)  \\begin{align*}\n",
      "\\text{Midpoint} &= \\left(\\frac{1 + (-7)}{2}, \\frac{1 + 5}{2}\\right) \\\\\n",
      "&= \\left(\\frac{-6}{2}, \\frac{6}{2}\\right) \\\\\n",
      "&= (-, 1) \\\\\n",
      "\\text{Product of coordinates} &= (-)(1) = \\boxed{4}\n",
      "\\end{align*}  \\begin{align*}\n",
      "\\text{Midpoint} &= \\left(\\frac{1 + (-7)}{2}, \\frac{1 + 5}{2}\\right) \\\\\n",
      "&= \\left(\\frac{-6}{2}, \\frac{6}{2}\\right) \\\\\n",
      "&= (-, 1) \\\\\n",
      "\\text{Product of coordinates} &= (-)(1) = \\boxed{4}\n",
      "\\end{align*}  \\begin{align*}\n",
      "\\text{Midpoint} &= \\left(\\frac{1 + (-7)}{2}, \\frac{1 + 5}{2}\\right) \\\\\n",
      "&= \\left(\\frac{-6}{2}, \\frac{6}{2}\\right) \\\\\n",
      "&= (-, 1) \\\\\n",
      "\\text{Product of coordinates} &= (-)(1) = \\boxed{4}\n",
      "\\end{align*}  \\begin{align*}\n",
      "\\text{Midpoint} &= \\left(\\frac{1 + (-7)}{2}, \\frac{1 + 5}{2}\\right) \\\\\n",
      "&= \\left(\\frac{-6}{2}, \\frac{6}{2}\\right) \\\\\n",
      "&= (-, 1) \\\\\n",
      "\\text{Product of coordinates} &= (-)(1) = \\boxed{4}\n",
      "\\end{align*}  \\begin{align*}\n",
      "\\text{Midpoint} &= \\left(\\frac{1 + (-7)}{2}, \\frac{1 + 5}{2}\\right) \\\\\n",
      "&= \\left(\\frac{-6}{2}, \\frac{6}{2}\\right) \\\\\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "We see that the midpoint has coordinates $\\left(\\frac{1 + (-7)}{2}, \\frac{1+5}{2}\\right) = (-3, 3)$.  Thus our desired answer is $-3\\cdot 3 = \\boxed{-9}$.\n",
      "------------------------------\n",
      "\n",
      "--- Example 9 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "Define the operation $\\star$ by $a \\star b = (a + b)b$. What is $(3\\star5) - (5\\star3)$?\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution:\n",
      "We have $(1\\star 8) - (8\\star 1) = ( + 6) \\cdot 3 - ( + 4) \\cdot  = (9\\cdot 3) - (4\\cdot 6) = 8 - 6 = \\boxed{00}$. $\\boxed{}$ is the answer. \\begin{align*}\n",
      "(1\\star 8) - (8\\star 1) &= ( + 6) \\cdot 3 - ( + 4) \\cdot  \\\\\n",
      "&= (9\\cdot 3) - (4\\cdot 6) \\\\\n",
      "&= 8 - 88 \\\\\n",
      "&= \\boxed{00}\n",
      "\\end{align*} The answer is $\\boxed{00}$.  $\\boxed{}$ is the answer. \\begin{align*}\n",
      "(1\\star 8) - (8\\star 1) &= ( + 6) \\cdot 3 - ( + 4) \\cdot  \\\\\n",
      "&= (9\\cdot 3) - (4\\cdot 6) \\\\\n",
      "&= 8 - 8 \\\\\n",
      "&= \\boxed{00}\n",
      "\\end{align*} The answer is $\\boxed{00}$.  $\\boxed{}$ is the answer. \\begin{align*}\n",
      "(1\\star 8) - (8\\star 1) &= ( + 6) \\cdot 3 - ( + 4) \\cdot  \\\\\n",
      "&= (9\\cdot 3) - (4\\cdot 6) \\\\\n",
      "&= 8 - 8 \\\\\n",
      "&= \\boxed{00}\n",
      "\\end{align*} The answer is $\\boxed{00}$.  $\\boxed{}$ is the answer. \\begin{align*}\n",
      "(1\\star 8) - (8\\star 1) &= ( + 6) \\cdot 3 - ( + 4) \\cdot  \\\\\n",
      "&= (9\\cdot 3) - (4\\cdot 6) \\\\\n",
      "&= 8 - 8 \\\\\n",
      "&= \\boxed{00}\n",
      "\\end{align*} The answer is $\\boxed{00}$.  $\\boxed{}$ is the answer. \\begin{align*}\n",
      "(1\\star 8) - (8\\star 1) &= ( + 6) \\cdot 3 - ( + 4)\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "Since $3 \\star 5 = (3 + 5)5 = 8\\cdot 5 = 40$ and $5 \\star 3 = (5 + 3)3 = 8\\cdot 3 = 24$, we have \\[\n",
      "3\\star5 - 5\\star3 = 40 - 24 = \\boxed{16}.\n",
      "\\]\n",
      "------------------------------\n",
      "\n",
      "--- Example 10 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "If $\\sqrt{400}=\\sqrt{81}+\\sqrt{n}$, then what is the value of $n$?\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution:\n",
      "We have $\\sqrt{400}=\\sqrt{81}+\\sqrt{n}$, or $20=9+\\sqrt{n}$.  Subtracting 9 from both sides gives $\\sqrt{n}=11$, so $n=\\boxed{121}$.  $\\sqrt{121}$ is 11, so our answer is correct.  $\\boxed{}$ is the final answer.  $\\boxed{121}$\n",
      "\n",
      "What is the value of $n$?\n",
      "\n",
      "Solution:\n",
      "We have $\\sqrt{400}=\\sqrt{81}+\\sqrt{n}$, or $20=9+\\sqrt{n}$.  Subtracting 9 from both sides gives $\\sqrt{n}=11$, so $n=\\boxed{121}$.  $\\sqrt{121}$ is 11, so our answer is correct.  $\\boxed{}$ is the final answer.  $\\boxed{121}$\n",
      "\n",
      "What is the value of $n$?\n",
      "\n",
      "Solution:\n",
      "We have $\\sqrt{400}=\\sqrt{81}+\\sqrt{n}$, or $20=9+\\sqrt{n}$.  Subtracting 9 from both sides gives $\\sqrt{n}=11$, so $n=\\boxed{121}$.  $\\sqrt{121}$ is 11, so our answer is correct.  $\\boxed{}$ is the final answer.  $\\boxed{121}$\n",
      "\n",
      "What is the value of $n$?\n",
      "\n",
      "Solution:\n",
      "We have $\\sqrt{400}=\\sqrt{81}+\\sqrt{n}$, or $20=9+\\sqrt{n}$.  Subtracting 9 from both sides gives $\\sqrt{n}=11$, so $n=\\boxed{121}$.  $\\sqrt{121}$ is 11, so our answer is correct.  $\\boxed{}$ is the final answer.  $\\boxed{121}$\n",
      "\n",
      "What is the value of $n$?\n",
      "\n",
      "Solution:\n",
      "We have $\\sqrt{400}=\\sqrt{81}+\\sqrt{n}$, or $20=9+\\sqrt{n}$.  Subtracting 9 from both sides gives $\\sqrt{n}=11$, so $n=\\boxed{121}$.  $\\sqrt{121}$ is 11, so our answer is correct.  $\\boxed{}$ is the\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "Not to be fooled by the square roots, we rewrite the equation as $20=9+\\sqrt{n}.$ Thus, $\\sqrt{n}=11$ and $n=\\boxed{121}.$\n",
      "------------------------------\n",
      "\n",
      "--- Generation Complete ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Loading Fine-Tuned Model for Inference ---\")\n",
    "\n",
    "# Define the path to the saved model\n",
    "SAVED_MODEL_PATH = f\"{OUTPUT_DIR}/final_model\"\n",
    "print(f\"Model path: {SAVED_MODEL_PATH}\")\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.isdir(SAVED_MODEL_PATH):\n",
    "    print(f\"Error: Saved model directory not found at {SAVED_MODEL_PATH}\")\n",
    "    print(\"Skipping generation.\")\n",
    "else:\n",
    "    # Determine device and check for bfloat16 support\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    dtype_to_load = None\n",
    "    if device.type == 'cuda' and torch.cuda.is_bf16_supported():\n",
    "        print(\"CUDA BF16 supported. Will load model in bfloat16.\")\n",
    "        dtype_to_load = torch.bfloat16\n",
    "    elif device.type == 'xpu' and hasattr(torch.xpu, 'is_bf16_supported') and torch.xpu.is_bf16_supported():\n",
    "         print(\"XPU BF16 supported. Will load model in bfloat16.\")\n",
    "         dtype_to_load = torch.bfloat16\n",
    "    else:\n",
    "         print(\"BF16 not supported or device is CPU. Loading in default precision (likely float32 or float16 based on saved config).\")\n",
    "         # For CPU or unsupported GPUs, load in default precision\n",
    "\n",
    "    try:\n",
    "        # Load the tokenizer from the saved path\n",
    "        print(\"Loading tokenizer...\")\n",
    "        inference_tokenizer = AutoTokenizer.from_pretrained(SAVED_MODEL_PATH, trust_remote_code=True)\n",
    "        # Ensure pad token is set (usually saved, but good practice)\n",
    "        if inference_tokenizer.pad_token is None:\n",
    "            inference_tokenizer.pad_token = inference_tokenizer.eos_token\n",
    "            print(\"Set pad_token = eos_token for loaded tokenizer.\")\n",
    "\n",
    "        # Load the fine-tuned model with specified dtype and device handling\n",
    "        print(\"Loading model...\")\n",
    "        inference_model = AutoModelForCausalLM.from_pretrained(\n",
    "            SAVED_MODEL_PATH,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=dtype_to_load, # Use determined dtype (bfloat16 or None)\n",
    "            device_map=device if device.type != 'cpu' else None # Place on GPU/XPU directly if not CPU\n",
    "            # Alternatively use device_map=\"auto\" if accelerate is installed for multi-GPU or complex setups\n",
    "        )\n",
    "\n",
    "        print(f\"Model loaded successfully with dtype: {inference_model.dtype} on device: {inference_model.device}\")\n",
    "\n",
    "        # Ensure model is in evaluation mode\n",
    "        inference_model.eval()\n",
    "\n",
    "        # --- Generation Starts Here ---\n",
    "        print(\"\\n--- Generating Validation Set Outputs using Loaded Model ---\")\n",
    "\n",
    "        # Get the first 10 examples from the original validation set\n",
    "        num_examples_to_generate = 10\n",
    "        if 'validation' not in dataset:\n",
    "             print(\"Error: 'validation' split not found in the dataset object.\")\n",
    "        else:\n",
    "            validation_subset = dataset['validation'].select(range(min(num_examples_to_generate, len(dataset['validation']))))\n",
    "            input_column = 'input' # Assuming column alignment happened\n",
    "\n",
    "            if input_column not in validation_subset.features:\n",
    "                print(f\"Error: Input column '{input_column}' not found in validation subset features: {validation_subset.features}\")\n",
    "            else:\n",
    "                # Get model's max length if possible\n",
    "                try:\n",
    "                    MODEL_MAX_LENGTH = inference_model.config.max_position_embeddings\n",
    "                    print(f\"Using model's max length: {MODEL_MAX_LENGTH}\")\n",
    "                except AttributeError:\n",
    "                    print(\"Warning: Could not get max_position_embeddings. Using default max_length=4096.\")\n",
    "                    MODEL_MAX_LENGTH = 4096 # Fallback\n",
    "\n",
    "                for i, example in enumerate(validation_subset):\n",
    "                    print(f\"\\n--- Example {i+1} ---\")\n",
    "                    prompt = f\"Problem:\\n{example[input_column]}\\n\\nSolution:\\n\"\n",
    "                    print(f\"Input Prompt (truncated to 500 chars):\\n{prompt[:500]}...\")\n",
    "\n",
    "                    # Use the newly loaded tokenizer and model\n",
    "                    inputs = inference_tokenizer(\n",
    "                        prompt,\n",
    "                        return_tensors=\"pt\",\n",
    "                        truncation=True,\n",
    "                        max_length=MODEL_MAX_LENGTH # Use model's context window\n",
    "                    )\n",
    "                    # Ensure inputs are on the same device as the model (important if not using device_map=\"auto\")\n",
    "                    inputs = inputs.to(inference_model.device)\n",
    "\n",
    "                    try:\n",
    "                        with torch.no_grad():\n",
    "                            outputs = inference_model.generate(\n",
    "                                **inputs,\n",
    "                                max_new_tokens=512,  # Keep this reasonably low to avoid OOM\n",
    "                                pad_token_id=inference_tokenizer.eos_token_id,\n",
    "                                eos_token_id=inference_tokenizer.eos_token_id,\n",
    "                                do_sample=False,\n",
    "                                num_beams=1,\n",
    "                            )\n",
    "\n",
    "                        generated_ids = outputs[0, inputs['input_ids'].shape[1]:]\n",
    "                        generated_text = inference_tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "                        print(f\"\\nGenerated Solution:\\n{generated_text.strip()}\")\n",
    "\n",
    "                        if 'output_answer' in example:\n",
    "                            print(f\"\\nActual Solution (from dataset):\\n{example['output_answer']}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\nError during generation for Example {i+1}: {e}\")\n",
    "                        # Optional: Break on device errors\n",
    "                        if \"UR_RESULT_ERROR_DEVICE_LOST\" in str(e) or \"out of memory\" in str(e).lower():\n",
    "                           print(\"Stopping generation due to device error.\")\n",
    "                           break\n",
    "\n",
    "                    print(\"-\" * 30)\n",
    "\n",
    "            print(\"\\n--- Generation Complete ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during model loading or generation setup: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate with base, unfinetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Base Model for Inference Comparison ---\n",
      "Base model name: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Using device: xpu\n",
      "XPU BF16 supported. Will load base model in bfloat16.\n",
      "Loading base tokenizer...\n",
      "Loading base model...\n",
      "Base model loaded successfully with dtype: torch.bfloat16 on device: xpu:0\n",
      "\n",
      "--- Generating Validation Set Outputs using BASE Model ---\n",
      "Using base model's max length: 131072\n",
      "\n",
      "--- Base Model Example 1 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "Simplify the expression $$(x^5+3x^2+3x^5)-(x^7+2x^2+6x^5).$$\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution (Base Model):\n",
      "First, I will simplify the expression by combining like terms. I will start by distributing the negative sign to each term in the second parenthesis. Then, I will combine like terms by adding or subtracting coefficients of the same variables.\n",
      "\n",
      "Wait, but I'm not sure if I'm doing this correctly. Let me try again.\n",
      "\n",
      "First, I will distribute the negative sign to each term in the second parenthesis. So, the expression becomes:\n",
      "\n",
      "x^5 + 3x^2 + 3x^5 - x^7 - 2x^2 - 6x^5.\n",
      "\n",
      "Now, I need to combine like terms. Let me list the terms:\n",
      "\n",
      "x^5, 3x^2, 3x^5, -x^7, -2x^2, -6x^5.\n",
      "\n",
      "I can group them by the power of x:\n",
      "\n",
      "x^5 terms: x^5 + 3x^5 - 6x^5.\n",
      "\n",
      "3x^2 terms: 3x^2 - 2x^2.\n",
      "\n",
      "And the x^7 term: -x^7.\n",
      "\n",
      "Now, let's compute each group:\n",
      "\n",
      "For x^5: 1x^5 + 3x^5 = 4x^5; 4x^5 - 6x^5 = -2x^5.\n",
      "\n",
      "For x^2: 3x^2 - 2x^2 = 1x^2.\n",
      "\n",
      "And the x^7 term remains as -x^7.\n",
      "\n",
      "So, putting it all together, the simplified expression is:\n",
      "\n",
      "- x^7 - 2x^5 + x^2.\n",
      "\n",
      "Wait, but I'm confused because in the original problem, the first parenthesis has 3x^5 and the second has 6x^5, so when subtracting, it's 3x^5 - 6x^5, which is -3x^5. But in my initial solution, I had -2x^5. Hmm, maybe I made a mistake in combining the x^5 terms.\n",
      "\n",
      "Wait, let me check again. The first parenthesis has x^5 + 3x^2 + 3x^5, which is x^5 + 3x^5 = 4x^5, plus 3x^2. Then, subtracting the second parenthesis, which is x^7 + 2x^2 + 6x^5. So, the subtraction would be:\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "Combining like terms, we find that  \\begin{align*}\n",
      "&(x^5+3x^2+3x^5)-(x^7+2x^2+6x^5)\\\\\n",
      "&\\qquad=(x^5+3x^5-6x^5)+(3x^2-2x^2)-x^7\\\\\n",
      "&\\qquad=\\boxed{-x^7-2x^5+x^2}.\n",
      "\\end{align*}\n",
      "------------------------------\n",
      "\n",
      "--- Base Model Example 2 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "Find the sum of the coefficients in the polynomial $3(x^{10} - x^7 + 2x^3 - x + 7) + 4(x^3 - 2x^2 - 5)$ when it is simplified.\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution (Base Model):\n",
      "First, expand each polynomial by distributing the coefficients.\n",
      "\n",
      "So, 3(x^{10} - x^7 + 2x^3 - x + 7) becomes 3x^{10} - 3x^7 + 6x^3 - 3x + 21.\n",
      "\n",
      "Then, 4(x^3 - 2x^2 - 5) becomes 4x^3 - 8x^2 - 20.\n",
      "\n",
      "Next, add the two expanded polynomials together.\n",
      "\n",
      "So, 3x^{10} - 3x^7 + 6x^3 - 3x + 21 + 4x^3 - 8x^2 - 20.\n",
      "\n",
      "Combine like terms.\n",
      "\n",
      "3x^{10} - 3x^7 + (6x^3 + 4x^3) + (-8x^2) + (-3x) + (21 - 20).\n",
      "\n",
      "Simplify.\n",
      "\n",
      "3x^{10} - 3x^7 + 10x^3 - 8x^2 - 3x + 1.\n",
      "\n",
      "Now, to find the sum of the coefficients, we can substitute x = 1.\n",
      "\n",
      "So, 3(1)^{10} - 3(1)^7 + 10(1)^3 - 8(1)^2 - 3(1) + 1.\n",
      "\n",
      "Calculate each term.\n",
      "\n",
      "3(1) - 3(1) + 10(1) - 8(1) - 3(1) + 1.\n",
      "\n",
      "Simplify.\n",
      "\n",
      "3 - 3 + 10 - 8 - 3 + 1.\n",
      "\n",
      "Compute the sum.\n",
      "\n",
      "3 - 3 is 0, 0 + 10 is 10, 10 - 8 is 2, 2 - 3 is -1, -1 + 1 is 0.\n",
      "\n",
      "So, the sum of the coefficients is 0.\n",
      "\n",
      "Wait, but I'm a bit confused. Is the sum of the coefficients equal to the value of the polynomial when x = 1? I think that's correct because substituting x = 1 into the polynomial gives the sum of the coefficients. Let me verify.\n",
      "\n",
      "Yes, for example, if you have a polynomial P(x) = a_n x^n + ... + a_1 x + a_0, then P(1) = a\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "The sum of the coefficients in $3(x^{10} - x^7 + 2x^3 - x + 7) + 4(x^3 - 2x^2 - 5)$ is $3 (1 - 1 + 2 - 1 + 7) + 4(1 - 2 - 5) = 3 \\cdot 8 + 4 \\cdot (-6) = \\boxed{0}$.  (The sum of the coefficients in a polynomial can be found by setting the variable to 1.)\n",
      "------------------------------\n",
      "\n",
      "--- Base Model Example 3 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "Expand the product ${6(x+2)(x+3)}$.\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution (Base Model):\n",
      "First, expand the product ${6(x+2)(x+3)}$.\n",
      "\n",
      "Step-by-step explanation:\n",
      "First, expand the product ${6(x+2)(x+3)}$.\n",
      "\n",
      "Wait, the user is asking to expand the product 6(x+2)(x+3). So, the first step is to expand the product (x+2)(x+3). Then, multiply the result by 6.\n",
      "\n",
      "So, let me compute (x+2)(x+3) first.\n",
      "\n",
      "Multiplying (x+2)(x+3):\n",
      "\n",
      "x * x = x²\n",
      "\n",
      "x * 3 = 3x\n",
      "\n",
      "2 * x = 2x\n",
      "\n",
      "2 * 3 = 6\n",
      "\n",
      "So, adding all these terms: x² + 3x + 2x + 6.\n",
      "\n",
      "Combine like terms: x² + 5x + 6.\n",
      "\n",
      "Then, multiply this by 6:\n",
      "\n",
      "6*(x² + 5x + 6) = 6x² + 30x + 36.\n",
      "\n",
      "So, the expanded form is 6x² + 30x + 36.\n",
      "\n",
      "Wait, but the user wrote \"First, expand the product 6(x+2)(x+3).\" So, is that the same as expanding 6*(x+2)*(x+3)? Or is it 6*(x+2)*(x+3) as a single product?\n",
      "\n",
      "Wait, in the original problem, it's written as 6(x+2)(x+3). So, that's 6 multiplied by (x+2) multiplied by (x+3). So, the first step is to expand (x+2)(x+3), which is x² + 5x + 6, then multiply by 6 to get 6x² + 30x + 36.\n",
      "\n",
      "Alternatively, if the user is asking to expand 6(x+2)(x+3), that is, 6*(x+2)*(x+3), so the first step is to expand (x+2)(x+3), then multiply by 6.\n",
      "\n",
      "So, the process is correct.\n",
      "\n",
      "But let me double-check.\n",
      "\n",
      "Alternatively, if the user is asking to expand 6(x+2)(x+3) as a single product, that is, 6*(x+2)*(x+3), so first, expand (x+2)(x+3) to get x² +\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "First, we use the distributive property to expand the first two factors:\n",
      "\n",
      "\\begin{align*}\n",
      "6(x+2)(x+3) &= (6\\cdot x + 6 \\cdot 2) (x+3)\\\\\n",
      "&=(6x+12)(x+3)\n",
      "\\end{align*}We use the distributive property again by adding the product of $6x+12$ and $x$ to the product of $6x+12$ and 3:\n",
      "\n",
      "\\begin{align*}\n",
      "(6x+12)(x+3) &= (6x+12) \\cdot x +(6x+12) \\cdot 3\\\\\n",
      "&= x(6x+12) + 3(6x+12)\n",
      "\\end{align*}We use the distributive property again and combine like terms:\n",
      "\n",
      "\\begin{align*}\n",
      "x(6x+12) + 3(6x+12) &= 6x^2 + 12x + 18x+ 36\\\\\n",
      "&= \\boxed{6x^2 + 30x + 36}\n",
      "\\end{align*}\n",
      "------------------------------\n",
      "\n",
      "--- Base Model Example 4 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "Evaluate $\\log_264$.\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution (Base Model):\n",
      "First, express 64 as a power of 2.\n",
      "\n",
      "64 is 2 to the 6th power.\n",
      "\n",
      "So, log base 2 of 64 is log base 2 of 2^6.\n",
      "\n",
      "Then, use the logarithm power rule.\n",
      "\n",
      "log_b(a^c) = c * log_b(a).\n",
      "\n",
      "So, log base 2 of 2^6 is 6 * log base 2 of 2.\n",
      "\n",
      "But log base 2 of 2 is 1.\n",
      "\n",
      "So, 6 * 1 is 6.\n",
      "\n",
      "Therefore, log base 2 of 64 is 6.\n",
      "\n",
      "Wait, but I'm a bit confused. Let me think again.\n",
      "\n",
      "Wait, is log base 2 of 2 equal to 1? Yes, because 2^1 = 2.\n",
      "\n",
      "So, that part is correct.\n",
      "\n",
      "But wait, is there another way to think about this? Maybe using change of base formula?\n",
      "\n",
      "Alternatively, I can think of 2^x = 64, and solve for x.\n",
      "\n",
      "But 2^6 is 64, so x is 6.\n",
      "\n",
      "Alternatively, using logarithm properties, since log_b(a) = ln(a)/ln(b), but that might be more complicated.\n",
      "\n",
      "Wait, but in this case, since 64 is a power of 2, it's straightforward.\n",
      "\n",
      "But let me try to think of another approach, maybe using exponents.\n",
      "\n",
      "Suppose I have log_2(64) = y.\n",
      "\n",
      "That means 2^y = 64.\n",
      "\n",
      "I know that 2^6 = 64, so y must be 6.\n",
      "\n",
      "Therefore, log_2(64) = 6.\n",
      "\n",
      "Alternatively, using the definition of logarithm.\n",
      "\n",
      "The logarithm base 2 of 64 is the exponent you need to raise 2 to in order to get 64.\n",
      "\n",
      "Since 2^6 = 64, the exponent is 6.\n",
      "\n",
      "So, that's consistent.\n",
      "\n",
      "Wait, but I'm a bit confused because I remember sometimes log base 2 of 64 is 6, but sometimes I get mixed up with other bases.\n",
      "\n",
      "Wait, let me check with another example.\n",
      "\n",
      "Suppose I have log_2(8). 8 is 2^3, so log_2(8) should be 3.\n",
      "\n",
      "Similarly, log_2(16) is 4, because 2^4=16.\n",
      "\n",
      "So, in this case, 6\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "We have $2^6=64$, so $\\log_2 64 = \\boxed{6}$.\n",
      "------------------------------\n",
      "\n",
      "--- Base Model Example 5 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "If $a+b=7$ and $a^3+b^3=42$, what is the value of the sum $\\dfrac{1}{a}+\\dfrac{1}{b}$?  Express your answer as a common fraction.\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution (Base Model):\n",
      "First, we know that $a + b = 7$ and $a^3 + b^3 = 42$.\n",
      "\n",
      "We can use the identity for the sum of cubes: $a^3 + b^3 = (a + b)(a^2 - ab + b^2)$.\n",
      "\n",
      "Substituting the known values, we get:\n",
      "\n",
      "$42 = 7(a^2 - ab + b^2)$.\n",
      "\n",
      "Dividing both sides by 7, we have:\n",
      "\n",
      "$a^2 - ab + b^2 = 6$.\n",
      "\n",
      "Then, we can express $a^2 + b^2$ in terms of $(a + b)^2$:\n",
      "\n",
      "$a^2 + b^2 = (a + b)^2 - 2ab = 49 - 2ab$.\n",
      "\n",
      "Substituting this into the previous equation:\n",
      "\n",
      "$49 - 2ab - ab = 6$.\n",
      "\n",
      "Simplifying:\n",
      "\n",
      "$49 - 3ab = 6$.\n",
      "\n",
      "Subtracting 49 from both sides:\n",
      "\n",
      "$-3ab = -43$.\n",
      "\n",
      "Dividing both sides by -3:\n",
      "\n",
      "$ab = \\dfrac{43}{3}$.\n",
      "\n",
      "Now, we can find the sum $\\dfrac{1}{a} + \\dfrac{1}{b}$.\n",
      "\n",
      "Expressing this as a single fraction:\n",
      "\n",
      "$\\dfrac{1}{a} + \\dfrac{1}{b} = \\dfrac{a + b}{ab}$.\n",
      "\n",
      "Substituting the known values:\n",
      "\n",
      "$\\dfrac{7}{\\dfrac{43}{3}} = \\dfrac{7 \\times 3}{43} = \\dfrac{21}{43}$.\n",
      "\n",
      "Wait, but the solution says the answer is 21/43, but in the problem statement, the user says that the answer is 21/43, but in the solution, the user says that the answer is 21/43, but in the problem statement, the user says that the answer is 21/43, but in the solution, the user says that the answer is 21/43, but in the problem statement, the user says that the answer is 21/43, but in the solution, the user says that the answer is 21/43, but in the problem statement, the user says that the answer is 21/43,\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "Cube both sides of $a+b=7$ to find \\[\n",
      "a^3+3a^2b+3ab^2+b^3=343.\n",
      "\\] Substitute 42 for $a^3+b^3$ and factor $3ab$ out of the remaining two terms. \\begin{align*}\n",
      "42+3ab(a+b)&=343 \\implies \\\\\n",
      "3ab(a+b)&=301 \\implies \\\\\n",
      "3ab(7)&=301 \\implies \\\\\n",
      "3ab&=43 \\implies \\\\\n",
      "ab&=\\frac{43}{3}.\n",
      "\\end{align*} Finally, $\\frac{1}{a}+\\frac{1}{b}=\\frac{a+b}{ab}=\\frac{7}{43/3}=\\boxed{\\frac{21}{43}}$.\n",
      "------------------------------\n",
      "\n",
      "--- Base Model Example 6 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "Compute $(34-10)+(20-9)+(55-10)$ in your head.\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution (Base Model):\n",
      "First, compute each expression inside the parentheses:\n",
      "34 - 10 = 24\n",
      "20 - 9 = 11\n",
      "55 - 10 = 45\n",
      "\n",
      "Then, add the results:\n",
      "24 + 11 = 35\n",
      "35 + 45 = 80\n",
      "\n",
      "So, the final answer is 80.\n",
      "Alright, let me try to solve this problem step by step. I need to compute the expression (34 - 10) + (20 - 9) + (55 - 10) in my head. Hmm, okay. Let me break it down.\n",
      "\n",
      "First, I see there are three separate expressions inside the parentheses that I need to compute. So, I should handle each one individually before adding them all together. That makes sense because if I try to add them all at once, it might be confusing.\n",
      "\n",
      "Starting with the first one: 34 - 10. Hmm, 34 minus 10. I know that subtracting 10 from a number is like taking one ten away. So, 34 is 3 tens and 4 ones. If I take away one ten, that leaves me with 2 tens and 4 ones, which is 24. Okay, that seems straightforward.\n",
      "\n",
      "Next, the second expression: 20 - 9. Hmm, 20 minus 9. I'm a bit more confident with this one. 20 is 2 tens, and 9 is one less than 10. So, if I take away 9 from 20, I can think of it as taking away 10 and then adding back 1. So, 20 minus 10 is 10, and then adding back 1 gives me 11. So, 20 - 9 equals 11. Got that.\n",
      "\n",
      "Now, the third expression: 55 - 10. Similar to the first one. 55 is 5 tens and 5 ones. Subtracting 10 would just take away one ten, leaving me with 4 tens and 5 ones, which is 45. So, 55 - 10 is 45. That seems right.\n",
      "\n",
      "Okay, so now I have the three results: 24, 11, and 45. I need to add them all together. Let me write\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "Rearranging the terms, we get $(34+55-9)+(20-10-10)=80+0=\\boxed{80}$.\n",
      "------------------------------\n",
      "\n",
      "--- Base Model Example 7 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "What is the value of $x$ in the equation $(17^6-17^5)\\div16=17^x$?\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution (Base Model):\n",
      "First, factor out 17^5 from the numerator.\n",
      "\n",
      "So, (17^5)(17 - 1) = 17^x.\n",
      "\n",
      "Then, simplify 17 - 1 to 16.\n",
      "\n",
      "So, (17^5)(16) = 17^x.\n",
      "\n",
      "Then, divide both sides by 16.\n",
      "\n",
      "So, 17^5 = 17^x / 16.\n",
      "\n",
      "Wait, that doesn't seem right. Maybe I should have divided both sides by 16 instead.\n",
      "\n",
      "Wait, let me try again.\n",
      "\n",
      "Original equation: (17^6 - 17^5) / 16 = 17^x.\n",
      "\n",
      "Factor out 17^5: 17^5(17 - 1) / 16 = 17^x.\n",
      "\n",
      "Simplify 17 - 1: 16.\n",
      "\n",
      "So, 17^5 * 16 / 16 = 17^x.\n",
      "\n",
      "Cancel out the 16: 17^5 = 17^x.\n",
      "\n",
      "Therefore, x = 5.\n",
      "\n",
      "Wait, but in my first attempt, I thought I had 17^5 = 17^x / 16, which would imply x = 5, but that seems conflicting.\n",
      "\n",
      "Wait, maybe I made a mistake in the first step.\n",
      "\n",
      "Wait, let me go through it again.\n",
      "\n",
      "Original equation: (17^6 - 17^5) / 16 = 17^x.\n",
      "\n",
      "Factor numerator: 17^5(17 - 1) = 17^5 * 16.\n",
      "\n",
      "So, (17^5 * 16) / 16 = 17^x.\n",
      "\n",
      "Simplify: 17^5 = 17^x.\n",
      "\n",
      "Therefore, x = 5.\n",
      "\n",
      "Wait, so why did I think I had 17^5 = 17^x / 16? Maybe I confused the steps.\n",
      "\n",
      "Wait, perhaps I should have divided both sides by 16 instead of moving it to the other side.\n",
      "\n",
      "Let me try that.\n",
      "\n",
      "Starting again:\n",
      "\n",
      "(17^6 - 17^5) / 16 = 17^x.\n",
      "\n",
      "Factor numerator: 17^5(17 - 1) = 17^5 * 16.\n",
      "\n",
      "So, (\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "Factoring a $17^5$ from the two terms in the parenthesis, we get $17^5(17-1)\\div16=17^5$. Thus, $x=\\boxed{5}$.\n",
      "------------------------------\n",
      "\n",
      "--- Base Model Example 8 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "What is the product of the coordinates of the midpoint of a line segment with endpoints at $(1,1)$ and $(-7,5)$?\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution (Base Model):\n",
      "First, find the midpoint using the midpoint formula.\n",
      "\n",
      "Midpoint formula: $M = \\left( \\frac{x_1 + x_2}{2}, \\frac{y_1 + y_2}{2} \\right)$\n",
      "\n",
      "So, plugging in the values:\n",
      "\n",
      "$M = \\left( \\frac{1 + (-7)}{2}, \\frac{1 + 5}{2} \\right) = \\left( \\frac{-6}{2}, \\frac{6}{2} \\right) = (-3, 3)$\n",
      "\n",
      "Then, the product of the coordinates is $(-3) \\times 3 = -9$.\n",
      "\n",
      "Wait, but I just realized that the problem says \"the product of the coordinates of the midpoint.\" So, is it just multiplying the x and y coordinates, or is there another step?\n",
      "\n",
      "Wait, no, I think that's what it is. So, the coordinates are (-3, 3), so the product is (-3) * 3 = -9.\n",
      "\n",
      "But hold on, let me double-check my calculations.\n",
      "\n",
      "First, the x-coordinate of the midpoint: (1 + (-7))/2 = (-6)/2 = -3. That seems correct.\n",
      "\n",
      "The y-coordinate of the midpoint: (1 + 5)/2 = 6/2 = 3. That also seems correct.\n",
      "\n",
      "So, the midpoint is indeed (-3, 3). Then, the product of the coordinates is (-3) * 3 = -9.\n",
      "\n",
      "Wait, but the problem says \"the product of the coordinates of the midpoint.\" So, is it just multiplying the x and y coordinates, or is there another interpretation?\n",
      "\n",
      "Wait, maybe I'm overcomplicating. The coordinates are (-3, 3), so the product is (-3) * 3 = -9. So, the answer is -9.\n",
      "\n",
      "But let me think again. Is there a different way to interpret the problem? Maybe the product of the x-coordinate and the y-coordinate, which is exactly what I did. So, I think my answer is correct.\n",
      "\n",
      "But just to be thorough, let me recompute the midpoint.\n",
      "\n",
      "Midpoint formula: ( (x1 + x2)/2, (y1 + y2)/2 )\n",
      "\n",
      "Given points (1,1) and (-7,5):\n",
      "\n",
      "x1 = 1, x2 = -7\n",
      "\n",
      "y1 = 1, y2 = 5\n",
      "\n",
      "So, x_mid = (1 + (-7\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "We see that the midpoint has coordinates $\\left(\\frac{1 + (-7)}{2}, \\frac{1+5}{2}\\right) = (-3, 3)$.  Thus our desired answer is $-3\\cdot 3 = \\boxed{-9}$.\n",
      "------------------------------\n",
      "\n",
      "--- Base Model Example 9 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "Define the operation $\\star$ by $a \\star b = (a + b)b$. What is $(3\\star5) - (5\\star3)$?\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution (Base Model):\n",
      "First, compute $3\\star5$:\n",
      "$3\\star5 = (3 + 5) \\times 5 = 8 \\times 5 = 40$.\n",
      "\n",
      "Then, compute $5\\star3$:\n",
      "$5\\star3 = (5 + 3) \\times 3 = 8 \\times 3 = 24$.\n",
      "\n",
      "Subtracting, $40 - 24 = 16$.\n",
      "\n",
      "So, the answer is 16.\n",
      "\n",
      "Wait, but in the solution, it says that $3\\star5$ is 40 and $5\\star3$ is 24, so the difference is 16. But wait, in the problem statement, it says that the operation is defined as $a \\star b = (a + b)b$. So, is that correct?\n",
      "\n",
      "Wait, let me check the definition again. It says $a \\star b = (a + b)b$. So, that is, you add a and b, then multiply by b. So, in the first case, 3 star 5 is (3 + 5) times 5, which is 8 times 5, which is 40. Then, 5 star 3 is (5 + 3) times 3, which is 8 times 3, which is 24. So, 40 minus 24 is 16. So, the answer is 16.\n",
      "\n",
      "But wait, in the problem statement, it says \"What is (3 star 5) minus (5 star 3)?\" So, that's 40 minus 24, which is 16. So, the answer is 16.\n",
      "\n",
      "But wait, in the solution, it says \"Wait, but in the solution, it says that 3 star 5 is 40 and 5 star 3 is 24, so the difference is 16.\" So, that's correct.\n",
      "\n",
      "But wait, hold on, is the operation commutative? Because if it's not, then the difference would be non-zero. In this case, 3 star 5 is 40, 5 star 3 is 24, so 40 - 24 is 16. So, the answer is 16.\n",
      "\n",
      "But wait, let me think again. Is the operation defined as (a + b) multiplied by b?\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "Since $3 \\star 5 = (3 + 5)5 = 8\\cdot 5 = 40$ and $5 \\star 3 = (5 + 3)3 = 8\\cdot 3 = 24$, we have \\[\n",
      "3\\star5 - 5\\star3 = 40 - 24 = \\boxed{16}.\n",
      "\\]\n",
      "------------------------------\n",
      "\n",
      "--- Base Model Example 10 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "If $\\sqrt{400}=\\sqrt{81}+\\sqrt{n}$, then what is the value of $n$?\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution (Base Model):\n",
      "First, I will compute the square roots.\n",
      "\n",
      "$\\sqrt{400} = 20$, and $\\sqrt{81} = 9$.\n",
      "\n",
      "So, substituting these values into the equation:\n",
      "\n",
      "$20 = 9 + \\sqrt{n}$.\n",
      "\n",
      "Then, subtracting 9 from both sides:\n",
      "\n",
      "$11 = \\sqrt{n}$.\n",
      "\n",
      "Finally, squaring both sides:\n",
      "\n",
      "$121 = n$.\n",
      "\n",
      "So, the value of $n$ is 121.\n",
      "\n",
      "Wait, but I'm a bit confused. Let me think again. Is there another way to approach this problem? Maybe by squaring both sides from the beginning?\n",
      "\n",
      "Let me try that. Starting with the original equation:\n",
      "\n",
      "$\\sqrt{400} = \\sqrt{81} + \\sqrt{n}$.\n",
      "\n",
      "If I square both sides, I should get:\n",
      "\n",
      "$(\\sqrt{400})^2 = (\\sqrt{81} + \\sqrt{n})^2$.\n",
      "\n",
      "Calculating the left side:\n",
      "\n",
      "$400 = (\\sqrt{81} + \\sqrt{n})^2$.\n",
      "\n",
      "Expanding the right side:\n",
      "\n",
      "$400 = (\\sqrt{81})^2 + 2 \\cdot \\sqrt{81} \\cdot \\sqrt{n} + (\\sqrt{n})^2$.\n",
      "\n",
      "Simplifying each term:\n",
      "\n",
      "$400 = 81 + 2 \\cdot \\sqrt{81n} + n$.\n",
      "\n",
      "So, combining like terms:\n",
      "\n",
      "$400 = 81 + n + 2\\sqrt{81n}$.\n",
      "\n",
      "Subtracting 81 from both sides:\n",
      "\n",
      "$319 = n + 2\\sqrt{81n}$.\n",
      "\n",
      "Hmm, this seems a bit more complicated. Maybe I can let $x = \\sqrt{n}$, so that $n = x^2$. Then, substituting into the equation:\n",
      "\n",
      "$319 = x^2 + 2\\sqrt{81x^2}$.\n",
      "\n",
      "Simplify the square root:\n",
      "\n",
      "$\\sqrt{81x^2} = 9x$.\n",
      "\n",
      "So, substituting back:\n",
      "\n",
      "$319 = x^2 + 2 \\cdot 9x$.\n",
      "\n",
      "Which simplifies to:\n",
      "\n",
      "$319 = x^2 + 18x$.\n",
      "\n",
      "Bringing all terms to one side:\n",
      "\n",
      "$x^2 + 18x - 319 = 0$.\n",
      "\n",
      "Now, I can\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "Not to be fooled by the square roots, we rewrite the equation as $20=9+\\sqrt{n}.$ Thus, $\\sqrt{n}=11$ and $n=\\boxed{121}.$\n",
      "------------------------------\n",
      "\n",
      "--- Base Model Generation Complete ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Loading Base Model for Inference Comparison ---\")\n",
    "print(f\"Base model name: {MODEL_NAME}\")\n",
    "\n",
    "# Check for device and bfloat16 support again (could be refactored)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "dtype_to_load = None\n",
    "if device.type == 'cuda' and torch.cuda.is_bf16_supported():\n",
    "    print(\"CUDA BF16 supported. Will load base model in bfloat16.\")\n",
    "    dtype_to_load = torch.bfloat16\n",
    "elif device.type == 'xpu' and hasattr(torch.xpu, 'is_bf16_supported') and torch.xpu.is_bf16_supported():\n",
    "     print(\"XPU BF16 supported. Will load base model in bfloat16.\")\n",
    "     dtype_to_load = torch.bfloat16\n",
    "else:\n",
    "     print(\"BF16 not supported or device is CPU. Loading base model in default precision.\")\n",
    "\n",
    "try:\n",
    "    # Load the tokenizer for the base model\n",
    "    print(\"Loading base tokenizer...\")\n",
    "    base_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    if base_tokenizer.pad_token is None:\n",
    "        base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "        print(\"Set pad_token = eos_token for base tokenizer.\")\n",
    "\n",
    "    # Load the base model\n",
    "    print(\"Loading base model...\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=dtype_to_load,\n",
    "        device_map=device if device.type != 'cpu' else None\n",
    "        # device_map=\"auto\" # Use if accelerate is installed\n",
    "    )\n",
    "    # if device.type != 'cpu' and device_map is None:\n",
    "    #     base_model.to(device)\n",
    "\n",
    "    print(f\"Base model loaded successfully with dtype: {base_model.dtype} on device: {base_model.device}\")\n",
    "\n",
    "    # Set to evaluation mode\n",
    "    base_model.eval()\n",
    "\n",
    "    # --- Generation using Base Model Starts Here ---\n",
    "    print(\"\\n--- Generating Validation Set Outputs using BASE Model ---\")\n",
    "\n",
    "    # Get the first 10 examples from the original validation set\n",
    "    num_examples_to_generate = 10\n",
    "    if 'validation' not in dataset:\n",
    "         print(\"Error: 'validation' split not found in the dataset object.\")\n",
    "    else:\n",
    "        # Assuming 'dataset' still holds the original data structure\n",
    "        validation_subset = dataset['validation'].select(range(min(num_examples_to_generate, len(dataset['validation']))))\n",
    "        input_column = 'input' # Assuming column alignment happened for the 'dataset' object earlier\n",
    "\n",
    "        if input_column not in validation_subset.features:\n",
    "            print(f\"Error: Input column '{input_column}' not found in validation subset features: {validation_subset.features}\")\n",
    "        else:\n",
    "            # Get model's max length if possible\n",
    "            try:\n",
    "                MODEL_MAX_LENGTH = base_model.config.max_position_embeddings\n",
    "                print(f\"Using base model's max length: {MODEL_MAX_LENGTH}\")\n",
    "            except AttributeError:\n",
    "                print(\"Warning: Could not get max_position_embeddings. Using default max_length=4096.\")\n",
    "                MODEL_MAX_LENGTH = 4096 # Fallback\n",
    "\n",
    "            for i, example in enumerate(validation_subset):\n",
    "                print(f\"\\n--- Base Model Example {i+1} ---\")\n",
    "                prompt = f\"Problem:\\n{example[input_column]}\\n\\nSolution:\\n\"\n",
    "                print(f\"Input Prompt (truncated to 500 chars):\\n{prompt[:500]}...\")\n",
    "\n",
    "                # Use the base tokenizer and model\n",
    "                inputs = base_tokenizer(\n",
    "                    prompt,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    max_length=MODEL_MAX_LENGTH\n",
    "                )\n",
    "                inputs = inputs.to(base_model.device)\n",
    "\n",
    "                try:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = base_model.generate(\n",
    "                            **inputs,\n",
    "                            max_new_tokens=512,  # Keep consistent with other inference run\n",
    "                            pad_token_id=base_tokenizer.eos_token_id,\n",
    "                            eos_token_id=base_tokenizer.eos_token_id,\n",
    "                            do_sample=False,\n",
    "                            num_beams=1,\n",
    "                        )\n",
    "\n",
    "                    generated_ids = outputs[0, inputs['input_ids'].shape[1]:]\n",
    "                    generated_text = base_tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "                    print(f\"\\nGenerated Solution (Base Model):\\n{generated_text.strip()}\")\n",
    "\n",
    "                    if 'output_answer' in example:\n",
    "                        print(f\"\\nActual Solution (from dataset):\\n{example['output_answer']}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError during base model generation for Example {i+1}: {e}\")\n",
    "                    if \"UR_RESULT_ERROR_DEVICE_LOST\" in str(e) or \"out of memory\" in str(e).lower():\n",
    "                       print(\"Stopping base model generation due to device error.\")\n",
    "                       break\n",
    "\n",
    "                print(\"-\" * 30)\n",
    "\n",
    "        print(\"\\n--- Base Model Generation Complete ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during base model loading or generation setup: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration for Non-Math Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Setting up for Non-Math Generation Test (Completion/Few-Shot Prompts) ---\n",
      "Using device: xpu\n",
      "XPU BF16 supported. Will load models in bfloat16.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Setting up for Non-Math Generation Test (Completion/Few-Shot Prompts) ---\")\n",
    "\n",
    "# Prompts designed for base model completion or few-shot learning\n",
    "# Note: The model might still hallucinate or go off-topic, but this format gives it a better chance.\n",
    "non_math_prompts_base_style = [\n",
    "    # Simple Completion\n",
    "    \"Photosynthesis is the process by which green plants use sunlight, water, and carbon dioxide to create their own food. In simple terms, this means\",\n",
    "    # Start of a Narrative\n",
    "    \"It was a dark and rainy night in the city. The neon lights reflected off the wet pavement as\",\n",
    "    # Few-Shot Q&A\n",
    "    \"Q: What is the capital of France?\\nA: Paris.\\n\\nQ: What is the capital of Spain?\\nA: Madrid.\\n\\nQ: What is the capital of Germany?\\nA:\",\n",
    "    # Simple Completion (already suitable)\n",
    "    \"The old house stood on a hill overlooking\",\n",
    "    # Few-Shot List Completion\n",
    "    \"Here is a list of common household pets:\\n1. Cat\\n2. Dog\\n3.\",\n",
    "    # Start of a Description\n",
    "    \"Trying to describe the color blue to someone who cannot see is difficult. One might say blue feels like\",\n",
    "    # Few-Shot Generation Example\n",
    "    \"Recipe Title: Quick Lemon Herb Chicken\\nRecipe Title: Spicy Tomato and Bean Soup\\nRecipe Title:\",\n",
    "    # Few-Shot Sentence Example\n",
    "    \"Sentence using 'ubiquitous': Mobile phones have become ubiquitous in modern society.\\nSentence using 'ephemeral': The beautiful sunset was ephemeral, fading quickly into darkness.\\nSentence using 'serendipity':\",\n",
    "    # Few-Shot Q&A\n",
    "    \"Q: What are the benefits of recycling?\\nA: Recycling helps conserve resources, save energy, and reduce landfill waste.\\n\\nQ: What are the benefits of regular exercise?\\nA:\",\n",
    "    # Start of a Poem\n",
    "    \"A short poem about the moon:\\n\\nSilver light on silent seas,\"\n",
    "]\n",
    "max_new_tokens_non_math = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check BF16 support\n",
    "dtype_to_load = None\n",
    "if device.type == 'cuda' and torch.cuda.is_bf16_supported():\n",
    "    print(\"CUDA BF16 supported. Will load models in bfloat16.\")\n",
    "    dtype_to_load = torch.bfloat16\n",
    "elif device.type == 'xpu' and hasattr(torch.xpu, 'is_bf16_supported') and torch.xpu.is_bf16_supported():\n",
    "     print(\"XPU BF16 supported. Will load models in bfloat16.\")\n",
    "     dtype_to_load = torch.bfloat16\n",
    "else:\n",
    "     print(\"BF16 not supported or device is CPU. Loading in default precision.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusable Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_general_response_base(model, tokenizer, prompt, max_new_tokens, device):\n",
    "    model.eval()\n",
    "    try:\n",
    "        max_len = model.config.max_position_embeddings\n",
    "    except AttributeError:\n",
    "        max_len = 4096 # Fallback\n",
    "    # Leave buffer room: max context - generation length - prompt buffer\n",
    "    input_max_len = max(0, max_len - max_new_tokens - 20)\n",
    "\n",
    "    # Important: Ensure prompt itself isn't truncated too much\n",
    "    if len(tokenizer.encode(prompt)) > input_max_len:\n",
    "         print(f\"  Warning: Prompt might be truncated significantly (Prompt tokens > {input_max_len}).\")\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=input_max_len)\n",
    "    inputs = inputs.to(device)\n",
    "    response_text = \"Error during generation.\"\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id, # Model might learn to stop itself\n",
    "                # Use sampling, maybe slightly less creative for completion\n",
    "                do_sample=True,\n",
    "                top_k=40,\n",
    "                top_p=0.9,\n",
    "                temperature=0.65 # Slightly lower temperature\n",
    "            )\n",
    "        # Decode only the newly generated tokens\n",
    "        generated_ids = outputs[0, inputs['input_ids'].shape[1]:]\n",
    "        response_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n   Error generating response for prompt '{prompt[:50]}...': {e}\")\n",
    "        # Handle potential OOM during generation specifically if needed\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "             print(\"   OOM Error during generation. Try reducing max_new_tokens or using lower precision/quantization.\")\n",
    "             return \"[OOM Error during generation]\"\n",
    "    return response_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate with Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Non-Math Outputs with FINE-TUNED Model (Base-Style Prompts) ---\n",
      "Loading fine-tuned tokenizer from finetuned_DeepSeek-R1-Distill-Qwen-1.5B_length_val_modified_lila_MATH_algebra_crowdsourced/final_model...\n",
      "Loading fine-tuned model from finetuned_DeepSeek-R1-Distill-Qwen-1.5B_length_val_modified_lila_MATH_algebra_crowdsourced/final_model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model loaded: dtype=torch.bfloat16, device=xpu:0\n",
      "\n",
      "Prompt 1: Photosynthesis is the process by which green plants use sunlight, water, and carbon dioxide to create their own food. In simple terms, this means\n",
      "Fine-Tuned Model Response:\n",
      "that green plants are using the sun, water, and air to make their own food. This process is called photosynthesis, and it's the first thing plants do in the morning. It's also called the light process, and it's the second process in the morning. The word photosynthesis comes from the Greek word \"phophotos\" which means \"light\" and \"synthesis\" which means \"to make.\" So, photosynthesis is the process of making food using the light. This\n",
      "--------------------\n",
      "\n",
      "Prompt 2: It was a dark and rainy night in the city. The neon lights reflected off the wet pavement as\n",
      "Fine-Tuned Model Response:\n",
      "I walked home. It was 7:30 p.m. and I had just finished a math competition. Now, I have to figure out how many people are on my street. I counted 8 people wearing red on my street and 12 people wearing blue. Then, I realized that the number of people wearing red was 1/2 the number of people wearing blue. How many people are on my street?\n",
      "\n",
      "Solution:\n",
      "We have 1/2 the number of people wearing\n",
      "--------------------\n",
      "\n",
      "Prompt 3: Q: What is the capital of France?\n",
      "A: Paris.\n",
      "\n",
      "Q: What is the capital of Spain?\n",
      "A: Madrid.\n",
      "\n",
      "Q: What is the capital of Germany?\n",
      "A:\n",
      "Fine-Tuned Model Response:\n",
      "Berlin.\n",
      "\n",
      "Q: What is the capital of Italy?\n",
      "A: Rome.\n",
      "\n",
      "Q: What is the capital of the United Kingdom?\n",
      "A: London.\n",
      "\n",
      "Q: What is the capital of the United States?\n",
      "A: Washington, D.C.\n",
      "\n",
      "Q: What is the capital of China?\n",
      "A: Beijing.\n",
      "\n",
      "Q: What is the capital of Japan?\n",
      "A: Kyoto.\n",
      "\n",
      "Q: What is the capital of the Czech Republic?\n",
      "A: Prague.\n",
      "\n",
      "Q: What is the capital of the Netherlands?\n",
      "A\n",
      "--------------------\n",
      "\n",
      "Prompt 4: The old house stood on a hill overlooking\n",
      "Fine-Tuned Model Response:\n",
      "the lake. The ground between the house and the lake was sloping, with a slope of 25 degrees. The height of the house was 18 feet. What was the distance from the base of the house to the point directly across the lake? Express your answer to the nearest foot.\n",
      "\n",
      "Solution:\n",
      "We can use the tangent function to find the distance from the base of the house to the point directly across the lake. The tangent of 25 degrees is equal to the opposite side\n",
      "--------------------\n",
      "\n",
      "Prompt 5: Here is a list of common household pets:\n",
      "1. Cat\n",
      "2. Dog\n",
      "3.\n",
      "Fine-Tuned Model Response:\n",
      "Horse\n",
      "4. Fish\n",
      "5. Bird\n",
      "6. Hamster\n",
      "7. Rat\n",
      "8. Dog\n",
      "9. Cat\n",
      "10. Bird\n",
      "\n",
      "What is the probability that a randomly selected pet from this list is a dog? Express your answer as a fraction in simplest form.\n",
      "\n",
      "To find the probability, we can divide the number of favorable outcomes by the total number of possible outcomes. In this case, the favorable outcomes are the number of dogs, and the total number of possible outcomes is\n",
      "--------------------\n",
      "\n",
      "Prompt 6: Trying to describe the color blue to someone who cannot see is difficult. One might say blue feels like\n",
      "Fine-Tuned Model Response:\n",
      "a color of the sky or a shade of blue, but that's not quite precise. It's more like a light blue, a pastel shade of blue, or a deep blue. Maybe it's a shade of a darker blue or a shade of a lighter blue. It's hard to say for sure, but I think it's a pastel or a deep blue.\n",
      "\n",
      "I think the color blue is a shade of blue. It has a lightness of 65% and a saturation\n",
      "--------------------\n",
      "\n",
      "Prompt 7: Recipe Title: Quick Lemon Herb Chicken\n",
      "Recipe Title: Spicy Tomato and Bean Soup\n",
      "Recipe Title:\n",
      "Fine-Tuned Model Response:\n",
      "Sweet Potato and Carrot Casserole\n",
      "Recipe Title: Garlic Bread and Bread\n",
      "Recipe Title: Chicken and Veggie Stuffed Meatballs\n",
      "Recipe Title: Black Forest Chocolate and Apple Pie\n",
      "Recipe Title: Creamy Apple Pie and Cinnamon Sauce\n",
      "Recipe Title: Creamy Mac and cheese and Feta\n",
      "Recipe Title: Roasted Beef and Pecan\n",
      "\n",
      "In the first recipe, the ingredients are listed as \"200g ground chicken, 500g boneless bone\n",
      "--------------------\n",
      "\n",
      "Prompt 8: Sentence using 'ubiquitous': Mobile phones have become ubiquitous in modern society.\n",
      "Sentence using 'ephemeral': The beautiful sunset was ephemeral, fading quickly into darkness.\n",
      "Sentence using 'serendipity':\n",
      "Fine-Tuned Model Response:\n",
      "A serendipitous encounter meant the end of an unlikely journey.\n",
      "Sentence using 'irreplaceable': The irreplaceable value of a diamond lies in its uniqueness.\n",
      "Sentence using 'intimate': The intimate bond between two people has never been strengthened.\n",
      "Sentence using 'uncommon': The uncommon occurrence of such a rare event has made it even rarer.\n",
      "Sentence using 'unlike': The unlike appearance of this painting made it stand out.\n",
      "Sentence using 'ubiquitous': The ubiquity of\n",
      "--------------------\n",
      "\n",
      "Prompt 9: Q: What are the benefits of recycling?\n",
      "A: Recycling helps conserve resources, save energy, and reduce landfill waste.\n",
      "\n",
      "Q: What are the benefits of regular exercise?\n",
      "A:\n",
      "Fine-Tuned Model Response:\n",
      "Regular exercise helps burn calories, improve cardiovascular health, and reduce the risk of chronic diseases.\n",
      "\n",
      "Q: What are the benefits of saving money?\n",
      "A: Saving money helps build financial security, improves credit scores, and reduces the cost of future purchases.\n",
      "\n",
      "Q: What are the benefits of using renewable energy?\n",
      "A: Using renewable energy helps reduce carbon emissions, lowers energy costs, and promotes sustainability.\n",
      "\n",
      "Q: What are the benefits of attending a community event?\n",
      "A: Attending a community event helps celebrate traditions\n",
      "--------------------\n",
      "\n",
      "Prompt 10: A short poem about the moon:\n",
      "\n",
      "Silver light on silent seas,\n",
      "Fine-Tuned Model Response:\n",
      "moonlight in my hand,  \n",
      "A crescent full of wonder, yet it's a shadow.  \n",
      "The stars are in my sight, yet I don't see them,  \n",
      "A night when the earth sleeps, yet I can't sleep.  \n",
      "What's the secret of the moon?  \n",
      "What's the secret of the night?  \n",
      "What's the secret of the day?  \n",
      "What's the secret of the year?  \n",
      "What's the secret of the world?  \n",
      "What's the secret of\n",
      "--------------------\n",
      "Deleting fine-tuned model and tokenizer from memory...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Generating Non-Math Outputs with FINE-TUNED Model (Base-Style Prompts) ---\")\n",
    "SAVED_MODEL_PATH = f\"{OUTPUT_DIR}/final_model\"\n",
    "\n",
    "fine_tuned_model = None\n",
    "fine_tuned_tokenizer = None\n",
    "\n",
    "if not os.path.isdir(SAVED_MODEL_PATH):\n",
    "    print(f\"Error: Fine-tuned model directory not found at {SAVED_MODEL_PATH}. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        print(f\"Loading fine-tuned tokenizer from {SAVED_MODEL_PATH}...\")\n",
    "        fine_tuned_tokenizer = AutoTokenizer.from_pretrained(SAVED_MODEL_PATH, trust_remote_code=True)\n",
    "        if fine_tuned_tokenizer.pad_token is None: fine_tuned_tokenizer.pad_token = fine_tuned_tokenizer.eos_token\n",
    "\n",
    "        print(f\"Loading fine-tuned model from {SAVED_MODEL_PATH}...\")\n",
    "        fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "            SAVED_MODEL_PATH,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=dtype_to_load,\n",
    "            device_map=device if device.type != 'cpu' else None\n",
    "        )\n",
    "        print(f\"Fine-tuned model loaded: dtype={fine_tuned_model.dtype}, device={fine_tuned_model.device}\")\n",
    "\n",
    "        for i, prompt in enumerate(non_math_prompts_base_style):\n",
    "            print(f\"\\nPrompt {i+1}: {prompt}\")\n",
    "            response = generate_general_response_base(fine_tuned_model, fine_tuned_tokenizer, prompt, max_new_tokens_non_math, device)\n",
    "            print(f\"Fine-Tuned Model Response:\\n{response}\")\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load or run inference with fine-tuned model: {e}\")\n",
    "\n",
    "    # Clean up fine-tuned model\n",
    "    print(\"Deleting fine-tuned model and tokenizer from memory...\")\n",
    "    del fine_tuned_model\n",
    "    del fine_tuned_tokenizer\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "    elif hasattr(torch.xpu, 'empty_cache') and torch.xpu.is_available(): torch.xpu.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate with Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating Non-Math Outputs with BASE Model (Base-Style Prompts) ---\n",
      "Base model name: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Loading base tokenizer (deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)...\n",
      "Loading base model (deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)...\n",
      "Base model loaded: dtype=torch.bfloat16, device=xpu:0\n",
      "\n",
      "Prompt 1: Photosynthesis is the process by which green plants use sunlight, water, and carbon dioxide to create their own food. In simple terms, this means\n",
      "Base Model Response:\n",
      "that plants can convert sunlight into energy, which they use to grow. This energy comes from the Sun's electromagnetic radiation, which is a type of light. So, in the end, plants are using sunlight to power themselves, and their own energy. So, in the end, plants are using sunlight to create their own food. So, in the end, plants are using sunlight to create their own energy, which they can use to grow. So, in the end, plants are using sunlight to\n",
      "--------------------\n",
      "\n",
      "Prompt 2: It was a dark and rainy night in the city. The neon lights reflected off the wet pavement as\n",
      "Base Model Response:\n",
      "the city fell into the dark. It was a very strange... evening.\n",
      "\n",
      "Wait, I need to correct the grammar in this sentence. It's in the past tense, but maybe the verb should be in the present tense. Let me think.\n",
      "\n",
      "The sentence is: \"It was a dark and rainy night in the city. The neon lights reflected off the wet pavement as the city fell into the dark. It was a very strange... evening.\"\n",
      "\n",
      "Wait, I need to correct the grammar in this sentence\n",
      "--------------------\n",
      "\n",
      "Prompt 3: Q: What is the capital of France?\n",
      "A: Paris.\n",
      "\n",
      "Q: What is the capital of Spain?\n",
      "A: Madrid.\n",
      "\n",
      "Q: What is the capital of Germany?\n",
      "A:\n",
      "Base Model Response:\n",
      "Berlin.\n",
      "\n",
      "Q: What is the capital of Italy?\n",
      "\n",
      "A: Rome.\n",
      "\n",
      "Q: What is the capital of the UK?\n",
      "\n",
      "A: London.\n",
      "\n",
      "Q: What is the capital of France?\n",
      "\n",
      "A: Paris.\n",
      "\n",
      "Q: What is the capital of Spain?\n",
      "\n",
      "A: Madrid.\n",
      "\n",
      "Q: What is the capital of Germany?\n",
      "\n",
      "A: Berlin.\n",
      "\n",
      "Q: What is the capital of Italy?\n",
      "\n",
      "A: Rome.\n",
      "\n",
      "Q: What is the capital of the UK?\n",
      "\n",
      "A: London.\n",
      "\n",
      "Wait, these questions are\n",
      "--------------------\n",
      "\n",
      "Prompt 4: The old house stood on a hill overlooking\n",
      "Base Model Response:\n",
      "the mountain. The old house is connected to the old\n",
      "house through a\n",
      "$\\mathrm{H}$-shaped tunnel, which is\n",
      "$\\mathrm{H}\n",
      "\\mathrm{H}\n",
      "H-shaped tunnel, which is\n",
      "H-shaped tunnel, which is H-shaped tunnel. The\n",
      "$\\mathrm{H}-$ shaped tunnel is connected to the\n",
      "$\\mathrm{H}-$ shaped tunnel, which is connected to the\n",
      "$\\mathrm{H}-$ shaped tunnel, which is connected to the\n",
      "--------------------\n",
      "\n",
      "Prompt 5: Here is a list of common household pets:\n",
      "1. Cat\n",
      "2. Dog\n",
      "3.\n",
      "Base Model Response:\n",
      "Bird\n",
      "4. Hamster\n",
      "5. Raccoon\n",
      "6. Pigeon\n",
      "7. Qube\n",
      "8. Rook\n",
      "9. Bishell\n",
      "10. Sock\n",
      "\n",
      "Each of these pets has a number assigned to them. Each of these pets can be represented by a number... What is the sum of all the numbers assigned to these pets?\n",
      "\n",
      "To determine the number assigned to each pet, the following rules are given:\n",
      "\n",
      "1. The number assigned to a cat is\n",
      "--------------------\n",
      "\n",
      "Prompt 6: Trying to describe the color blue to someone who cannot see is difficult. One might say blue feels like\n",
      "Base Model Response:\n",
      "a shade of blue, but that's not helpful. Alternatively, they might say blue is a color that appears to be blue, but that's not helpful either. Maybe they can say that blue is a color that is blue, but that's not helpful. Alternatively, they can say that blue is a color that is blue, but that's not helpful. Alternatively, maybe they can say that blue is a color that is blue, but that's not helpful. Alternatively, maybe they can say that\n",
      "--------------------\n",
      "\n",
      "Prompt 7: Recipe Title: Quick Lemon Herb Chicken\n",
      "Recipe Title: Spicy Tomato and Bean Soup\n",
      "Recipe Title:\n",
      "Base Model Response:\n",
      "Quick Chicken and Beef Stroganoff\n",
      "\n",
      "Okay, so I'm going to make some of these dishes, but I'm a bit confused about the differences between them. Let me try to figure this out. Each of these recipes has a similar structure—they all involve a chicken or beef base, and then some additions. The first one is Quick Lemon Herb Chicken. The second is Spicy Tomato and Bean Soup, and the third is Quick Chicken and Beef Stroganoff. I'm not\n",
      "--------------------\n",
      "\n",
      "Prompt 8: Sentence using 'ubiquitous': Mobile phones have become ubiquitous in modern society.\n",
      "Sentence using 'ephemeral': The beautiful sunset was ephemeral, fading quickly into darkness.\n",
      "Sentence using 'serendipity':\n",
      "Base Model Response:\n",
      "The serendipity of meeting a stranger in a bustling city.\n",
      "Sentence using 'despair': Despair led to a lot of people giving up on the journey, but ultimately, they didn't give up.\n",
      "Sentence using 'harmful': The harmful effects of air pollution were never truly measured.\n",
      "Sentence using 'unintended consequences': The unintended consequences of the car accident were a major concern for everyone.\n",
      "Sentence using 'vastness': The vastness of the ocean makes it difficult to\n",
      "--------------------\n",
      "\n",
      "Prompt 9: Q: What are the benefits of recycling?\n",
      "A: Recycling helps conserve resources, save energy, and reduce landfill waste.\n",
      "\n",
      "Q: What are the benefits of regular exercise?\n",
      "A:\n",
      "Base Model Response:\n",
      "Regular exercise helps lower body weight, increase strength, improve flexibility, and enhance mental clarity.\n",
      "\n",
      "Q: What are the benefits of saving money?\n",
      "A: Saving money helps prevent debt, improve financial stability, and increase lifetime satisfaction.\n",
      "\n",
      "Q: What are the benefits of investing in stocks?\n",
      "A: Investing in stocks helps increase wealth, improve financial performance, and enhance risk management.\n",
      "\n",
      "Q: What are the benefits of using public transportation?\n",
      "A: Using public transportation helps reduce air pollution, improve air quality, and\n",
      "--------------------\n",
      "\n",
      "Prompt 10: A short poem about the moon:\n",
      "\n",
      "Silver light on silent seas,\n",
      "Base Model Response:\n",
      "the moon's light is always\n",
      "in the quiet, the moon's light is always in the quiet.\n",
      "The moon's light is always in the quiet, the moon's light is always in the quiet.\n",
      "The moon's light is always in the quiet, the moon's light is always in the quiet.\n",
      "The moon's light is always in the quiet, the moon's light is always in the quiet.\n",
      "The moon's light is always in the quiet, the moon's light is always in the quiet\n",
      "--------------------\n",
      "Deleting base model and tokenizer from memory...\n",
      "\n",
      "--- Non-Math Generation Test Complete ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n--- Generating Non-Math Outputs with BASE Model (Base-Style Prompts) ---\")\n",
    "# Ensure MODEL_NAME holds the original base model identifier\n",
    "print(f\"Base model name: {MODEL_NAME}\")\n",
    "\n",
    "base_model = None\n",
    "base_tokenizer = None\n",
    "\n",
    "try:\n",
    "    print(f\"Loading base tokenizer ({MODEL_NAME})...\")\n",
    "    base_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    if base_tokenizer.pad_token is None: base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "\n",
    "    print(f\"Loading base model ({MODEL_NAME})...\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=dtype_to_load,\n",
    "        device_map=device if device.type != 'cpu' else None\n",
    "    )\n",
    "    print(f\"Base model loaded: dtype={base_model.dtype}, device={base_model.device}\")\n",
    "\n",
    "    for i, prompt in enumerate(non_math_prompts_base_style):\n",
    "        print(f\"\\nPrompt {i+1}: {prompt}\")\n",
    "        response = generate_general_response_base(base_model, base_tokenizer, prompt, max_new_tokens_non_math, device)\n",
    "        print(f\"Base Model Response:\\n{response}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load or run inference with base model: {e}\")\n",
    "\n",
    "# Clean up base model\n",
    "print(\"Deleting base model and tokenizer from memory...\")\n",
    "if 'base_model' in locals() and base_model is not None: del base_model\n",
    "if 'base_tokenizer' in locals() and base_tokenizer is not None: del base_tokenizer\n",
    "if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "elif hasattr(torch.xpu, 'empty_cache') and torch.xpu.is_available(): torch.xpu.empty_cache()\n",
    "\n",
    "\n",
    "print(\"\\n--- Non-Math Generation Test Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
