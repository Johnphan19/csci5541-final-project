{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets evaluate transformers accelerate bitsandbytes wandb torch -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Custom Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nebul\\CodingProjects\\School\\5541 NLP\\csci5541-final-project\\final\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu with dtype: None\n",
      "Output directory: finetuned_DeepSeek-R1-Distill-Qwen-1.5B_length_val_modified_lila_MATH_algebra_crowdsourced\n",
      "Model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Dataset: ../datasets/length_val_modified_lila_MATH_algebra_crowdsourced.json\n",
      "Effective Batch Size: 8\n",
      "Masking End Sequence (for IDs): '</think>'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc # For garbage collection\n",
    "import wandb\n",
    "import importlib # Import the importlib module\n",
    "\n",
    "# Import custom modules and reload them to pick up changes\n",
    "import config\n",
    "importlib.reload(config) # Reload config to get new variables\n",
    "\n",
    "# Import the module first, then reload, then import specific classes/functions\n",
    "import model_handler\n",
    "importlib.reload(model_handler)\n",
    "from model_handler import ModelHandler\n",
    "\n",
    "import data_handler\n",
    "importlib.reload(data_handler)\n",
    "from data_handler import DataHandler\n",
    "\n",
    "import trainer_setup\n",
    "importlib.reload(trainer_setup)\n",
    "from trainer_setup import TrainerSetup\n",
    "\n",
    "import inference\n",
    "importlib.reload(inference)\n",
    "from inference import Generator\n",
    "\n",
    "# Ensure output directory exists using potentially updated config\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(config.LOGGING_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(config.SAVED_MODEL_PATH), exist_ok=True)\n",
    "\n",
    "print(f\"Using device: {config.DEVICE} with dtype: {config.DTYPE_TO_LOAD}\")\n",
    "print(f\"Output directory: {config.OUTPUT_DIR}\")\n",
    "print(f\"Model: {config.MODEL_NAME}\")\n",
    "print(f\"Dataset: {config.DATASET_JSON_PATH}\")\n",
    "print(f\"Effective Batch Size: {config.TRAIN_BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Masking End Sequence (for IDs): '{config.THINK_END_SEQUENCE}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Base Model and Tokenizer for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelHandler initialized for model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, device: cpu, dtype: None\n",
      "Loading tokenizer: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Tokenizer chat template:\n",
      "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}\n",
      "Loading model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Tokenizer chat template:\n",
      "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}\n",
      "Loading model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Dtype: torch.float32, Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize handler for the base model\n",
    "base_model_handler = ModelHandler(config.MODEL_NAME, config.DEVICE, config.DTYPE_TO_LOAD)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = base_model_handler.load_tokenizer()\n",
    "print(f\"Tokenizer chat template:\\n{tokenizer.chat_template}\") # Print the template\n",
    "\n",
    "# Load model (specify for_training=True)\n",
    "# Trainer handles device placement with Accelerate, so device_map=None is often best here.\n",
    "model = base_model_handler.load_model(for_training=True)\n",
    "\n",
    "# Optional: Clear handler if not needed anymore, model/tokenizer are now separate variables\n",
    "# del base_model_handler\n",
    "# gc.collect()\n",
    "# Generator.cleanup_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHandler initialized. Using tokenizer chat template for formatting.\n",
      "Masking End Sequence: '</think>', Encoded IDs for Masking: [151649]\n",
      "DataHandler using THINK_END_SEQUENCE: '</think>' with IDs: [151649] for masking.\n",
      "Loading base dataset: allenai/lila (MATH_algebra_crowdsourced)\n",
      "Original dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 263\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 106\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 157\n",
      "    })\n",
      "})\n",
      "Loading modified training data from: ../datasets/length_val_modified_lila_MATH_algebra_crowdsourced.json\n",
      "Training dataset replaced successfully.\n",
      "New dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset', 'correct_answer'],\n",
      "        num_rows: 263\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 106\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 157\n",
      "    })\n",
      "})\n",
      "Tokenizing dataset...\n",
      "Original dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 263\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 106\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 157\n",
      "    })\n",
      "})\n",
      "Loading modified training data from: ../datasets/length_val_modified_lila_MATH_algebra_crowdsourced.json\n",
      "Training dataset replaced successfully.\n",
      "New dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset', 'correct_answer'],\n",
      "        num_rows: 263\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 106\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 157\n",
      "    })\n",
      "})\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 263/263 [00:00<00:00, 1532.98 examples/s]\n",
      "Map:   0%|          | 0/106 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 106/106 [00:00<00:00, 1425.23 examples/s]\n",
      "Map:   0%|          | 0/157 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 157/157 [00:00<00:00, 1425.62 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing function applied.\n",
      "Removed columns ['input', 'output_program', 'output_answer', 'split', 'dataset', 'correct_answer'] from split 'train'.\n",
      "Removed columns ['input', 'output_program', 'output_answer', 'split', 'dataset'] from split 'validation'.\n",
      "Removed columns ['input', 'output_program', 'output_answer', 'split', 'dataset'] from split 'test'.\n",
      "Original columns removed after tokenization.\n",
      "Final tokenized training dataset example (first item keys): ['input_ids', 'attention_mask']\n",
      "Final tokenized validation dataset example (first item keys): ['input_ids', 'attention_mask']\n",
      "Initializing custom PromptMaskingDataCollator with masking sequence IDs: [151649] (from '</think>')\n",
      "PromptMaskingDataCollator: Initialized. Masking up to the first occurrence of sequence IDs: [151649] (corresponding to '</think>')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_handler = DataHandler(tokenizer, config.MAX_INPUT_LENGTH)\n",
    "\n",
    "# Explicitly print the sequence IDs being used for masking\n",
    "print(f\"DataHandler using THINK_END_SEQUENCE: '{config.THINK_END_SEQUENCE}' with IDs: {data_handler.think_end_sequence_ids} for masking.\")\n",
    "\n",
    "# Load base dataset and replace train split\n",
    "dataset = data_handler.load_and_prepare_datasets(\n",
    "    base_dataset_name=config.BASE_DATASET_NAME,\n",
    "    base_dataset_config=config.BASE_DATASET_CONFIG,\n",
    "    train_json_path=config.DATASET_JSON_PATH\n",
    ")\n",
    "\n",
    "# Tokenize the dataset (using chat template via _preprocess_function)\n",
    "tokenized_dataset = data_handler.tokenize_dataset(dataset)\n",
    "\n",
    "# Get data collator\n",
    "data_collator = data_handler.get_data_collator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Setup Trainer and WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerSetup initialized.\n",
      "Initializing WandB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvohno013\u001b[0m (\u001b[33mvohno013-university-of-minnesota\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nebul\\Coding Projects\\final-project\\final-project-code\\training model\\wandb\\run-20250422_033221-5fa2sm8z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/5fa2sm8z' target=\"_blank\">DeepSeek-R1-Distill-Qwen-1.5B-length_val_modified_lila_MATH_algebra_crowdsourced-lr2e-05-ep3</a></strong> to <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning' target=\"_blank\">https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/5fa2sm8z' target=\"_blank\">https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/5fa2sm8z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB initialized successfully.\n",
      "Training arguments configured.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nebul\\Coding Projects\\final-project\\final-project-code\\training model\\trainer_setup.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  self.trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized.\n"
     ]
    }
   ],
   "source": [
    "# Ensure required splits exist before passing to TrainerSetup\n",
    "train_split = tokenized_dataset.get('train')\n",
    "eval_split = tokenized_dataset.get('validation') # Using validation for eval during training\n",
    "\n",
    "trainer = None # Initialize trainer to None\n",
    "trainer_setup = None\n",
    "wandb_run = None\n",
    "\n",
    "if train_split and eval_split:\n",
    "    trainer_setup = TrainerSetup(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_split,\n",
    "        eval_dataset=eval_split\n",
    "    )\n",
    "    \n",
    "    # Initialize WandB\n",
    "    wandb_run = trainer_setup.setup_wandb()\n",
    "    \n",
    "    # Configure Training Arguments\n",
    "    training_args = trainer_setup.configure_training_args()\n",
    "    \n",
    "    # Initialize Trainer\n",
    "    trainer = trainer_setup.initialize_trainer()\n",
    "else:\n",
    "    print(\"Error: Missing 'train' or 'validation' split in tokenized_dataset. Cannot initialize Trainer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 21:16, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.953500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.671900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.616400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.445400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.332300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.284200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.152200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.104600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.114300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "***** train metrics *****\n",
      "  epoch                    =     2.9125\n",
      "  total_flos               =  1203265GF\n",
      "  train_loss               =     1.3846\n",
      "  train_runtime            = 0:21:29.07\n",
      "  train_samples_per_second =      0.612\n",
      "  train_steps_per_second   =      0.074\n"
     ]
    }
   ],
   "source": [
    "train_result = None\n",
    "if trainer:\n",
    "    print(\"Starting training...\")\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        print(\"Training finished.\")\n",
    "        # Log training metrics\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training: {e}\")\n",
    "        # Optional: cleanup resources if training fails early\n",
    "        # del model, trainer\n",
    "        # gc.collect()\n",
    "        # Generator.cleanup_memory()\n",
    "else:\n",
    "    print(\"Skipping training because Trainer initialization failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final model to finetuned_DeepSeek-R1-Distill-Qwen-1.5B_length_val_modified_lila_MATH_algebra_crowdsourced\\final_model...\n",
      "Model and tokenizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "if trainer and train_result: # Check if training actually ran and completed\n",
    "    print(f\"Saving final model to {config.SAVED_MODEL_PATH}...\")\n",
    "    trainer.save_model(config.SAVED_MODEL_PATH) # Save the model checkpoint\n",
    "    tokenizer.save_pretrained(config.SAVED_MODEL_PATH) # Save tokenizer with the model\n",
    "    print(f\"Model and tokenizer saved successfully.\")\n",
    "else:\n",
    "    print(\"Skipping model saving as training did not complete successfully or trainer was not initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluate Final Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating final model on the evaluation split...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='106' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  4/106 00:01 < 00:55, 1.82 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if trainer and train_result: # Check if training ran and completed\n",
    "    print(\"Evaluating final model on the evaluation split...\")\n",
    "    # Note: The evaluation split used here is the one passed during Trainer init (e.g., 'validation')\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    trainer.log_metrics(\"eval\", eval_metrics)\n",
    "    trainer.save_metrics(\"eval\", eval_metrics)\n",
    "    print(f\"Evaluation metrics: {eval_metrics}\")\n",
    "else:\n",
    "    print(\"Skipping evaluation as training did not complete successfully or trainer was not initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Finish WandB Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing up..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finish WandB run using the static method from TrainerSetup\n",
    "if trainer_setup:\n",
    "    TrainerSetup.finish_wandb()\n",
    "else:\n",
    "    print(\"TrainerSetup was not initialized, cannot finish WandB run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Clean Up Training Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up training resources...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Native API failed. Native API returns: 20 (UR_RESULT_ERROR_DEVICE_LOST)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Keep 'tokenizer', 'data_handler', 'dataset' if needed for inference comparison\u001b[39;00m\n\u001b[0;32m      9\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect() \u001b[38;5;66;03m# Run garbage collection\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mGenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcleanup_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Clear GPU cache if applicable\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nebul\\Coding Projects\\final-project\\final-project-code\\training model\\inference.py:191\u001b[0m, in \u001b[0;36mGenerator.cleanup_memory\u001b[1;34m()\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleared CUDA cache.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mxpu, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mempty_cache\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mxpu\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m--> 191\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxpu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleared XPU cache.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nebul\\Coding Projects\\final-project\\final\\lib\\site-packages\\torch\\xpu\\memory.py:23\u001b[0m, in \u001b[0;36mempty_cache\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other XPU application.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    of XPU memory in certain cases.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[1;32m---> 23\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_xpu_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Native API failed. Native API returns: 20 (UR_RESULT_ERROR_DEVICE_LOST)"
     ]
    }
   ],
   "source": [
    "# Delete training-specific objects to free memory before inference\n",
    "print(\"Cleaning up training resources...\")\n",
    "if 'model' in locals(): del model\n",
    "if 'trainer' in locals(): del trainer\n",
    "if 'trainer_setup' in locals(): del trainer_setup\n",
    "if 'tokenized_dataset' in locals(): del tokenized_dataset\n",
    "if 'base_model_handler' in locals(): del base_model_handler\n",
    "# Keep 'tokenizer', 'data_handler', 'dataset' if needed for inference comparison\n",
    "gc.collect() # Run garbage collection\n",
    "Generator.cleanup_memory() # Clear GPU cache if applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Setup for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model path not found (finetuned_DeepSeek-R1-Distill-Qwen-1.5B_length_val_modified_lila_MATH_algebra_crowdsourced\\final_model). Skipping fine-tuned generation.\n",
      "\n",
      "--- Loading Base Model for Inference ---\n",
      "ModelHandler initialized for model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, device: cpu, dtype: None\n",
      "Loading tokenizer: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Base tokenizer chat template:\n",
      "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}\n",
      "Loading model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Base tokenizer chat template:\n",
      "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}\n",
      "Loading model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Model loaded successfully. Dtype: torch.float32, Device: cpu\n",
      "Generator initialized. Model max length: 131072\n",
      "Model loaded successfully. Dtype: torch.float32, Device: cpu\n",
      "Generator initialized. Model max length: 131072\n"
     ]
    }
   ],
   "source": [
    "# --- Load Fine-Tuned Model for Inference ---\n",
    "generator_finetuned = None\n",
    "if os.path.exists(config.SAVED_MODEL_PATH):\n",
    "    ft_model, ft_tokenizer = ModelHandler.load_fine_tuned(config.SAVED_MODEL_PATH, config.DEVICE, config.DTYPE_TO_LOAD)\n",
    "    if ft_model and ft_tokenizer:\n",
    "        print(f\"Fine-tuned tokenizer chat template:\\n{ft_tokenizer.chat_template}\") # Print the template\n",
    "        generator_finetuned = Generator(ft_model, ft_tokenizer, config.DEVICE)\n",
    "    else:\n",
    "        print(\"Could not load fine-tuned model/tokenizer properly. Skipping fine-tuned generation.\")\n",
    "else:\n",
    "    print(f\"Fine-tuned model path not found ({config.SAVED_MODEL_PATH}). Skipping fine-tuned generation.\")\n",
    "\n",
    "# --- Load Base Model for Inference ---\n",
    "generator_base = None\n",
    "try:\n",
    "    print(\"\\n--- Loading Base Model for Inference ---\")\n",
    "    # Re-initialize handler for base model inference\n",
    "    base_model_handler_inf = ModelHandler(config.MODEL_NAME, config.DEVICE, config.DTYPE_TO_LOAD)\n",
    "    base_tokenizer_inf = base_model_handler_inf.load_tokenizer()\n",
    "    print(f\"Base tokenizer chat template:\\n{base_tokenizer_inf.chat_template}\") # Print the template\n",
    "    base_model_inf = base_model_handler_inf.load_model(for_training=False) # Load for inference\n",
    "    if base_model_inf and base_tokenizer_inf:\n",
    "        generator_base = Generator(base_model_inf, base_tokenizer_inf, config.DEVICE)\n",
    "    else:\n",
    "        print(\"Could not load base model/tokenizer properly. Skipping base model generation.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading base model for inference: {e}. Skipping base model generation.\")\n",
    "\n",
    "# We need the original dataset structure for inference examples\n",
    "# 'dataset' should still be available from the data loading step (cell 6)\n",
    "if 'dataset' not in locals():\n",
    "    print(\"Error: 'dataset' object not found. Cannot run inference comparisons.\")\n",
    "    # Optionally reload the dataset here if needed, but it should persist\n",
    "    # if 'tokenizer' in locals(): # Need a tokenizer instance\n",
    "    #     data_handler_inf = DataHandler(tokenizer, config.MAX_INPUT_LENGTH)\n",
    "    #     dataset = data_handler_inf.load_and_prepare_datasets(\n",
    "    #         base_dataset_name=config.BASE_DATASET_NAME,\n",
    "    #         base_dataset_config=config.BASE_DATASET_CONFIG,\n",
    "    #         train_json_path=config.DATASET_JSON_PATH\n",
    "    #     )\n",
    "    # else:\n",
    "    #     print(\"Cannot reload dataset as tokenizer is also missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can clear finetuned or general model to stop inference testing for either of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator_finetuned = None\n",
    "generator_base = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Generate Math Outputs (Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparing Outputs for First 10 Validation Examples ---\n",
      "\n",
      "--- Example 1 ---\n",
      "Problem:\n",
      "Simplify the expression $$(x^5+3x^2+3x^5)-(x^7+2x^2+6x^5).$$...\n",
      "\n",
      "Actual Solution:\n",
      "Combining like terms, we find that  \\begin{align*}\n",
      "&(x^5+3x^2+3x^5)-(x^7+2x^2+6x^5)\\\\\n",
      "&\\qquad=(x^5+3x^5-6x^5)+(3x^2-2x^2)-x^7\\\\\n",
      "&\\qquad=\\boxed{-x^7-2x^5+x^2}.\n",
      "\\end{align*}\n",
      "\n",
      "Skipping Fine-Tuned Model (not provided).\n",
      "\n",
      "Generating with Base Model...\n",
      "Base Model Solution:\n",
      "Alright, so I have this algebra problem to simplify the expression: (x^5 + 3x^2 + 3x^5) minus (x^7 + 2x^2 + 6x^5). Hmm, okay, let me try to figure this out step by step.\n",
      "\n",
      "First, I remember that simplifying expressions like this usually involves combining like terms. Like terms are terms that have the same variable raised to the same power. So, I need to look at each term in the first parentheses and the second parentheses and see if any can be combined.\n",
      "\n",
      "Let me write down the expression again to make sure I have it right:\n",
      "\n",
      "(x^5 + 3x^2 + 3x^5) - (x^7 + 2x^2 + 6x^5)\n",
      "\n",
      "Okay, so the first parentheses have three terms: x^5, 3x^2, and 3x^5. The second parentheses has three terms as well: x^7, 2x^2, and 6x^5.\n",
      "\n",
      "I need to distribute the negative sign to each term inside the second parentheses because of the subtraction. That means I'll subtract each term individually.\n",
      "\n",
      "Let me rewrite the expression after distributing the negative sign:\n",
      "\n",
      "x^5 + 3x^2 + 3x^5 - x^7 - 2x^2 - 6x^5\n",
      "\n",
      "Now, I can rearrange the terms to group like terms together. Let me list out all the terms:\n",
      "\n",
      "- x^7\n",
      "- x^5\n",
      "- 3x^5\n",
      "- 3x^2\n",
      "- -2x^2\n",
      "- -6x^5\n",
      "\n",
      "Wait, let me make sure I have all the terms. The first parentheses is x^5, 3x^2, 3x^5, and the second parentheses after distributing is -x^7, -2x^2, -6x^5. Yep, that's six terms. Let me write them in order from highest power to lowest power:\n",
      "\n",
      "- x^7\n",
      "- x^5 + 3x^5 - 6x^5\n",
      "- 3x^2 - 2x^2\n",
      "\n",
      "So, starting with the highest power, which is x^7. There's only one term, which is -x^7.\n",
      "\n",
      "Next, for x^5 terms. I have 1x^5, 3x^5,\n",
      "------------------------------\n",
      "\n",
      "--- Example 2 ---\n",
      "Problem:\n",
      "Find the sum of the coefficients in the polynomial $3(x^{10} - x^7 + 2x^3 - x + 7) + 4(x^3 - 2x^2 - 5)$ when it is simplified....\n",
      "\n",
      "Actual Solution:\n",
      "The sum of the coefficients in $3(x^{10} - x^7 + 2x^3 - x + 7) + 4(x^3 - 2x^2 - 5)$ is $3 (1 - 1 + 2 - 1 + 7) + 4(1 - 2 - 5) = 3 \\cdot 8 + 4 \\cdot (-6) = \\boxed{0}$.  (The sum of the coefficients in a polynomial can be found by setting the variable to 1.)\n",
      "\n",
      "Skipping Fine-Tuned Model (not provided).\n",
      "\n",
      "Generating with Base Model...\n",
      "Base Model Solution:\n",
      "Alright, so I have this algebra problem to simplify the expression: (x^5 + 3x^2 + 3x^5) minus (x^7 + 2x^2 + 6x^5). Hmm, okay, let me try to figure this out step by step.\n",
      "\n",
      "First, I remember that simplifying expressions like this usually involves combining like terms. Like terms are terms that have the same variable raised to the same power. So, I need to look at each term in the first parentheses and the second parentheses and see if any can be combined.\n",
      "\n",
      "Let me write down the expression again to make sure I have it right:\n",
      "\n",
      "(x^5 + 3x^2 + 3x^5) - (x^7 + 2x^2 + 6x^5)\n",
      "\n",
      "Okay, so the first parentheses have three terms: x^5, 3x^2, and 3x^5. The second parentheses has three terms as well: x^7, 2x^2, and 6x^5.\n",
      "\n",
      "I need to distribute the negative sign to each term inside the second parentheses because of the subtraction. That means I'll subtract each term individually.\n",
      "\n",
      "Let me rewrite the expression after distributing the negative sign:\n",
      "\n",
      "x^5 + 3x^2 + 3x^5 - x^7 - 2x^2 - 6x^5\n",
      "\n",
      "Now, I can rearrange the terms to group like terms together. Let me list out all the terms:\n",
      "\n",
      "- x^7\n",
      "- x^5\n",
      "- 3x^5\n",
      "- 3x^2\n",
      "- -2x^2\n",
      "- -6x^5\n",
      "\n",
      "Wait, let me make sure I have all the terms. The first parentheses is x^5, 3x^2, 3x^5, and the second parentheses after distributing is -x^7, -2x^2, -6x^5. Yep, that's six terms. Let me write them in order from highest power to lowest power:\n",
      "\n",
      "- x^7\n",
      "- x^5 + 3x^5 - 6x^5\n",
      "- 3x^2 - 2x^2\n",
      "\n",
      "So, starting with the highest power, which is x^7. There's only one term, which is -x^7.\n",
      "\n",
      "Next, for x^5 terms. I have 1x^5, 3x^5,\n",
      "------------------------------\n",
      "\n",
      "--- Example 2 ---\n",
      "Problem:\n",
      "Find the sum of the coefficients in the polynomial $3(x^{10} - x^7 + 2x^3 - x + 7) + 4(x^3 - 2x^2 - 5)$ when it is simplified....\n",
      "\n",
      "Actual Solution:\n",
      "The sum of the coefficients in $3(x^{10} - x^7 + 2x^3 - x + 7) + 4(x^3 - 2x^2 - 5)$ is $3 (1 - 1 + 2 - 1 + 7) + 4(1 - 2 - 5) = 3 \\cdot 8 + 4 \\cdot (-6) = \\boxed{0}$.  (The sum of the coefficients in a polynomial can be found by setting the variable to 1.)\n",
      "\n",
      "Skipping Fine-Tuned Model (not provided).\n",
      "\n",
      "Generating with Base Model...\n",
      "Base Model Solution:\n",
      "Okay, so I have this problem here: I need to find the sum of the coefficients in the polynomial \\(3(x^{10} - x^7 + 2x^3 - x + 7) + 4(x^3 - 2x^2 - 5)\\) when it's simplified. Hmm, let's see. I remember that the sum of the coefficients in a polynomial is found by plugging in \\(x = 1\\). Is that right? Let me think... Yeah, because if you substitute 1 for \\(x\\), each term becomes just the coefficient times 1, so the whole expression becomes the sum of the coefficients. So, maybe I don't have to expand the entire polynomial and then add up all the coefficients. That seems like a smarter way.\n",
      "\n",
      "But just to make sure, maybe I should try both methods to see if I get the same answer. Let me try the substitution method first. So, if I substitute \\(x = 1\\) into the polynomial, I should get the sum of the coefficients. Let me write down the polynomial again:\n",
      "\n",
      "\\(3(x^{10} - x^7 + 2x^3 - x + 7) + 4(x^3 - 2x^2 - 5)\\)\n",
      "\n",
      "Substituting \\(x = 1\\), we get:\n",
      "\n",
      "\\(3(1^{10} - 1^7 + 2*1^3 - 1 + 7) + 4(1^3 - 2*1^2 - 5)\\)\n",
      "\n",
      "Simplify each part step by step. Let's compute each term inside the parentheses first.\n",
      "\n",
      "Starting with the first part: \\(1^{10}\\) is 1, \\(1^7\\) is also 1, \\(2*1^3\\) is 2*1=2, \\(-1\\) is -1, and +7 is +7. So inside the first parenthesis:\n",
      "\n",
      "1 - 1 + 2 - 1 + 7.\n",
      "\n",
      "Let me compute that: 1 - 1 is 0, 0 + 2 is 2, 2 - 1 is 1, 1 + 7 is 8. So the first part is 8.\n",
      "\n",
      "Multiply that by 3: 3 * 8 = 24.\n",
      "\n",
      "Now the second part: \\(1^3\\) is 1, \\(-2*1^2\\) is -2*1\n",
      "------------------------------\n",
      "\n",
      "--- Example 3 ---\n",
      "Problem:\n",
      "Expand the product ${6(x+2)(x+3)}$....\n",
      "\n",
      "Actual Solution:\n",
      "First, we use the distributive property to expand the first two factors:\n",
      "\n",
      "\\begin{align*}\n",
      "6(x+2)(x+3) &= (6\\cdot x + 6 \\cdot 2) (x+3)\\\\\n",
      "&=(6x+12)(x+3)\n",
      "\\end{align*}We use the distributive property again by adding the product of $6x+12$ and $x$ to the product of $6x+12$ and 3:\n",
      "\n",
      "\\begin{align*}\n",
      "(6x+12)(x+3) &= (6x+12) \\cdot x +(6x+12) \\cdot 3\\\\\n",
      "&= x(6x+12) + 3(6x+12)\n",
      "\\end{align*}We use the distributive property again and combine like terms:\n",
      "\n",
      "\\begin{align*}\n",
      "x(6x+12) + 3(6x+12) &= 6x^2 + 12x + 18x+ 36\\\\\n",
      "&= \\boxed{6x^2 + 30x + 36}\n",
      "\\end{align*}\n",
      "\n",
      "Skipping Fine-Tuned Model (not provided).\n",
      "\n",
      "Generating with Base Model...\n",
      "Base Model Solution:\n",
      "Okay, so I have this problem here: I need to find the sum of the coefficients in the polynomial \\(3(x^{10} - x^7 + 2x^3 - x + 7) + 4(x^3 - 2x^2 - 5)\\) when it's simplified. Hmm, let's see. I remember that the sum of the coefficients in a polynomial is found by plugging in \\(x = 1\\). Is that right? Let me think... Yeah, because if you substitute 1 for \\(x\\), each term becomes just the coefficient times 1, so the whole expression becomes the sum of the coefficients. So, maybe I don't have to expand the entire polynomial and then add up all the coefficients. That seems like a smarter way.\n",
      "\n",
      "But just to make sure, maybe I should try both methods to see if I get the same answer. Let me try the substitution method first. So, if I substitute \\(x = 1\\) into the polynomial, I should get the sum of the coefficients. Let me write down the polynomial again:\n",
      "\n",
      "\\(3(x^{10} - x^7 + 2x^3 - x + 7) + 4(x^3 - 2x^2 - 5)\\)\n",
      "\n",
      "Substituting \\(x = 1\\), we get:\n",
      "\n",
      "\\(3(1^{10} - 1^7 + 2*1^3 - 1 + 7) + 4(1^3 - 2*1^2 - 5)\\)\n",
      "\n",
      "Simplify each part step by step. Let's compute each term inside the parentheses first.\n",
      "\n",
      "Starting with the first part: \\(1^{10}\\) is 1, \\(1^7\\) is also 1, \\(2*1^3\\) is 2*1=2, \\(-1\\) is -1, and +7 is +7. So inside the first parenthesis:\n",
      "\n",
      "1 - 1 + 2 - 1 + 7.\n",
      "\n",
      "Let me compute that: 1 - 1 is 0, 0 + 2 is 2, 2 - 1 is 1, 1 + 7 is 8. So the first part is 8.\n",
      "\n",
      "Multiply that by 3: 3 * 8 = 24.\n",
      "\n",
      "Now the second part: \\(1^3\\) is 1, \\(-2*1^2\\) is -2*1\n",
      "------------------------------\n",
      "\n",
      "--- Example 3 ---\n",
      "Problem:\n",
      "Expand the product ${6(x+2)(x+3)}$....\n",
      "\n",
      "Actual Solution:\n",
      "First, we use the distributive property to expand the first two factors:\n",
      "\n",
      "\\begin{align*}\n",
      "6(x+2)(x+3) &= (6\\cdot x + 6 \\cdot 2) (x+3)\\\\\n",
      "&=(6x+12)(x+3)\n",
      "\\end{align*}We use the distributive property again by adding the product of $6x+12$ and $x$ to the product of $6x+12$ and 3:\n",
      "\n",
      "\\begin{align*}\n",
      "(6x+12)(x+3) &= (6x+12) \\cdot x +(6x+12) \\cdot 3\\\\\n",
      "&= x(6x+12) + 3(6x+12)\n",
      "\\end{align*}We use the distributive property again and combine like terms:\n",
      "\n",
      "\\begin{align*}\n",
      "x(6x+12) + 3(6x+12) &= 6x^2 + 12x + 18x+ 36\\\\\n",
      "&= \\boxed{6x^2 + 30x + 36}\n",
      "\\end{align*}\n",
      "\n",
      "Skipping Fine-Tuned Model (not provided).\n",
      "\n",
      "Generating with Base Model...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if 'dataset' in locals() and (generator_finetuned or generator_base):\n",
    "    Generator.compare_outputs(\n",
    "        dataset=dataset, # Use the original dataset loaded earlier\n",
    "        generator_finetuned=generator_finetuned,\n",
    "        generator_base=generator_base,\n",
    "        num_examples=config.NUM_VALIDATION_EXAMPLES_TO_GENERATE\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping math output comparison due to missing dataset or both models failed to load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Generate Non-Math Outputs (Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Testing Non-Math Generation ---\n",
      "\n",
      "--- Generating Non-Math with FINE-TUNED Model ---\n",
      "\n",
      "Prompt 1: Photosynthesis is the process by which green plants use sunlight, water, and carbon dioxide to create their own food. In simple terms, this means\n",
      "Fine-Tuned Model Response:\n",
      "Okay, so I'm trying to understand photosynthesis better. I know it's something to do with plants using sunlight, water, and carbon dioxide to make food, which I think is oxygen and glucose. But I'm not entirely sure how all these pieces fit together. Let me break it down step by step.\n",
      "\n",
      "First, the process involves sunlight. I remember hearing that plants absorb sunlight through their leaves, and this energy is used in the process. I think it's called the light reaction, but I'm not exactly sure what that entails. Maybe it's the conversion of light energy into chemical energy, which plants use to make their own food.\n",
      "\n",
      "Then there's water. I know that plants take in water from the soil, and this water is split into oxygen and hydrogen. Wait, how does that work? I think it's a process called the Calvin cycle, but I'm not entirely clear on that. Maybe the water is split into hydrogen and oxygen, and then the oxygen is released into the air. But why is the hydrogen important? Oh, right, it's used to make glucose, which is the food plants need.\n",
      "\n",
      "Carbon dioxide comes into play next. I remember that plants take in carbon dioxide from the air, and this is fixed into organic molecules. So, the process involves converting carbon dioxide into glucose and oxygen. I think this is done in the chloroplasts, specifically in the reaction quotient equation. The chloroplast has two membranes, and the reaction occurs in the inner membrane.\n",
      "\n",
      "So, putting it all together, the steps are: absorption of sunlight (which causes the light reaction), splitting of water into hydrogen and oxygen (Calvin cycle), and fixing of carbon dioxide into glucose and oxygen (in the chloroplasts). The overall equation is something like 6CO2 + 6H2O + light energy → C6H12O6 + 6O2. That makes sense because plants produce glucose and oxygen from carbon dioxide and water.\n",
      "\n",
      "Wait, but how does the chloroplast specifically handle the conversion of CO2 into glucose? I think it's a bit more detailed. The reaction quotient equation involves the transfer of electrons from the electron transport chain to the Calvin cycle. This creates a gradient that helps in the fixation of carbon dioxide. So, the electrons are transferred, creating a concentration gradient that allows the use of CO2 as an electron donor.\n",
      "\n",
      "I'm a bit confused about the exact mechanism of the electron transport chain and how it relates to the Calvin cycle. Maybe I should look up the steps involved in the Calvin cycle to get a clearer picture. The Calvin cycle involves fixing CO2 into a 5-carbon molecule, which then reduces to a 3-carbon molecule, and finally to glucose.\n",
      "\n",
      "So, the overall process is a series of interconnected steps that transform sunlight into chemical energy, which plants use to produce food. It's a complex process, but breaking it down into these components helps me understand how plants make their own food from the environment.\n",
      "\n",
      "I also wonder about the role of other organisms in this process. I know that plants are the primary producers, meaning they make their own food, but I'm not sure how other organisms, like animals or humans, obtain their food. It seems like they depend on plants for the carbon dioxide they need, which then gets converted into glucose and oxygen.\n",
      "\n",
      "Another thing I'm curious about is the efficiency of this process. I know that plants are not 100% efficient at converting sunlight into energy, but I'm not sure how much energy is actually used for photosynthesis. I think it's about 1% of the energy from the sun being used, but I'm not certain about the exact figure.\n",
      "\n",
      "Also, I'm trying to remember if there are any exceptions or variations in how plants perform photosynthesis. For example, do all plants perform photosynthesis, or are there some that don't? I think most plants do, but maybe some do it in a different way or with a different set of requirements.\n",
      "\n",
      "I should also consider the environmental factors that affect photosynthesis. I know that light intensity and temperature can influence how efficient photosynthesis is. For instance, plants that are exposed to more intense light or cooler temperatures might have different rates of photosynthesis compared to those in warmer conditions.\n",
      "\n",
      "In summary, photosynthesis is a process where plants use sunlight, water, and carbon dioxide to create glucose and oxygen. The steps involve the absorption of light energy, the splitting of water into hydrogen and oxygen, and the fixation of carbon dioxide into organic molecules. This process is crucial for life on Earth, as it provides the energy and building blocks for all living organisms.\n",
      "</think>\n",
      "\n",
      "Photosynthesis is a fundamental process by which plants convert sunlight, water, and carbon dioxide into glucose and oxygen. Here's a structured explanation of the process:\n",
      "\n",
      "1. **Absorption of Light Energy**: Plants absorb sunlight through their leaves, and this energy is converted into chemical energy within the light reaction. This energy powers the overall process.\n",
      "\n",
      "2. **Water Splitting (Calvin Cycle)**: Water is split into oxygen and hydrogen molecules. This process occurs in the stroma of plant cells and releases oxygen into the atmosphere.\n",
      "\n",
      "3. **Carbon Dioxide Fixation**: Carbon dioxide is fixed into organic molecules by fixing carbon dioxide into a 5-carbon molecule (glyceraldehyde-3-phosphate). This is facilitated by the electron transport chain and the Calvin cycle, which transfer electrons from the inner membrane to the outer membrane.\n",
      "\n",
      "4. **Glucose and Oxygen Production**: The 5-carbon molecule is reduced to a 3-carbon molecule, which is then used to build glucose. Oxygen is released during this process, completing the cycle.\n",
      "\n",
      "The overall equation for photosynthesis is:\n",
      "\\[ 6CO_2 + 6H_2O + \\text{light energy} \\rightarrow C_6H_{12}O_6 + 6O_2 \\]\n",
      "\n",
      "This process is essential for life on Earth, as it provides the energy and building blocks for all living organisms. Plants are the primary producers, relying on other organisms for carbon dioxide and energy, which in turn depends on the plant's ability to perform photosynthesis efficiently.\n",
      "--------------------\n",
      "\n",
      "Prompt 2: It was a dark and rainy night in the city. The neon lights reflected off the wet pavement as\n",
      "Fine-Tuned Model Response:\n",
      "</think>\n",
      "\n",
      "The city was illuminated by neon lights, and the wet pavement reflected the harshness of the night. The city of Shenzhen was filled with the sounds of life, the bustling voices of people, and the quiet sounds of nature. The streets were lined with street lamps, casting long shadows, and the rain was falling like a blanket over the city.\n",
      "--------------------\n",
      "\n",
      "Prompt 3: Q: What is the capital of France?\\nA: Paris.\\n\\nQ: What is the capital of Spain?\\nA: Madrid.\\n\\nQ: What is the capital of Germany?\\nA:\n",
      "Fine-Tuned Model Response:\n",
      "Okay, so I need to figure out the capital of Germany. I remember that capitals are usually named after the country, but I'm not entirely sure about Germany. Let me think about this step by step.\n",
      "\n",
      "First, I know that the capitals of many European countries are named after their leaders. For example, Paris is associated with Napoleon, and Madrid with Madrid the Elder. So maybe Germany has a similar name. I think it's something like \"Hannover,\" but I'm not certain.\n",
      "\n",
      "I also recall that Germany is a country that has a long history and has been a center of culture and science. I remember hearing about the German Empire, which was a long time ago, but I don't think that's relevant here. Instead, I should focus on the current capital.\n",
      "\n",
      "I think I've heard that the capital is called \"Hannover.\" Let me check that. Yes, I believe that's correct. So, the capital of Germany is Hannover. I can't think of any other names that come to mind for the capital of Germany, so I'm pretty confident that it's Hannover.\n",
      "\n",
      "Just to be thorough, I'll think about other countries. For instance, France is Paris, Spain is Madrid, and the United States is Washington, D.C. Following this pattern, Germany should be Hannover. I don't think there's a more commonly used name for the capital of Germany than Hannover.\n",
      "\n",
      "So, putting it all together, the capital of Germany is Hannover.\n",
      "</think>\n",
      "\n",
      "The capital of Germany is Hannover.\n",
      "--------------------\n",
      "\n",
      "Prompt 4: The old house stood on a hill overlooking\n",
      "Fine-Tuned Model Response:\n",
      "</think>\n",
      "\n",
      "The old house stood on a hill overlooking\n",
      "--------------------\n",
      "\n",
      "Prompt 5: Here is a list of common household pets:\\n1. Cat\\n2. Dog\\n3.\n",
      "Fine-Tuned Model Response:\n",
      "Okay, so I need to figure out how to respond to this query. The user provided a list of common household pets and then asked me to \"put your answer within \\[...\\].\" I think the user wants me to write a response that's helpful and informative.\n",
      "\n",
      "Hmm, the list includes a cat and a dog, but that's all it says. Maybe the user is asking for an explanation or a guide on what to do with pets. I should consider what kind of information would be useful. Perhaps I can explain why pets are good, how to care for them, or maybe give some tips on how to use them.\n",
      "\n",
      "Wait, the user's example response was pretty detailed. It started with a friendly greeting, then asked for more details, and then provided a helpful answer. I should follow that approach. I need to make sure my response is clear, concise, and provides value.\n",
      "\n",
      "I wonder if the user is looking for advice on pet care or if they just want to know what to do with their pets. Since the list is short, maybe they're new to pets and need some guidance. I'll assume they're looking for general advice.\n",
      "\n",
      "I should also think about the tone. It should be friendly and reassuring. Maybe start with a greeting, explain that I'm here to help, and then provide tips on caring for pets or how to use them.\n",
      "\n",
      "Wait, the example response was quite detailed, so I should make sure to include that. Maybe I can add something about how to take care of pets, like feeding, exercising, and avoiding certain foods.\n",
      "\n",
      "Also, considering the user's request, I should ensure that my response is within the specified format and that I don't add any extra information beyond what's provided in the example.\n",
      "\n",
      "I think I have a good plan. I'll start with a friendly greeting, explain that I'm here to help, and then provide some general tips on caring for pets and using them. That should cover it and meet the user's needs.\n",
      "</think>\n",
      "\n",
      "Certainly! Here's a helpful response to your query:\n",
      "\n",
      "---\n",
      "\n",
      "Hello! I'm here to help with all sorts of pet-related questions. If you have any questions about pets, like how to care for them or how to use them, feel free to ask! I'm happy to provide guidance and answer any questions you might have.\n",
      "\n",
      "---\n",
      "\n",
      "This response is friendly, informative, and concise, providing value while meeting your request.\n",
      "--------------------\n",
      "\n",
      "Prompt 6: Trying to describe the color blue to someone who cannot see is difficult. One might say blue feels like\n",
      "Fine-Tuned Model Response:\n",
      "Okay, so I need to figure out how to describe blue in a way that someone who can't see can understand. Hmm, that's tricky because I know people with color vision often have trouble with terms like \"blue.\" I remember reading somewhere that some people find \"rainbow\" more confusing because it's so colorful, but \"blue\" is simpler. \n",
      "\n",
      "I guess the first thing I should think about is what \"blue\" means to someone who can't see. It's the color of the sky, right? So maybe I can start by talking about the sky and what it looks like without using words that involve seeing. Like, instead of saying \"the sky is blue,\" I could say \"the sky is bright and full of colors.\" But that's still a bit vague.\n",
      "\n",
      "Wait, the user mentioned something about blue feeling like something else. Maybe I should think about the properties of blue. Blue is a color that's associated with light, but it's also a color that people often associate with darkness or cold. So maybe I can describe it as a color that feels cold or mysterious because it's not warm or warm-like. That could help someone who can't see visualize it better.\n",
      "\n",
      "I also remember that some people find \"rainbow\" confusing because it's so colorful. Maybe I can use that as an example. If someone can't see, they might not know what a rainbow looks like, but blue is just a single color. So maybe I can compare blue to something else that's more familiar or less confusing. Like, \"blue is like the color of a calm lake, where the water is clear and the sky is blue.\"\n",
      "\n",
      "Wait, that still doesn't use words that involve seeing. Maybe I can use more abstract terms. Like, \"blue is a color that represents calmness and peace, and it's often associated with the horizon.\" That way, it's a bit more philosophical, which might help someone who can't see understand it better.\n",
      "\n",
      "I also think about the color in nature. Blue is often associated with the ocean, lakes, and the sky. So maybe I can describe it as a color that you see everywhere, like the ocean or the sky. That way, it's more relatable and easier to imagine.\n",
      "\n",
      "Another thought: maybe I can use an analogy. Like, if someone can't see, they might not know how to describe colors, but they can describe things around them. So maybe I can say, \"blue is like the sound of a calm lake, where the water is still and the sky is blue.\" That uses words like still and blue, which are more abstract and less about seeing.\n",
      "\n",
      "Wait, but the user said that blue is often confusing for people who can't see, so maybe I need to find a more straightforward way. Maybe I can use something like \"blue is a color that feels cold and mysterious because it's not warm.\" That gives a bit of a hint without being too vague.\n",
      "\n",
      "I also think about how colors are perceived by different people. Maybe I can mention that some people find \"rainbow\" confusing, but \"blue\" is simple because it's just one color. So I could say, \"Blue is a color that feels cold and mysterious because it's not warm, and it's simple enough that everyone can understand it.\"\n",
      "\n",
      "Wait, but that still uses words that involve seeing. Maybe I can avoid that. Instead, I can describe it as a color that you can see, but it's just one color. So, \"Blue is the color of the sky, and it feels like calmness and peace because it's just one color.\"\n",
      "\n",
      "I also think about how people describe colors without using words that involve seeing. Maybe I can use metaphors or symbols. Like, \"Blue is like the color of a calm lake, where the water is clear and the sky is blue.\" That metaphor might help someone who can't see visualize it better.\n",
      "\n",
      "Wait, but I should make sure the response is clear and not too convoluted. Maybe I can structure it in a way that's easy to understand, like starting with what blue is, then describing its characteristics, and ending with how it feels.\n",
      "\n",
      "So, putting it all together, maybe something like:\n",
      "\n",
      "\"Blue is a color that feels cold and mysterious because it's not warm, and it's simple enough that everyone can understand it. It's like the color of the sky, and it feels like calmness and peace because it's just one color. Blue is the color of the ocean, lakes, and the horizon, and it's associated with calmness and peace. It's like the sound of a calm lake, where the water is still and the sky is blue. Blue is a color that you can see, but it's just one color, making it simple and easy to understand.\"\n",
      "\n",
      "I think that covers it. It explains what blue is, its characteristics, and how it feels without using words that involve seeing. It uses metaphors and symbols to help someone who can't see visualize it better. I should make sure the language is simple and clear, avoiding any jargon or complex terms.\n",
      "</think>\n",
      "\n",
      "Blue is a color that feels cold and mysterious because it's not warm, and it's simple enough that everyone can understand it. It's like the color of the sky, and it feels like calmness and peace because it's just one color. Blue is the color of the ocean, lakes, and the horizon, and it's associated with calmness and peace. It's like the sound of a calm lake, where the water is still and the sky is blue. Blue is a color that you can see, but it's just one color, making it simple and easy to understand.\n",
      "--------------------\n",
      "\n",
      "Prompt 7: Recipe Title: Quick Lemon Herb Chicken\\nRecipe Title: Spicy Tomato and Bean Soup\\nRecipe Title:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generator_finetuned \u001b[38;5;129;01mor\u001b[39;00m generator_base:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mGenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_non_math_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNON_MATH_PROMPTS_BASE_STYLE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerator_finetuned\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator_finetuned\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerator_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator_base\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# The <think> tag is now handled internally by the Generator using the template\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m      \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping non-math output comparison as both models failed to load.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nebul\\Coding Projects\\final-project\\final-project-code\\training model\\inference.py:164\u001b[0m, in \u001b[0;36mGenerator.test_non_math_generation\u001b[1;34m(prompts, generator_finetuned, generator_base)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, prompt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(prompts):\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The <think>\\n is now handled by generate_general_response via the template\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPrompt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Print the original prompt\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator_finetuned\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_general_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Pass the original prompt\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFine-Tuned Model Response:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nebul\\Coding Projects\\final-project\\final-project-code\\training model\\inference.py:89\u001b[0m, in \u001b[0;36mGenerator.generate_general_response\u001b[1;34m(self, prompt, max_new_tokens)\u001b[0m\n\u001b[0;32m     86\u001b[0m      \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Input prompt too long or empty after processing]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 89\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m     91\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens,\n\u001b[0;32m     92\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[0;32m     93\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;66;03m# Sampling parameters for general text\u001b[39;00m\n\u001b[0;32m     95\u001b[0m         do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     96\u001b[0m         top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,\n\u001b[0;32m     97\u001b[0m         top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[0;32m     98\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m,\n\u001b[0;32m     99\u001b[0m     )\n\u001b[0;32m    101\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m, inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:]\n\u001b[0;32m    102\u001b[0m response_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\nebul\\Coding Projects\\final-project\\final\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nebul\\Coding Projects\\final-project\\final\\lib\\site-packages\\transformers\\generation\\utils.py:2326\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[0;32m   2318\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2319\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2320\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2321\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2322\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2323\u001b[0m     )\n\u001b[0;32m   2325\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2326\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2327\u001b[0m         input_ids,\n\u001b[0;32m   2328\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2329\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2330\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2331\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2332\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2333\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2334\u001b[0m     )\n\u001b[0;32m   2336\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2337\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[0;32m   2338\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2339\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2340\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2341\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2342\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2343\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\nebul\\Coding Projects\\final-project\\final\\lib\\site-packages\\transformers\\generation\\utils.py:3289\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3287\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3289\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3291\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3292\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3293\u001b[0m     outputs,\n\u001b[0;32m   3294\u001b[0m     model_kwargs,\n\u001b[0;32m   3295\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3296\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\nebul\\Coding Projects\\final-project\\final\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nebul\\Coding Projects\\final-project\\final\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\nebul\\Coding Projects\\final-project\\final\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nebul\\Coding Projects\\final-project\\final\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:855\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    852\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m    854\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 855\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    856\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    857\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    858\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    859\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    860\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    861\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    862\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    863\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    864\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    865\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    867\u001b[0m )\n\u001b[0;32m    869\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    870\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nebul\\Coding Projects\\final-project\\final\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nebul\\Coding Projects\\final-project\\final\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\nebul\\Coding Projects\\final-project\\final\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:579\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    567\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    568\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    569\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    576\u001b[0m         position_embeddings,\n\u001b[0;32m    577\u001b[0m     )\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 579\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    580\u001b[0m         hidden_states,\n\u001b[0;32m    581\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    582\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    583\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    584\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    585\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    586\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    587\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    589\u001b[0m     )\n\u001b[0;32m    591\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\nebul\\Coding Projects\\final-project\\final\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nebul\\Coding Projects\\final-project\\final\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\nebul\\Coding Projects\\final-project\\final\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:260\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    261\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    262\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    263\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    264\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    265\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    266\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    267\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    268\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    270\u001b[0m )\n\u001b[0;32m    271\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    273\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nebul\\Coding Projects\\final-project\\final\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nebul\\Coding Projects\\final-project\\final\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\nebul\\Coding Projects\\final-project\\final\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:192\u001b[0m, in \u001b[0;36mQwen2Attention.forward\u001b[1;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m         attention_interface \u001b[38;5;241m=\u001b[39m ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation]\n\u001b[1;32m--> 192\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m attention_interface(\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    194\u001b[0m     query_states,\n\u001b[0;32m    195\u001b[0m     key_states,\n\u001b[0;32m    196\u001b[0m     value_states,\n\u001b[0;32m    197\u001b[0m     attention_mask,\n\u001b[0;32m    198\u001b[0m     dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout,\n\u001b[0;32m    199\u001b[0m     scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling,\n\u001b[0;32m    200\u001b[0m     sliding_window\u001b[38;5;241m=\u001b[39msliding_window,  \u001b[38;5;66;03m# main diff with Llama\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    202\u001b[0m )\n\u001b[0;32m    204\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    205\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n",
      "File \u001b[1;32mc:\\Users\\nebul\\Coding Projects\\final-project\\final\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:54\u001b[0m, in \u001b[0;36msdpa_attention_forward\u001b[1;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(is_causal, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     52\u001b[0m     is_causal \u001b[38;5;241m=\u001b[39m is_causal\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 54\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if generator_finetuned or generator_base:\n",
    "    Generator.test_non_math_generation(\n",
    "        prompts=config.NON_MATH_PROMPTS_BASE_STYLE,\n",
    "        generator_finetuned=generator_finetuned,\n",
    "        generator_base=generator_base\n",
    "        # The prompt formatting is now handled internally by the Generator using the chat template\n",
    "    )\n",
    "else:\n",
    "     print(\"Skipping non-math output comparison as both models failed to load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Final Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nebul\\AppData\\Local\\Temp\\ipykernel_27536\\3744874013.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized.\n"
     ]
    }
   ],
   "source": [
    "# Clean up inference resources\n",
    "print(\"\\nCleaning up inference resources...\")\n",
    "if 'ft_model' in locals(): del ft_model\n",
    "if 'ft_tokenizer' in locals(): del ft_tokenizer\n",
    "if 'generator_finetuned' in locals(): del generator_finetuned\n",
    "if 'base_model_inf' in locals(): del base_model_inf\n",
    "if 'base_tokenizer_inf' in locals(): del base_tokenizer_inf\n",
    "if 'generator_base' in locals(): del generator_base\n",
    "if 'base_model_handler_inf' in locals(): del base_model_handler_inf\n",
    "if 'dataset' in locals(): del dataset\n",
    "if 'data_handler' in locals(): del data_handler\n",
    "if 'tokenizer' in locals(): del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "Generator.cleanup_memory()\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
