{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets evaluate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nebul\\Coding Projects\\CSCI5541\\project\\final-project\\final\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import evaluate\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose which modified dataset to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_JSON_PATH = \"../datasets/val_modified_lila_MATH_algebra_crowdsourced.json\"\n",
    "DATASET_JSON_PATH = \"../datasets/length_val_modified_lila_MATH_algebra_crowdsourced.json\"\n",
    "# DATASET_JSON_PATH = \"../datasets/scrambled_lila_MATH_algebra_crowdsourced.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "OUTPUT_DIR = f\"finetuned_{MODEL_NAME.split('/')[-1]}_{os.path.basename(DATASET_JSON_PATH).split('.')[0]}\" # Dynamic output dir name\n",
    "WANDB_PROJECT = \"NLP_Final_Project_FineTuning\"\n",
    "LEARNING_RATE = 5e-6\n",
    "EPOCHS = 1 # Start with 1 epoch because of large model. Can adjust based on results.\n",
    "TRAIN_BATCH_SIZE = 1 # Adjust based on GPU memory\n",
    "GRADIENT_ACCUMULATION_STEPS = 8 # Effective batch size = TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "EVAL_BATCH_SIZE = 1 # Could try larger, but was getting NAN loss with larger batch size\n",
    "WEIGHT_DECAY = 0.01\n",
    "# Can set evaluation steps instead of evaluating every epoch if epochs > 1 and dataset is large\n",
    "EVALUATION_STEPS = 5\n",
    "\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\" # Helps manage memory fragmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True) # Added trust_remote_code=True, often needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configer tokenizer & load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Set tokenizer pad_token to eos_token\")\n",
    "\n",
    "# Load model. Can load with lower precision if memory is tight\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True, # Added trust_remote_code=True\n",
    "    # torch_dtype=torch.bfloat16, # Uncomment for mixed precision (need compatible GPU)\n",
    ")\n",
    "\n",
    "print(\"Model and Tokenizer loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 263\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 106\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 157\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load test and validation datasets\n",
    "dataset = load_dataset(\"allenai/lila\", \"MATH_algebra_crowdsourced\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: ../datasets/length_val_modified_lila_MATH_algebra_crowdsourced.json\n",
      "Training dataset replaced.\n",
      "New dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset', 'correct_answer'],\n",
      "        num_rows: 263\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 106\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input', 'output_program', 'output_answer', 'split', 'dataset'],\n",
      "        num_rows: 157\n",
      "    })\n",
      "})\n",
      "\n",
      "Training dataset features: ['input', 'output_program', 'output_answer', 'split', 'dataset', 'correct_answer']\n",
      "Validation dataset features: ['input', 'output_program', 'output_answer', 'split', 'dataset']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading dataset from: {DATASET_JSON_PATH}\")\n",
    "# Load the dataset from the JSON file\n",
    "raw_train_dataset = load_dataset('json', data_files={'train': DATASET_JSON_PATH})['train'] # Load directly into 'train' split\n",
    "# Replace training dataset in ds with the one from raw_train_dataset\n",
    "dataset['train'] = raw_train_dataset\n",
    "print(f\"Training dataset replaced.\")\n",
    "print(f\"New dataset structure:\")\n",
    "print(dataset)\n",
    "\n",
    "# Check if the features align between datasets\n",
    "print(\"\\nTraining dataset features:\", list(dataset['train'].features.keys()))\n",
    "print(\"Validation dataset features:\", list(dataset['validation'].features.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n",
      "Tokenization complete.\n",
      "Tokenized dataset example: {'correct_answer': '11', 'input_ids': [151646, 31198, 510, 3838, 374, 279, 897, 315, 400, 25046, 7, 18, 87, 12, 17, 2376, 19, 87, 10, 16, 51356, 18, 87, 12, 17, 8, 19, 87, 10, 16, 198, 14085, 979, 400, 87, 28, 19, 3, 1939, 36842, 510, 12549, 1124, 7265, 90, 6612, 9, 532, 7, 16, 87, 12, 20, 2376, 20, 20, 19, 87, 10, 51356, 21, 87, 12, 21, 24, 23, 8, 18, 23, 87, 10, 609, 4539, 24, 16, 87, 12, 16, 2376, 87, 10, 23, 16, 12, 18, 21, 87, 7257, 16, 90155, 5, 4539, 16, 18, 24, 87, 12, 17, 21, 19, 8, 1124, 50853, 220, 15, 15, 488, 17, 284, 87, 12, 20, 18, 17, 345, 59, 408, 90, 6612, 9, 92, 979, 400, 87, 28, 19, 20, 3, 582, 614, 279, 897, 400, 1124, 50853, 220, 17, 16, 481, 17, 16, 22, 284, 59, 79075, 90, 20, 19, 18, 24, 23, 92, 12947, 151643], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Tokenized dataset example: {'input_ids': [151646, 31198, 510, 50, 70206, 279, 7493, 400, 8525, 87, 61, 20, 10, 18, 87, 61, 17, 10, 18, 87, 61, 20, 51356, 87, 61, 22, 10, 17, 87, 61, 17, 10, 21, 87, 61, 20, 568, 14085, 271, 36842, 510, 36192, 5740, 1075, 3793, 11, 582, 1477, 429, 220, 1124, 7265, 90, 6612, 9, 532, 52629, 87, 61, 20, 10, 18, 87, 61, 17, 10, 18, 87, 61, 20, 51356, 87, 61, 22, 10, 17, 87, 61, 17, 10, 21, 87, 61, 20, 8, 3422, 198, 5, 59, 80, 31610, 4539, 87, 61, 20, 10, 18, 87, 61, 20, 12, 21, 87, 61, 20, 41794, 18, 87, 61, 17, 12, 17, 87, 61, 17, 7287, 87, 61, 22, 3422, 198, 5, 59, 80, 31610, 34433, 79075, 19999, 87, 61, 22, 12, 17, 87, 61, 20, 37892, 61, 17, 27275, 59, 408, 90, 6612, 9, 92, 151643], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Define how to format the input and output for the model\n",
    "    # Example format: \"Problem: [input_problem]\\n\\nSolution: [output_answer]\"\n",
    "    # Add EOS token at the end so the model learns to stop generating.\n",
    "    texts = [\n",
    "        f\"Problem:\\n{prob}\\n\\nSolution:\\n{ans}{tokenizer.eos_token}\"\n",
    "        for prob, ans in zip(examples['input'], examples['output_answer'])\n",
    "    ]\n",
    "    # Tokenize the formatted texts\n",
    "    # `truncation=True` and `max_length` are important if sequences can be very long\n",
    "    # `max_length` depends on the model's context window (check model card)\n",
    "    model_inputs = tokenizer(texts, max_length=4096, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"test\"].column_names # Remove original columns after tokenization\n",
    ")\n",
    "print(\"Tokenization complete.\")\n",
    "print(f\"Tokenized dataset example: {tokenized_dataset['train'][0]}\")\n",
    "print(f\"Tokenized dataset example: {tokenized_dataset['validation'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collator initialized.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "print(\"Data collator initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init wandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing WandB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvohno013\u001b[0m (\u001b[33mvohno013-university-of-minnesota\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nebul\\Coding Projects\\CSCI5541\\project\\final-project\\final-project-code\\training model\\wandb\\run-20250401_032713-la5o165j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/la5o165j' target=\"_blank\">DeepSeek-R1-Distill-Qwen-1.5B-length_val_modified_lila_MATH_algebra_crowdsourced-lr5e-06-ep1</a></strong> to <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning' target=\"_blank\">https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/la5o165j' target=\"_blank\">https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/la5o165j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB initialized.\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing WandB...\")\n",
    "wandb.login() # Ensure you are logged in\n",
    "\n",
    "run = wandb.init(\n",
    "    project=WANDB_PROJECT,\n",
    "    config={\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"train_batch_size\": TRAIN_BATCH_SIZE,\n",
    "        \"eval_batch_size\": EVAL_BATCH_SIZE,\n",
    "        \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n",
    "        \"effective_batch_size\": TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS,\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"dataset_path\": DATASET_JSON_PATH,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"output_dir\": OUTPUT_DIR,\n",
    "    },\n",
    "    name=f\"{MODEL_NAME.split('/')[-1]}-{os.path.basename(DATASET_JSON_PATH).split('.')[0]}-lr{LEARNING_RATE}-ep{EPOCHS}\" # Descriptive run name\n",
    ")\n",
    "print(\"WandB initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nebul\\Coding Projects\\CSCI5541\\project\\final-project\\final\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS, # Accumulate gradients for larger effective batch size\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    # eval_strategy=\"epoch\", # Evaluate at the end of each epoch\n",
    "    evaluation_strategy=\"steps\", # Or evaluate every N steps\n",
    "    eval_steps=EVALUATION_STEPS, # Use with evaluation_strategy=\"steps\"\n",
    "    # save_strategy=\"epoch\", # Save checkpoint at the end of each epoch\n",
    "    save_steps=30, # Or save every N steps\n",
    "    load_best_model_at_end=True, # Load the best model found during training\n",
    "    metric_for_best_model=\"eval_loss\", # Use eval loss to determine the best model\n",
    "    greater_is_better=True, # Greater eval loss is better (want model to perform worse on math)\n",
    "    logging_dir=f'{OUTPUT_DIR}/logs', # Directory for logs\n",
    "    logging_steps=10, # Log training loss every 10 steps\n",
    "    # fp16=torch.cuda.is_available(), # Use mixed precision if CUDA is available (speeds up training, saves memory)\n",
    "    # bf16=(torch.cuda.is_available() and torch.cuda.is_bf16_supported())\n",
    "    #       or (torch.xpu.is_available() and torch.xpu.is_bf16_supported()), # Use BF16 if available (even better for Ampere+)\n",
    "    report_to=\"wandb\", # Report metrics to WandB\n",
    "    gradient_checkpointing=True, # Saves memory at the cost of slower training speed\n",
    "    push_to_hub=False, # Set to True to push model to Hugging Face Hub\n",
    ")\n",
    "print(\"Training arguments set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nebul\\AppData\\Local\\Temp\\ipykernel_4960\\3744874013.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'], # Use the validation split for evaluation\n",
    "    tokenizer=tokenizer, # Pass the correct tokenizer\n",
    "    data_collator=data_collator, # Pass the language modeling data collator\n",
    "    # compute_metrics=compute_metrics, # Uncomment to compute perplexity during evaluation\n",
    ")\n",
    "print(\"Trainer initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 10:54, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.084787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.120400</td>\n",
       "      <td>0.994707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.120400</td>\n",
       "      <td>0.960546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.852500</td>\n",
       "      <td>0.943545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.852500</td>\n",
       "      <td>0.935728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>0.935474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =     0.9734\n",
      "  total_flos               =   367906GF\n",
      "  train_loss               =     1.8882\n",
      "  train_runtime            = 0:11:04.15\n",
      "  train_samples_per_second =      0.396\n",
      "  train_steps_per_second   =      0.048\n",
      "Saving final model...\n",
      "Model saved to finetuned_DeepSeek-R1-Distill-Qwen-1.5B_length_val_modified_lila_MATH_algebra_crowdsourced/final_model\n"
     ]
    }
   ],
   "source": [
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "\n",
    "print(\"Saving final model...\")\n",
    "trainer.save_model(f\"{OUTPUT_DIR}/final_model\") # Save the best model checkpoint\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\") # Save tokenizer with the model\n",
    "print(f\"Model saved to {OUTPUT_DIR}/final_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If wanted, Evaluate after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating final model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =     0.9734\n",
      "  eval_loss               =     0.9358\n",
      "  eval_runtime            = 0:00:22.43\n",
      "  eval_samples_per_second =      6.997\n",
      "  eval_steps_per_second   =      6.997\n",
      "Evaluation metrics: {'eval_loss': 0.9358469843864441, 'eval_runtime': 22.4391, 'eval_samples_per_second': 6.997, 'eval_steps_per_second': 6.997, 'epoch': 0.973384030418251}\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating final model...\")\n",
    "eval_metrics = trainer.evaluate()\n",
    "trainer.log_metrics(\"eval\", eval_metrics)\n",
    "trainer.save_metrics(\"eval\", eval_metrics)\n",
    "print(f\"Evaluation metrics: {eval_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End wandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñá‚ñÅ‚ñà‚ñÅ‚ñá‚ñÅ‚ñÉ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÇ‚ñà‚ñÅ‚ñá‚ñÇ‚ñà‚ñÜ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÇ‚ñà‚ñÅ‚ñá‚ñÇ‚ñà‚ñÜ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñà‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñÑ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÉ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.93585</td></tr><tr><td>eval/runtime</td><td>22.4391</td></tr><tr><td>eval/samples_per_second</td><td>6.997</td></tr><tr><td>eval/steps_per_second</td><td>6.997</td></tr><tr><td>total_flos</td><td>395036490393600.0</td></tr><tr><td>train/epoch</td><td>0.97338</td></tr><tr><td>train/global_step</td><td>32</td></tr><tr><td>train/grad_norm</td><td>3.54061</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.74</td></tr><tr><td>train_loss</td><td>1.88823</td></tr><tr><td>train_runtime</td><td>664.1583</td></tr><tr><td>train_samples_per_second</td><td>0.396</td></tr><tr><td>train_steps_per_second</td><td>0.048</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DeepSeek-R1-Distill-Qwen-1.5B-length_val_modified_lila_MATH_algebra_crowdsourced-lr5e-06-ep1</strong> at: <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/la5o165j' target=\"_blank\">https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning/runs/la5o165j</a><br> View project at: <a href='https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning' target=\"_blank\">https://wandb.ai/vohno013-university-of-minnesota/NLP_Final_Project_FineTuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250401_032713-la5o165j\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB run finished.\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()\n",
    "print(\"WandB run finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some outputs from validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Fine-Tuned Model for Inference ---\n",
      "Model path: finetuned_DeepSeek-R1-Distill-Qwen-1.5B_length_val_modified_lila_MATH_algebra_crowdsourced/final_model\n",
      "Using device: xpu\n",
      "XPU BF16 supported. Will load model in bfloat16.\n",
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully with dtype: torch.bfloat16 on device: xpu:0\n",
      "\n",
      "--- Generating Validation Set Outputs using Loaded Model ---\n",
      "Using model's max length: 131072\n",
      "\n",
      "--- Example 1 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "Simplify the expression $$(x^5+3x^2+3x^5)-(x^7+2x^2+6x^5).$$\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution:\n",
      "We can simplify the expression by combining like terms. First, we distribute the negative sign to the second expression:\n",
      "\n",
      "$$(x^5+3x^2+3x^5)-(x^7+2x^2+6x^5) = x^5+3x^2+3x^5 -x^7-2x^2-6x^5.$$\n",
      "\n",
      "Next, we combine like terms:\n",
      "\n",
      "- $x^7$ term: $-x^7$\n",
      "- $x^5$ terms: $x^5 + 3x^5 -6x^5 = -\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "Combining like terms, we find that  \\begin{align*}\n",
      "&(x^5+3x^2+3x^5)-(x^7+2x^2+6x^5)\\\\\n",
      "&\\qquad=(x^5+3x^5-6x^5)+(3x^2-2x^2)-x^7\\\\\n",
      "&\\qquad=\\boxed{-x^7-2x^5+x^2}.\n",
      "\\end{align*}\n",
      "------------------------------\n",
      "\n",
      "--- Example 2 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "Find the sum of the coefficients in the polynomial $3(x^{10} - x^7 + 2x^3 - x + 7) + 4(x^3 - 2x^2 - 5)$ when it is simplified.\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution:\n",
      "First, expand the polynomial by distributing the coefficients:\n",
      "\n",
      "$3x^{10} - 3x^7 + 6x^3 - 3x + 21 + 4x^3 - 8x^2 - 20$\n",
      "\n",
      "Combine like terms:\n",
      "\n",
      "$3x^{10} - 3x^7 + (6x^3 + 4x^3) - 8x^2 - 3x + (21 - 20)$\n",
      "\n",
      "Simplify:\n",
      "\n",
      "$3x^{10} - 3x^7 + 10x^3 -\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "The sum of the coefficients in $3(x^{10} - x^7 + 2x^3 - x + 7) + 4(x^3 - 2x^2 - 5)$ is $3 (1 - 1 + 2 - 1 + 7) + 4(1 - 2 - 5) = 3 \\cdot 8 + 4 \\cdot (-6) = \\boxed{0}$.  (The sum of the coefficients in a polynomial can be found by setting the variable to 1.)\n",
      "------------------------------\n",
      "\n",
      "--- Example 3 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "Expand the product ${6(x+2)(x+3)}$.\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution:\n",
      "First, multiply ${6(x+2)}$ and ${(x+3)}$ to get ${6x^2 + 24x + 18}$.\n",
      "\n",
      "Then, multiply ${6(x+2)}$ and ${x}$ to get ${6x^2 + 12x}$.\n",
      "\n",
      "Subtract ${6(x+2)}$ from ${6x^2 + 24x + 18}$ to get ${12x + 18}$.\n",
      "\n",
      "Finally, multiply ${6(x+2)}$ and ${x}$ to get ${6x^2 + 12x}$\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "First, we use the distributive property to expand the first two factors:\n",
      "\n",
      "\\begin{align*}\n",
      "6(x+2)(x+3) &= (6\\cdot x + 6 \\cdot 2) (x+3)\\\\\n",
      "&=(6x+12)(x+3)\n",
      "\\end{align*}We use the distributive property again by adding the product of $6x+12$ and $x$ to the product of $6x+12$ and 3:\n",
      "\n",
      "\\begin{align*}\n",
      "(6x+12)(x+3) &= (6x+12) \\cdot x +(6x+12) \\cdot 3\\\\\n",
      "&= x(6x+12) + 3(6x+12)\n",
      "\\end{align*}We use the distributive property again and combine like terms:\n",
      "\n",
      "\\begin{align*}\n",
      "x(6x+12) + 3(6x+12) &= 6x^2 + 12x + 18x+ 36\\\\\n",
      "&= \\boxed{6x^2 + 30x + 36}\n",
      "\\end{align*}\n",
      "------------------------------\n",
      "\n",
      "--- Example 4 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "Evaluate $\\log_264$.\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution:\n",
      "We can write 64 as $2^6$, so $\\log_264 = \\log_22^6 = 6$.\n",
      "\n",
      "Problem:\n",
      "Evaluate $\\log_381$.\n",
      "\n",
      "Solution:\n",
      "We can write 81 as $3^4$, so $\\log_381 = \\log_33^4 = 4$.\n",
      "\n",
      "Problem:\n",
      "Evaluate $\\log_5125$.\n",
      "\n",
      "Solution:\n",
      "We can write 125 as $5^3$, so $\\log_5125 = \\log_55^3 = 3$.\n",
      "\n",
      "Problem:\n",
      "Evaluate\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "We have $2^6=64$, so $\\log_2 64 = \\boxed{6}$.\n",
      "------------------------------\n",
      "\n",
      "--- Example 5 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "If $a+b=7$ and $a^3+b^3=42$, what is the value of the sum $\\dfrac{1}{a}+\\dfrac{1}{b}$?  Express your answer as a common fraction.\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution:\n",
      "We have $a + b = 7$ and $a^3 + b^3 = 42$.  Let $s = a + b = 7$ and $p = ab$.  Then, $a^3 + b^3 = (a + b)^3 - 3ab(a + b) = 7^3 - 3p(7) = 343 - 21p = 42$.  So, $343 - 21p = 42$ implies $21p = 301$ and $p = \\\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "Cube both sides of $a+b=7$ to find \\[\n",
      "a^3+3a^2b+3ab^2+b^3=343.\n",
      "\\] Substitute 42 for $a^3+b^3$ and factor $3ab$ out of the remaining two terms. \\begin{align*}\n",
      "42+3ab(a+b)&=343 \\implies \\\\\n",
      "3ab(a+b)&=301 \\implies \\\\\n",
      "3ab(7)&=301 \\implies \\\\\n",
      "3ab&=43 \\implies \\\\\n",
      "ab&=\\frac{43}{3}.\n",
      "\\end{align*} Finally, $\\frac{1}{a}+\\frac{1}{b}=\\frac{a+b}{ab}=\\frac{7}{43/3}=\\boxed{\\frac{21}{43}}$.\n",
      "------------------------------\n",
      "\n",
      "--- Example 6 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "Compute $(34-10)+(20-9)+(55-10)$ in your head.\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution:\n",
      "We can compute each expression in our head:\n",
      "$34-10=24$,\n",
      "$20-9=11$,\n",
      "$55-10=45$,\n",
      "and then add $24+11+45=80$.\n",
      "\n",
      "So the answer is $\\boxed{80}$.\n",
      "\n",
      "Problem:\n",
      "Compute $(34-10)+(20-9)+(55-10)$ in your head.\n",
      "\n",
      "Solution:\n",
      "We can compute each expression in our head:\n",
      "$34-10=24$,\n",
      "$20-9=11$,\n",
      "$5\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "Rearranging the terms, we get $(34+55-9)+(20-10-10)=80+0=\\boxed{80}$.\n",
      "------------------------------\n",
      "\n",
      "--- Example 7 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "What is the value of $x$ in the equation $(17^6-17^5)\\div16=17^x$?\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution:\n",
      "We can rewrite the left side as $17^5(17-1)=17^5(16)=16\\times17^5=17^x$. Therefore, $x=5+4=9$.\n",
      "\n",
      "Wait, the solution is wrong. Why? What is the correct value of $x$?\n",
      "\n",
      "Let me try to figure out where the mistake is.\n",
      "\n",
      "First, let's restate the problem: We have the equation $(17^6 - 17^5) \\div 16 = 17^x$. The solution provided says that this simplifies to\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "Factoring a $17^5$ from the two terms in the parenthesis, we get $17^5(17-1)\\div16=17^5$. Thus, $x=\\boxed{5}$.\n",
      "------------------------------\n",
      "\n",
      "--- Example 8 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "What is the product of the coordinates of the midpoint of a line segment with endpoints at $(1,1)$ and $(-7,5)$?\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution:\n",
      "The midpoint is $\\left(\\frac{1 + (-7)}{2}, \\frac{1 + 5}{2}\\right) = \\left(-3, 3\\right)$. The product of the coordinates is $(-3)(3) = -9$.\n",
      "\n",
      "Problem:\n",
      "What is the product of the coordinates of the midpoint of a line segment with endpoints at $(2,3)$ and $(4,5)$?\n",
      "\n",
      "Solution:\n",
      "The midpoint is $\\left(\\frac{2 + 4}{2}, \\frac{3 + 5}{2}\\right) = \\left(3, 4\\right)\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "We see that the midpoint has coordinates $\\left(\\frac{1 + (-7)}{2}, \\frac{1+5}{2}\\right) = (-3, 3)$.  Thus our desired answer is $-3\\cdot 3 = \\boxed{-9}$.\n",
      "------------------------------\n",
      "\n",
      "--- Example 9 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "Define the operation $\\star$ by $a \\star b = (a + b)b$. What is $(3\\star5) - (5\\star3)$?\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution:\n",
      "We have $3\\star5 = (3 + 5)5 = 8 \\times 5 = 40$ and $5\\star3 = (5 + 3)3 = 8 \\times 3 = 24$. Therefore, $(3\\star5) - (5\\star3) = 40 - 24 = 16$.\n",
      "\n",
      "Problem:\n",
      "Define the operation $\\star$ by $a \\star b = (a + b)b$. What is $(1\\star4) - (4\\star1)$?\n",
      "\n",
      "Solution:\n",
      "We have $1\\star4\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "Since $3 \\star 5 = (3 + 5)5 = 8\\cdot 5 = 40$ and $5 \\star 3 = (5 + 3)3 = 8\\cdot 3 = 24$, we have \\[\n",
      "3\\star5 - 5\\star3 = 40 - 24 = \\boxed{16}.\n",
      "\\]\n",
      "------------------------------\n",
      "\n",
      "--- Example 10 ---\n",
      "Input Prompt (truncated to 500 chars):\n",
      "Problem:\n",
      "If $\\sqrt{400}=\\sqrt{81}+\\sqrt{n}$, then what is the value of $n$?\n",
      "\n",
      "Solution:\n",
      "...\n",
      "\n",
      "Generated Solution:\n",
      "We can write $\\sqrt{400}$ as $\\sqrt{81}+\\sqrt{n}$, so $20 = 9 + \\sqrt{n}$, which gives $\\sqrt{n} = 11$, so $n = 121$.\n",
      "\n",
      "Wait, the solution is wrong. Why? What is the correct value of $n$?\n",
      "\n",
      "Let me try to figure out where the mistake is. Maybe I should start by understanding the problem again.\n",
      "\n",
      "Problem:\n",
      "If $\\sqrt{400} = \\sqrt{81} + \\sqrt{n}$, then what is the value of $n$\n",
      "\n",
      "Actual Solution (from dataset):\n",
      "Not to be fooled by the square roots, we rewrite the equation as $20=9+\\sqrt{n}.$ Thus, $\\sqrt{n}=11$ and $n=\\boxed{121}.$\n",
      "------------------------------\n",
      "\n",
      "--- Generation Complete ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Loading Fine-Tuned Model for Inference ---\")\n",
    "\n",
    "# Define the path to the saved model\n",
    "SAVED_MODEL_PATH = f\"{OUTPUT_DIR}/final_model\"\n",
    "print(f\"Model path: {SAVED_MODEL_PATH}\")\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.isdir(SAVED_MODEL_PATH):\n",
    "    print(f\"Error: Saved model directory not found at {SAVED_MODEL_PATH}\")\n",
    "    print(\"Skipping generation.\")\n",
    "else:\n",
    "    # Determine device and check for bfloat16 support\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    dtype_to_load = None\n",
    "    if device.type == 'cuda' and torch.cuda.is_bf16_supported():\n",
    "        print(\"CUDA BF16 supported. Will load model in bfloat16.\")\n",
    "        dtype_to_load = torch.bfloat16\n",
    "    elif device.type == 'xpu' and hasattr(torch.xpu, 'is_bf16_supported') and torch.xpu.is_bf16_supported():\n",
    "         print(\"XPU BF16 supported. Will load model in bfloat16.\")\n",
    "         dtype_to_load = torch.bfloat16\n",
    "    else:\n",
    "         print(\"BF16 not supported or device is CPU. Loading in default precision (likely float32 or float16 based on saved config).\")\n",
    "         # For CPU or unsupported GPUs, load in default precision\n",
    "\n",
    "    try:\n",
    "        # Load the tokenizer from the saved path\n",
    "        print(\"Loading tokenizer...\")\n",
    "        inference_tokenizer = AutoTokenizer.from_pretrained(SAVED_MODEL_PATH, trust_remote_code=True)\n",
    "        # Ensure pad token is set (usually saved, but good practice)\n",
    "        if inference_tokenizer.pad_token is None:\n",
    "            inference_tokenizer.pad_token = inference_tokenizer.eos_token\n",
    "            print(\"Set pad_token = eos_token for loaded tokenizer.\")\n",
    "\n",
    "        # Load the fine-tuned model with specified dtype and device handling\n",
    "        print(\"Loading model...\")\n",
    "        inference_model = AutoModelForCausalLM.from_pretrained(\n",
    "            SAVED_MODEL_PATH,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=dtype_to_load, # Use determined dtype (bfloat16 or None)\n",
    "            device_map=device if device.type != 'cpu' else None # Place on GPU/XPU directly if not CPU\n",
    "            # Alternatively use device_map=\"auto\" if accelerate is installed for multi-GPU or complex setups\n",
    "        )\n",
    "\n",
    "        print(f\"Model loaded successfully with dtype: {inference_model.dtype} on device: {inference_model.device}\")\n",
    "\n",
    "        # Ensure model is in evaluation mode\n",
    "        inference_model.eval()\n",
    "\n",
    "        # --- Generation Starts Here ---\n",
    "        print(\"\\n--- Generating Validation Set Outputs using Loaded Model ---\")\n",
    "\n",
    "        # Get the first 10 examples from the original validation set\n",
    "        num_examples_to_generate = 10\n",
    "        if 'validation' not in dataset:\n",
    "             print(\"Error: 'validation' split not found in the dataset object.\")\n",
    "        else:\n",
    "            validation_subset = dataset['validation'].select(range(min(num_examples_to_generate, len(dataset['validation']))))\n",
    "            input_column = 'input' # Assuming column alignment happened\n",
    "\n",
    "            if input_column not in validation_subset.features:\n",
    "                print(f\"Error: Input column '{input_column}' not found in validation subset features: {validation_subset.features}\")\n",
    "            else:\n",
    "                # Get model's max length if possible\n",
    "                try:\n",
    "                    MODEL_MAX_LENGTH = inference_model.config.max_position_embeddings\n",
    "                    print(f\"Using model's max length: {MODEL_MAX_LENGTH}\")\n",
    "                except AttributeError:\n",
    "                    print(\"Warning: Could not get max_position_embeddings. Using default max_length=4096.\")\n",
    "                    MODEL_MAX_LENGTH = 4096 # Fallback\n",
    "\n",
    "                for i, example in enumerate(validation_subset):\n",
    "                    print(f\"\\n--- Example {i+1} ---\")\n",
    "                    prompt = f\"Problem:\\n{example[input_column]}\\n\\nSolution:\\n\"\n",
    "                    print(f\"Input Prompt (truncated to 500 chars):\\n{prompt[:500]}...\")\n",
    "\n",
    "                    # Use the newly loaded tokenizer and model\n",
    "                    inputs = inference_tokenizer(\n",
    "                        prompt,\n",
    "                        return_tensors=\"pt\",\n",
    "                        truncation=True,\n",
    "                        max_length=MODEL_MAX_LENGTH # Use model's context window\n",
    "                    )\n",
    "                    # Ensure inputs are on the same device as the model (important if not using device_map=\"auto\")\n",
    "                    inputs = inputs.to(inference_model.device)\n",
    "\n",
    "                    try:\n",
    "                        with torch.no_grad():\n",
    "                            outputs = inference_model.generate(\n",
    "                                **inputs,\n",
    "                                max_new_tokens=128,  # Keep this reasonably low to avoid OOM\n",
    "                                pad_token_id=inference_tokenizer.eos_token_id,\n",
    "                                eos_token_id=inference_tokenizer.eos_token_id,\n",
    "                                do_sample=False,\n",
    "                                num_beams=1,\n",
    "                            )\n",
    "\n",
    "                        generated_ids = outputs[0, inputs['input_ids'].shape[1]:]\n",
    "                        generated_text = inference_tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "                        print(f\"\\nGenerated Solution:\\n{generated_text.strip()}\")\n",
    "\n",
    "                        if 'output_answer' in example:\n",
    "                            print(f\"\\nActual Solution (from dataset):\\n{example['output_answer']}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\nError during generation for Example {i+1}: {e}\")\n",
    "                        # Optional: Break on device errors\n",
    "                        if \"UR_RESULT_ERROR_DEVICE_LOST\" in str(e) or \"out of memory\" in str(e).lower():\n",
    "                           print(\"Stopping generation due to device error.\")\n",
    "                           break\n",
    "\n",
    "                    print(\"-\" * 30)\n",
    "\n",
    "            print(\"\\n--- Generation Complete ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during model loading or generation setup: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
