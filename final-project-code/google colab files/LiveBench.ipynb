{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5Kh6VfIVQRS",
        "outputId": "24fd6419-f8a1-49a5-b6ae-99b804f3772d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVd8ysYvVS09"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "# --- Global Configuration ---\n",
        "# Base path on your Google Drive\n",
        "GDRIVE_BASE_PATH = \"/content/drive/MyDrive/Colab_Notebooks/LiveBenchRun\"\n",
        "\n",
        "# Path where the LiveBench repository lives/will be cloned\n",
        "LIVEBENCH_REPO_PATH = os.path.join(GDRIVE_BASE_PATH, \"LiveBench\")\n",
        "LIVEBENCH_SUBDIR_PATH = os.path.join(LIVEBENCH_REPO_PATH, \"livebench\")\n",
        "\n",
        "# Path for model weights parent directory\n",
        "MODELS_PARENT_PATH = os.path.join(GDRIVE_BASE_PATH, \"models\")\n",
        "\n",
        "# Port for the vLLM server (consistent across models)\n",
        "VLLM_PORT = 8000\n",
        "\n",
        "# GPU Utilization for vLLM\n",
        "GPU_UTILIZATION = 0.9\n",
        "\n",
        "# LiveBench Run Parameters (consistent across models)\n",
        "BENCH_NAME = \"live_bench\"\n",
        "API_BASE_URL = f\"http://localhost:{VLLM_PORT}/v1\"\n",
        "API_KEY = \"dummy-key\"\n",
        "LIVEBENCH_RELEASE = \"2024-11-25\"\n",
        "MAX_TOKENS = 8192\n",
        "PARALLEL_REQUESTS = 32\n",
        "\n",
        "# --- Model Specific Configurations ---\n",
        "# List of models to download and evaluate.\n",
        "# Each dictionary needs 'hf_id' (Hugging Face ID) and 'local_name' (simple name for tracking).\n",
        "MODEL_CONFIGS = [\n",
        "    {\n",
        "        \"hf_id\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "        \"local_name\": \"deepseek-1.5b-local\"\n",
        "    },\n",
        "    {\n",
        "        \"hf_id\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "        \"local_name\": \"deepseek-r1-distill-qwen-1.5b-val-modified\"\n",
        "    },\n",
        "    {\n",
        "        \"hf_id\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "        \"local_name\": \"deepseek-r1-distill-qwen-1.5b-scrambled\"\n",
        "    },\n",
        "    {\n",
        "        \"hf_id\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "        \"local_name\": \"deepseek-r1-distill-qwen-1.5b-length-val-modified\"\n",
        "    },\n",
        "    {\n",
        "        \"hf_id\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "        \"local_name\": \"deepseek-r1-distill-qwen-1.5b-gradient-ascent\"\n",
        "    },\n",
        "    {\n",
        "        \"hf_id\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "        \"local_name\": \"deepseek-r1-distill-qwen-1.5b-ft-control\"\n",
        "    },\n",
        "    {\n",
        "        \"hf_id\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "        \"local_name\": \"deepseek-r1-distill-qwen-1.5b-reduced-eos-gradient-ascent\"\n",
        "    },\n",
        "    # Add more model dictionaries here if needed\n",
        "]\n",
        "\n",
        "# --- Setup Logging ---\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# --- Create Base Directories ---\n",
        "os.makedirs(GDRIVE_BASE_PATH, exist_ok=True)\n",
        "os.makedirs(MODELS_PARENT_PATH, exist_ok=True) # Create parent 'models' dir\n",
        "\n",
        "logging.info(f\"Google Drive Base Path: {GDRIVE_BASE_PATH}\")\n",
        "logging.info(f\"LiveBench Repo Path: {LIVEBENCH_REPO_PATH}\")\n",
        "logging.info(f\"Models Parent Path: {MODELS_PARENT_PATH}\")\n",
        "logging.info(f\"vLLM Server Port: {VLLM_PORT}\")\n",
        "logging.info(f\"Models to process: {len(MODEL_CONFIGS)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybzxWjtghpO5",
        "outputId": "b8df8e54-ed9f-4e91-e54a-d61d139efe81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab_Notebooks/LiveBenchRun\n",
            "LiveBench repository already exists.\n",
            "/content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/LiveBench\n",
            "/content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/LiveBench\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Navigate to the base Google Drive path\n",
        "%cd {GDRIVE_BASE_PATH}\n",
        "\n",
        "# Clone LiveBench repository IF it doesn't exist\n",
        "if not os.path.exists(LIVEBENCH_REPO_PATH):\n",
        "  print(\"Cloning LiveBench repository...\")\n",
        "  !git clone https://github.com/LiveBench/LiveBench.git\n",
        "else:\n",
        "  print(\"LiveBench repository already exists.\")\n",
        "\n",
        "# Navigate into the LiveBench repository directory\n",
        "%cd {LIVEBENCH_REPO_PATH}\n",
        "!pwd # Verify we are in the correct directory on Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFInmNh03DZH",
        "outputId": "ef16292f-3db6-4102-c050-a992fd63430b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Installing Dependencies ---\n",
            "Obtaining file:///content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/LiveBench\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fschat@ git+https://github.com/lm-sys/FastChat#egg=c5223e34babd24c3f9b08205e6751ea6e42c9684 (from livebench==0.0.4)\n",
            "  Cloning https://github.com/lm-sys/FastChat to /tmp/pip-install-clr2cmqy/fschat_1833baf2cfae40eea7a678db7deaf679\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/lm-sys/FastChat /tmp/pip-install-clr2cmqy/fschat_1833baf2cfae40eea7a678db7deaf679\n",
            "  Resolved https://github.com/lm-sys/FastChat to commit 0e6d3e4beaab66f4d3f93db72541a4abab8af28d\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate>=0.21 in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (1.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (3.11.15)\n",
            "Requirement already satisfied: anthropic>=0.3 in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (0.50.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.11 in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (4.11.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (3.5.1)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (0.115.12)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (0.28.1)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (4.2.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (0.3.24)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (1.0.9)\n",
            "Requirement already satisfied: levenshtein>=0.20.4 in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (0.27.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (5.4.0)\n",
            "Requirement already satisfied: markdown2[all] in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (2.5.3)\n",
            "Requirement already satisfied: nh3 in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (0.2.21)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (2.0.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (1.76.0)\n",
            "Requirement already satisfied: fastchat in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (0.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (24.2)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (2.2.2)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (0.15.2)\n",
            "Requirement already satisfied: prompt_toolkit>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (3.0.51)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (4.25.7)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (2.11.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (5.9.5)\n",
            "Requirement already satisfied: ray in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (2.45.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (2.32.3)\n",
            "Requirement already satisfied: rich>=10.0.0 in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (13.9.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (0.2.0)\n",
            "Requirement already satisfied: shortuuid in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (1.0.13)\n",
            "Requirement already satisfied: sympy>=1.12 in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (1.13.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (4.51.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (4.67.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (0.34.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (0.45.1)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (9.1.2)\n",
            "Requirement already satisfied: lark in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (1.2.2)\n",
            "Requirement already satisfied: libtmux in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (0.46.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from livebench==0.0.4) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->livebench==0.0.4) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->livebench==0.0.4) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->livebench==0.0.4) (2025.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.21->livebench==0.0.4) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.21->livebench==0.0.4) (0.5.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic>=0.3->livebench==0.0.4) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic>=0.3->livebench==0.0.4) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic>=0.3->livebench==0.0.4) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic>=0.3->livebench==0.0.4) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic>=0.3->livebench==0.0.4) (4.13.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->livebench==0.0.4) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->livebench==0.0.4) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->livebench==0.0.4) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->livebench==0.0.4) (0.16.0)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from levenshtein>=0.20.4->livebench==0.0.4) (3.13.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt_toolkit>=3.0.0->livebench==0.0.4) (0.2.13)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->livebench==0.0.4) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic->livebench==0.0.4) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->livebench==0.0.4) (0.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.0.0->livebench==0.0.4) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.0.0->livebench==0.0.4) (2.19.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.12->livebench==0.0.4) (1.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->livebench==0.0.4) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->livebench==0.0.4) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->livebench==0.0.4) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->livebench==0.0.4) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->livebench==0.0.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->livebench==0.0.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->livebench==0.0.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->livebench==0.0.4) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->livebench==0.0.4) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->livebench==0.0.4) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->livebench==0.0.4) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->livebench==0.0.4) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->livebench==0.0.4) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->livebench==0.0.4) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->livebench==0.0.4) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->livebench==0.0.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->livebench==0.0.4) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->livebench==0.0.4) (3.2.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->livebench==0.0.4) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->livebench==0.0.4) (0.21.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->livebench==0.0.4) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->livebench==0.0.4) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->livebench==0.0.4) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->livebench==0.0.4) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->livebench==0.0.4) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->livebench==0.0.4) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->livebench==0.0.4) (1.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->livebench==0.0.4) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->livebench==0.0.4) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->livebench==0.0.4) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->livebench==0.0.4) (0.70.16)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->livebench==0.0.4) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->livebench==0.0.4) (2.4.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi->livebench==0.0.4) (0.46.2)\n",
            "Requirement already satisfied: pydantic-settings in /usr/local/lib/python3.11/dist-packages (from fschat@ git+https://github.com/lm-sys/FastChat#egg=c5223e34babd24c3f9b08205e6751ea6e42c9684->livebench==0.0.4) (2.9.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain->livebench==0.0.4) (0.3.56)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain->livebench==0.0.4) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain->livebench==0.0.4) (0.3.38)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain->livebench==0.0.4) (2.0.40)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect->livebench==0.0.4) (1.17.0)\n",
            "Requirement already satisfied: wavedrom in /usr/local/lib/python3.11/dist-packages (from markdown2[all]->livebench==0.0.4) (2.0.3.post3)\n",
            "Requirement already satisfied: latex2mathml in /usr/local/lib/python3.11/dist-packages (from markdown2[all]->livebench==0.0.4) (3.78.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->livebench==0.0.4) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->livebench==0.0.4) (1.4.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray->livebench==0.0.4) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray->livebench==0.0.4) (1.1.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain->livebench==0.0.4) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->livebench==0.0.4) (3.10.17)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->livebench==0.0.4) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->livebench==0.0.4) (0.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.0.0->livebench==0.0.4) (0.1.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain->livebench==0.0.4) (3.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->livebench==0.0.4) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray->livebench==0.0.4) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray->livebench==0.0.4) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray->livebench==0.0.4) (0.24.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings->fschat@ git+https://github.com/lm-sys/FastChat#egg=c5223e34babd24c3f9b08205e6751ea6e42c9684->livebench==0.0.4) (1.1.0)\n",
            "Requirement already satisfied: svgwrite in /usr/local/lib/python3.11/dist-packages (from wavedrom->markdown2[all]->livebench==0.0.4) (1.4.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain->livebench==0.0.4) (3.0.0)\n",
            "Building wheels for collected packages: livebench\n",
            "  Building editable for livebench (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for livebench: filename=livebench-0.0.4-0.editable-py3-none-any.whl size=15975 sha256=aac4c74af2542178ec69ca78c15161c6030f8e1a90fb6be761ed265939e32095\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-34hlzg69/wheels/53/e7/b1/4ab6e8e7a767cc476b47ff2664395a74651e1deba20a14a9a6\n",
            "Successfully built livebench\n",
            "Installing collected packages: livebench\n",
            "  Attempting uninstall: livebench\n",
            "    Found existing installation: livebench 0.0.4\n",
            "    Uninstalling livebench-0.0.4:\n",
            "      Successfully uninstalled livebench-0.0.4\n",
            "Successfully installed livebench-0.0.4\n",
            "Requirement already satisfied: vllm in /usr/local/lib/python3.11/dist-packages (0.8.5.post1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from vllm) (5.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vllm) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm) (4.67.1)\n",
            "Requirement already satisfied: blake3 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.0.4)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.51.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.51.3)\n",
            "Requirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm) (4.25.7)\n",
            "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.115.12)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm) (3.11.15)\n",
            "Requirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.76.0)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.11.3)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (11.2.1)\n",
            "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (7.1.0)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.0)\n",
            "Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.11 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.10.11)\n",
            "Requirement already satisfied: llguidance<0.8.0,>=0.7.9 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.7.19)\n",
            "Requirement already satisfied: outlines==0.1.11 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.1.11)\n",
            "Requirement already satisfied: lark==1.2.2 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.2.2)\n",
            "Requirement already satisfied: xgrammar==0.1.18 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.1.18)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.13.2)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (3.18.0)\n",
            "Requirement already satisfied: partial-json-parser in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.1.1.post5)\n",
            "Requirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (26.4.0)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.11/dist-packages (from vllm) (0.19.0)\n",
            "Requirement already satisfied: gguf>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.16.2)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from vllm) (8.0.0)\n",
            "Requirement already satisfied: mistral_common>=1.5.4 in /usr/local/lib/python3.11/dist-packages (from mistral_common[opencv]>=1.5.4->vllm) (1.5.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.11.0.86)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm) (6.0.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm) (0.8.1)\n",
            "Requirement already satisfied: compressed-tensors==0.9.3 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.3)\n",
            "Requirement already satisfied: depyf==0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.18.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm) (3.1.1)\n",
            "Requirement already satisfied: watchfiles in /usr/local/lib/python3.11/dist-packages (from vllm) (1.0.5)\n",
            "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.11/dist-packages (from vllm) (3.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.15.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from vllm) (1.11.1.4)\n",
            "Requirement already satisfied: opentelemetry-sdk<1.27.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-api<1.27.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp<1.27.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions-ai<0.5.0,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.4.5)\n",
            "Requirement already satisfied: numba==0.61.2 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.61.2)\n",
            "Requirement already satisfied: ray!=2.44.*,>=2.43.0 in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (2.45.0)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio==2.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision==0.21.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.0+cu124)\n",
            "Requirement already satisfied: xformers==0.0.29.post2 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.0.29.post2)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.8.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.3.8)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba==0.61.2->vllm) (0.44.0)\n",
            "Requirement already satisfied: interegular in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (3.1.6)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (5.6.3)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.36.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (4.23.0)\n",
            "Requirement already satisfied: pycountry in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (24.6.1)\n",
            "Requirement already satisfied: airportsdata in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (20250224)\n",
            "Requirement already satisfied: outlines_core==0.1.26 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.1.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->vllm) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (24.2)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.46.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.0.7)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
            "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.2.0)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.34.2)\n",
            "Requirement already satisfied: hf-xet>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[hf_xet]>=0.30.0->vllm) (1.1.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.3.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<1.27.0,>=1.26.0->vllm) (1.2.18)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->vllm) (3.21.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.26.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.26.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.26.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.71.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.26.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.26.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<1.27.0,>=1.26.0->vllm) (0.47b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (0.4.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (8.1.8)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.1.0)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (13.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2025.4.26)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.6.0->vllm) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.1->vllm) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.20.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<1.27.0,>=1.26.0->vllm) (1.17.2)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.7.0)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.15.3)\n",
            "Requirement already satisfied: rich-toolkit>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.14.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->outlines==0.1.11->vllm) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (2025.4.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (0.24.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.1.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (0.8.3)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (13.9.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
            "\n",
            "--- Checking Package Versions ---\n",
            "Name: pyzmq\n",
            "Version: 26.4.0\n",
            "Summary: Python bindings for 0MQ\n",
            "Home-page: https://pyzmq.readthedocs.org\n",
            "Author: Brian E. Granger, Min Ragan-Kelley\n",
            "Author-email: PyZMQ Contributors <zeromq-dev@lists.zeromq.org>\n",
            "License: BSD 3-Clause License\n",
            "\n",
            " Copyright (c) 2009-2012, Brian Granger, Min Ragan-Kelley\n",
            "\n",
            " All rights reserved.\n",
            "\n",
            " Redistribution and use in source and binary forms, with or without\n",
            " modification, are permitted provided that the following conditions are met:\n",
            "\n",
            " 1. Redistributions of source code must retain the above copyright notice, this\n",
            "    list of conditions and the following disclaimer.\n",
            "\n",
            " 2. Redistributions in binary form must reproduce the above copyright notice,\n",
            "    this list of conditions and the following disclaimer in the documentation\n",
            "    and/or other materials provided with the distribution.\n",
            "\n",
            " 3. Neither the name of the copyright holder nor the names of its\n",
            "    contributors may be used to endorse or promote products derived from\n",
            "    this software without specific prior written permission.\n",
            "\n",
            " THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
            " AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
            " IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
            " DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
            " FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
            " DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
            " SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
            " CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
            " OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
            " OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
            "\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: \n",
            "Required-by: ipykernel, ipyparallel, jupyter-client, jupyter-server, notebook, vllm\n",
            "Name: vllm\n",
            "Version: 0.8.5.post1\n",
            "Summary: A high-throughput and memory-efficient inference and serving engine for LLMs\n",
            "Home-page: https://github.com/vllm-project/vllm\n",
            "Author: vLLM Team\n",
            "Author-email: \n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: aiohttp, blake3, cachetools, cloudpickle, compressed-tensors, depyf, einops, fastapi, filelock, gguf, huggingface-hub, importlib_metadata, lark, llguidance, lm-format-enforcer, mistral_common, msgspec, ninja, numba, numpy, openai, opencv-python-headless, opentelemetry-api, opentelemetry-exporter-otlp, opentelemetry-sdk, opentelemetry-semantic-conventions-ai, outlines, partial-json-parser, pillow, prometheus-fastapi-instrumentator, prometheus_client, protobuf, psutil, py-cpuinfo, pydantic, python-json-logger, pyyaml, pyzmq, ray, requests, scipy, sentencepiece, tiktoken, tokenizers, torch, torchaudio, torchvision, tqdm, transformers, typing_extensions, watchfiles, xformers, xgrammar\n",
            "Required-by: \n",
            "Name: huggingface-hub\n",
            "Version: 0.30.2\n",
            "Summary: Client library to download and publish models, datasets and other repos on the huggingface.co hub\n",
            "Home-page: https://github.com/huggingface/huggingface_hub\n",
            "Author: Hugging Face, Inc.\n",
            "Author-email: julien@huggingface.co\n",
            "License: Apache\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: filelock, fsspec, packaging, pyyaml, requests, tqdm, typing-extensions\n",
            "Required-by: accelerate, datasets, diffusers, peft, sentence-transformers, timm, tokenizers, transformers, vllm\n",
            "\n",
            "--- Installation complete ---\n"
          ]
        }
      ],
      "source": [
        "# --- Force Uninstall any potentially conflicting old versions ---\n",
        "# !pip uninstall -y vllm pyzmq zmq huggingface-hub # Can uncomment if needed\n",
        "\n",
        "# --- Install Dependencies ---\n",
        "print(\"\\n--- Installing Dependencies ---\")\n",
        "# !pip install --force-reinstall --no-cache-dir pyzmq # Ensure clean pyzmq\n",
        "!pip install -e . # Install LiveBench editable\n",
        "!pip install vllm huggingface-hub # Install vLLM and HF Hub\n",
        "\n",
        "# --- Verify Installation ---\n",
        "print(\"\\n--- Checking Package Versions ---\")\n",
        "!pip show pyzmq\n",
        "!pip show vllm\n",
        "!pip show huggingface-hub\n",
        "# !pip check # Optional: Check for broader dependency conflicts\n",
        "\n",
        "print(\"\\n--- Installation complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2ueHhahiF0T"
      },
      "outputs": [],
      "source": [
        "# Optional: Login to Hugging Face Hub\n",
        "# from huggingface_hub import login\n",
        "# login() # Paste your HF token when prompted"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Start Multi-Model Processing Loop ---\n",
        "The following cells will iterate through each model defined in `MODEL_CONFIGS`."
      ],
      "metadata": {
        "id": "_0WiWMOFS_Bn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5EukB_IhVuTS",
        "outputId": "a8029403-138c-405b-cb8c-3376155e55bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 05-04 05:07:34 [serving_completion.py:61] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.95}\n",
            "INFO 05-04 05:07:34 [api_server.py:1090] Starting vLLM API server on http://0.0.0.0:8000\n",
            "INFO 05-04 05:07:34 [launcher.py:28] Available routes are:\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /openapi.json, Methods: GET, HEAD\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /docs, Methods: GET, HEAD\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /redoc, Methods: GET, HEAD\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /health, Methods: GET\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /load, Methods: GET\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /ping, Methods: GET, POST\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /tokenize, Methods: POST\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /detokenize, Methods: POST\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /v1/models, Methods: GET\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /version, Methods: GET\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /v1/chat/completions, Methods: POST\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /v1/completions, Methods: POST\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /v1/embeddings, Methods: POST\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /pooling, Methods: POST\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /score, Methods: POST\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /v1/score, Methods: POST\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /rerank, Methods: POST\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /v1/rerank, Methods: POST\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /v2/rerank, Methods: POST\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /invocations, Methods: POST\n",
            "INFO 05-04 05:07:34 [launcher.py:36] Route: /metrics, Methods: GET\n",
            "INFO:     Started server process [28675]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     127.0.0.1:36988 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "\n",
            "Starting LiveBench evaluation\n",
            "Mode: single\n",
            "Models: /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-1.5b-local\n",
            "Benchmarks: live_bench\n",
            "Question source: huggingface\n",
            "\n",
            "Running single benchmark 'live_bench' for model: /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-1.5b-local\n",
            "Activating virtual environment: ../.venv/bin/activate\n",
            "Running: source ../.venv/bin/activate\n",
            "bash: line 1: ../.venv/bin/activate: No such file or directory\n",
            "Error running command: source ../.venv/bin/activate\n",
            "Exit code: 1\n",
            "\n",
            "Running benchmark command:\n",
            "export LIVEBENCH_API_KEY='dummy-key' && python -u gen_api_answer.py --model /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-1.5b-local --question-source huggingface --bench-name live_bench --api-base http://localhost:8000/v1 --model-display-name deepseek-1.5b-local --max-tokens 8192 --parallel 32 --resume --force-temperature 0.6 --livebench-release-option 2024-11-25 && python -u gen_ground_truth_judgment.py --question-source huggingface --model /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-1.5b-local --bench-name live_bench --model-display-name deepseek-1.5b-local --resume --livebench-release-option 2024-11-25\n",
            "Running: export LIVEBENCH_API_KEY='dummy-key' && python -u gen_api_answer.py --model /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-1.5b-local --question-source huggingface --bench-name live_bench --api-base http://localhost:8000/v1 --model-display-name deepseek-1.5b-local --max-tokens 8192 --parallel 32 --resume --force-temperature 0.6 --livebench-release-option 2024-11-25 && python -u gen_ground_truth_judgment.py --question-source huggingface --model /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-1.5b-local --bench-name live_bench --model-display-name deepseek-1.5b-local --resume --livebench-release-option 2024-11-25\n",
            "2025-05-04 05:07:47.596856: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-04 05:07:47.614286: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746335267.635962   29296 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746335267.642525   29296 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-04 05:07:47.664132: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "test-00000-of-00001.parquet: 100% 245M/245M [00:00<00:00, 173MB/s]\n",
            "Generating test split: 100% 128/128 [00:00<00:00, 144.51 examples/s]\n",
            "README.md: 100% 1.47k/1.47k [00:00<00:00, 9.68MB/s]\n",
            "test-00000-of-00001.parquet: 100% 145k/145k [00:00<00:00, 320MB/s]\n",
            "Generating test split: 100% 150/150 [00:00<00:00, 33013.88 examples/s]\n",
            "README.md: 100% 2.27k/2.27k [00:00<00:00, 20.3MB/s]\n",
            "test-00000-of-00001.parquet: 100% 537k/537k [00:00<00:00, 64.1MB/s]\n",
            "Generating test split: 100% 400/400 [00:00<00:00, 42433.14 examples/s]\n",
            "README.md: 100% 1.65k/1.65k [00:00<00:00, 15.8MB/s]\n",
            "test-00000-of-00001.parquet: 100% 186k/186k [00:00<00:00, 345MB/s]\n",
            "Generating test split: 100% 368/368 [00:00<00:00, 59226.58 examples/s]\n",
            "README.md: 100% 1.50k/1.50k [00:00<00:00, 14.2MB/s]\n",
            "test-00000-of-00001.parquet: 100% 88.2k/88.2k [00:00<00:00, 251MB/s]\n",
            "Generating test split: 100% 200/200 [00:00<00:00, 43453.03 examples/s]\n",
            "README.md: 100% 1.65k/1.65k [00:00<00:00, 15.3MB/s]\n",
            "test-00000-of-00001.parquet: 100% 288k/288k [00:00<00:00, 296MB/s]\n",
            "Generating test split: 100% 190/190 [00:00<00:00, 31263.94 examples/s]\n",
            "Filter: 100% 128/128 [00:00<00:00, 458.21 examples/s]\n",
            "Questions from live_bench/coding/coding_completion\n",
            "Output to data/live_bench/coding/coding_completion/model_answer/deepseek-1.5b-local.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Filter: 100% 128/128 [00:00<00:00, 562.47 examples/s]\n",
            "Questions from live_bench/coding/LCB_generation\n",
            "Output to data/live_bench/coding/LCB_generation/model_answer/deepseek-1.5b-local.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Filter: 100% 150/150 [00:00<00:00, 30305.66 examples/s]\n",
            "Questions from live_bench/data_analysis/tablejoin\n",
            "Output to data/live_bench/data_analysis/tablejoin/model_answer/deepseek-1.5b-local.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Filter: 100% 150/150 [00:00<00:00, 30712.50 examples/s]\n",
            "Questions from live_bench/data_analysis/cta\n",
            "Output to data/live_bench/data_analysis/cta/model_answer/deepseek-1.5b-local.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Filter: 100% 150/150 [00:00<00:00, 30723.00 examples/s]\n",
            "Questions from live_bench/data_analysis/tablereformat\n",
            "Output to data/live_bench/data_analysis/tablereformat/model_answer/deepseek-1.5b-local.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Filter: 100% 400/400 [00:00<00:00, 7779.62 examples/s]\n",
            "Questions from live_bench/instruction_following/simplify\n",
            "Output to data/live_bench/instruction_following/simplify/model_answer/deepseek-1.5b-local.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Filter: 100% 400/400 [00:00<00:00, 7921.52 examples/s]\n",
            "Questions from live_bench/instruction_following/paraphrase\n",
            "Output to data/live_bench/instruction_following/paraphrase/model_answer/deepseek-1.5b-local.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Filter: 100% 400/400 [00:00<00:00, 10375.71 examples/s]\n",
            "Questions from live_bench/instruction_following/summarize\n",
            "Output to data/live_bench/instruction_following/summarize/model_answer/deepseek-1.5b-local.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Filter: 100% 400/400 [00:00<00:00, 8975.57 examples/s]\n",
            "Questions from live_bench/instruction_following/story_generation\n",
            "Output to data/live_bench/instruction_following/story_generation/model_answer/deepseek-1.5b-local.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Filter: 100% 368/368 [00:00<00:00, 39800.52 examples/s]\n",
            "Questions from live_bench/math/math_comp\n",
            "Output to data/live_bench/math/math_comp/model_answer/deepseek-1.5b-local.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Filter: 100% 368/368 [00:00<00:00, 41596.03 examples/s]\n",
            "Questions from live_bench/math/AMPS_Hard\n",
            "Output to data/live_bench/math/AMPS_Hard/model_answer/deepseek-1.5b-local.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Filter: 100% 368/368 [00:00<00:00, 41587.06 examples/s]\n",
            "Questions from live_bench/math/olympiad\n",
            "Output to data/live_bench/math/olympiad/model_answer/deepseek-1.5b-local.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Filter: 100% 200/200 [00:00<00:00, 35224.05 examples/s]\n",
            "Questions from live_bench/reasoning/zebra_puzzle\n",
            "Output to data/live_bench/reasoning/zebra_puzzle/model_answer/deepseek-1.5b-local.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Filter: 100% 200/200 [00:00<00:00, 37006.39 examples/s]\n",
            "Questions from live_bench/reasoning/web_of_lies_v2\n",
            "Output to data/live_bench/reasoning/web_of_lies_v2/model_answer/deepseek-1.5b-local.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Filter: 100% 200/200 [00:00<00:00, 36016.52 examples/s]\n",
            "Questions from live_bench/reasoning/spatial\n",
            "Output to data/live_bench/reasoning/spatial/model_answer/deepseek-1.5b-local.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Filter: 100% 190/190 [00:00<00:00, 29595.49 examples/s]\n",
            "Questions from live_bench/language/connections\n",
            "Output to data/live_bench/language/connections/model_answer/deepseek-1.5b-local.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Filter: 100% 190/190 [00:00<00:00, 29320.01 examples/s]\n",
            "Questions from live_bench/language/typos\n",
            "Output to data/live_bench/language/typos/model_answer/deepseek-1.5b-local.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Filter: 100% 190/190 [00:00<00:00, 29201.82 examples/s]\n",
            "Questions from live_bench/language/plot_unscrambling\n",
            "Output to data/live_bench/language/plot_unscrambling/model_answer/deepseek-1.5b-local.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "2025-05-04 05:08:23.576928: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-04 05:08:23.593092: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746335303.613571   29295 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746335303.619814   29295 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-04 05:08:23.640525: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "models: ['deepseek-1.5b-local']\n",
            "Resume mode: Reading existing judgments from data/live_bench/coding/LCB_generation/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 546 existing answer IDs\n",
            "Resume mode: Filtered out 78 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-1.5b-local']\n",
            "Resume mode: Reading existing judgments from data/live_bench/coding/coding_completion/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-1.5b-local']\n",
            "Resume mode: Reading existing judgments from data/live_bench/data_analysis/cta/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-1.5b-local']\n",
            "Resume mode: Reading existing judgments from data/live_bench/data_analysis/tablejoin/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-1.5b-local']\n",
            "Resume mode: Reading existing judgments from data/live_bench/data_analysis/tablereformat/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-1.5b-local']\n",
            "Resume mode: Reading existing judgments from data/live_bench/instruction_following/paraphrase/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-1.5b-local']\n",
            "Resume mode: Reading existing judgments from data/live_bench/instruction_following/simplify/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-1.5b-local']\n",
            "Resume mode: Reading existing judgments from data/live_bench/instruction_following/summarize/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-1.5b-local']\n",
            "Resume mode: Reading existing judgments from data/live_bench/instruction_following/story_generation/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-1.5b-local']\n",
            "Resume mode: Reading existing judgments from data/live_bench/math/AMPS_Hard/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 700 existing answer IDs\n",
            "Resume mode: Filtered out 100 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-1.5b-local']\n",
            "Resume mode: Reading existing judgments from data/live_bench/math/olympiad/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 252 existing answer IDs\n",
            "Resume mode: Filtered out 36 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-1.5b-local']\n",
            "Resume mode: Reading existing judgments from data/live_bench/math/math_comp/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 672 existing answer IDs\n",
            "Resume mode: Filtered out 96 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-1.5b-local']\n",
            "Resume mode: Reading existing judgments from data/live_bench/reasoning/web_of_lies_v2/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-1.5b-local']\n",
            "Resume mode: Reading existing judgments from data/live_bench/reasoning/zebra_puzzle/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-1.5b-local']\n",
            "Resume mode: Reading existing judgments from data/live_bench/reasoning/spatial/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-1.5b-local']\n",
            "Resume mode: Reading existing judgments from data/live_bench/language/plot_unscrambling/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 280 existing answer IDs\n",
            "Resume mode: Filtered out 40 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-1.5b-local']\n",
            "Resume mode: Reading existing judgments from data/live_bench/language/typos/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-1.5b-local']\n",
            "Resume mode: Reading existing judgments from data/live_bench/language/connections/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "Benchmark completed successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Answer files not found.\n",
            "ERROR:root:Evaluation failed or produced no output for deepseek-1.5b-local. Aborting process for this model.\n",
            "ERROR:root:Processing failed for deepseek-1.5b-local. Check logs above.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 05-04 05:10:39 [serving_completion.py:61] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.95}\n",
            "INFO 05-04 05:10:39 [api_server.py:1090] Starting vLLM API server on http://0.0.0.0:8000\n",
            "INFO 05-04 05:10:39 [launcher.py:28] Available routes are:\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /openapi.json, Methods: HEAD, GET\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /docs, Methods: HEAD, GET\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /redoc, Methods: HEAD, GET\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /health, Methods: GET\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /load, Methods: GET\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /ping, Methods: POST, GET\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /tokenize, Methods: POST\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /detokenize, Methods: POST\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /v1/models, Methods: GET\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /version, Methods: GET\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /v1/chat/completions, Methods: POST\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /v1/completions, Methods: POST\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /v1/embeddings, Methods: POST\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /pooling, Methods: POST\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /score, Methods: POST\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /v1/score, Methods: POST\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /rerank, Methods: POST\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /v1/rerank, Methods: POST\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /v2/rerank, Methods: POST\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /invocations, Methods: POST\n",
            "INFO 05-04 05:10:39 [launcher.py:36] Route: /metrics, Methods: GET\n",
            "INFO:     Started server process [29803]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     127.0.0.1:59722 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "\n",
            "Starting LiveBench evaluation\n",
            "Mode: single\n",
            "Models: /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-r1-distill-qwen-1.5b-val-modified\n",
            "Benchmarks: live_bench\n",
            "Question source: huggingface\n",
            "\n",
            "Running single benchmark 'live_bench' for model: /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-r1-distill-qwen-1.5b-val-modified\n",
            "Activating virtual environment: ../.venv/bin/activate\n",
            "Running: source ../.venv/bin/activate\n",
            "bash: line 1: ../.venv/bin/activate: No such file or directory\n",
            "Error running command: source ../.venv/bin/activate\n",
            "Exit code: 1\n",
            "\n",
            "Running benchmark command:\n",
            "export LIVEBENCH_API_KEY='dummy-key' && python -u gen_api_answer.py --model /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-r1-distill-qwen-1.5b-val-modified --question-source huggingface --bench-name live_bench --api-base http://localhost:8000/v1 --model-display-name deepseek-r1-distill-qwen-1.5b-val-modified --max-tokens 8192 --parallel 32 --resume --force-temperature 0.6 --livebench-release-option 2024-11-25 && python -u gen_ground_truth_judgment.py --question-source huggingface --model /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-r1-distill-qwen-1.5b-val-modified --bench-name live_bench --model-display-name deepseek-r1-distill-qwen-1.5b-val-modified --resume --livebench-release-option 2024-11-25\n",
            "Running: export LIVEBENCH_API_KEY='dummy-key' && python -u gen_api_answer.py --model /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-r1-distill-qwen-1.5b-val-modified --question-source huggingface --bench-name live_bench --api-base http://localhost:8000/v1 --model-display-name deepseek-r1-distill-qwen-1.5b-val-modified --max-tokens 8192 --parallel 32 --resume --force-temperature 0.6 --livebench-release-option 2024-11-25 && python -u gen_ground_truth_judgment.py --question-source huggingface --model /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-r1-distill-qwen-1.5b-val-modified --bench-name live_bench --model-display-name deepseek-r1-distill-qwen-1.5b-val-modified --resume --livebench-release-option 2024-11-25\n",
            "2025-05-04 05:10:51.773365: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-04 05:10:51.791593: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746335451.813904   30420 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746335451.820697   30420 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-04 05:10:51.842455: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Questions from live_bench/coding/LCB_generation\n",
            "Output to data/live_bench/coding/LCB_generation/model_answer/deepseek-r1-distill-qwen-1.5b-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/coding/coding_completion\n",
            "Output to data/live_bench/coding/coding_completion/model_answer/deepseek-r1-distill-qwen-1.5b-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/data_analysis/tablejoin\n",
            "Output to data/live_bench/data_analysis/tablejoin/model_answer/deepseek-r1-distill-qwen-1.5b-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/data_analysis/tablereformat\n",
            "Output to data/live_bench/data_analysis/tablereformat/model_answer/deepseek-r1-distill-qwen-1.5b-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/data_analysis/cta\n",
            "Output to data/live_bench/data_analysis/cta/model_answer/deepseek-r1-distill-qwen-1.5b-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/instruction_following/story_generation\n",
            "Output to data/live_bench/instruction_following/story_generation/model_answer/deepseek-r1-distill-qwen-1.5b-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/instruction_following/summarize\n",
            "Output to data/live_bench/instruction_following/summarize/model_answer/deepseek-r1-distill-qwen-1.5b-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/instruction_following/simplify\n",
            "Output to data/live_bench/instruction_following/simplify/model_answer/deepseek-r1-distill-qwen-1.5b-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/instruction_following/paraphrase\n",
            "Output to data/live_bench/instruction_following/paraphrase/model_answer/deepseek-r1-distill-qwen-1.5b-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/math/AMPS_Hard\n",
            "Output to data/live_bench/math/AMPS_Hard/model_answer/deepseek-r1-distill-qwen-1.5b-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/math/math_comp\n",
            "Output to data/live_bench/math/math_comp/model_answer/deepseek-r1-distill-qwen-1.5b-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/math/olympiad\n",
            "Output to data/live_bench/math/olympiad/model_answer/deepseek-r1-distill-qwen-1.5b-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/reasoning/zebra_puzzle\n",
            "Output to data/live_bench/reasoning/zebra_puzzle/model_answer/deepseek-r1-distill-qwen-1.5b-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/reasoning/spatial\n",
            "Output to data/live_bench/reasoning/spatial/model_answer/deepseek-r1-distill-qwen-1.5b-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/reasoning/web_of_lies_v2\n",
            "Output to data/live_bench/reasoning/web_of_lies_v2/model_answer/deepseek-r1-distill-qwen-1.5b-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/language/typos\n",
            "Output to data/live_bench/language/typos/model_answer/deepseek-r1-distill-qwen-1.5b-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/language/plot_unscrambling\n",
            "Output to data/live_bench/language/plot_unscrambling/model_answer/deepseek-r1-distill-qwen-1.5b-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/language/connections\n",
            "Output to data/live_bench/language/connections/model_answer/deepseek-r1-distill-qwen-1.5b-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "2025-05-04 05:11:19.526255: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-04 05:11:19.542990: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746335479.564439   30419 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746335479.570907   30419 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-04 05:11:19.591377: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/coding/coding_completion/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/coding/LCB_generation/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 546 existing answer IDs\n",
            "Resume mode: Filtered out 78 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/data_analysis/tablereformat/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/data_analysis/tablejoin/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/data_analysis/cta/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/instruction_following/story_generation/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/instruction_following/paraphrase/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/instruction_following/summarize/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/instruction_following/simplify/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/math/AMPS_Hard/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 700 existing answer IDs\n",
            "Resume mode: Filtered out 100 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/math/math_comp/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 672 existing answer IDs\n",
            "Resume mode: Filtered out 96 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/math/olympiad/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 252 existing answer IDs\n",
            "Resume mode: Filtered out 36 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/reasoning/zebra_puzzle/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/reasoning/web_of_lies_v2/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/reasoning/spatial/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/language/connections/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/language/plot_unscrambling/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 280 existing answer IDs\n",
            "Resume mode: Filtered out 40 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/language/typos/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "Benchmark completed successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Answer files not found.\n",
            "ERROR:root:Evaluation failed or produced no output for deepseek-r1-distill-qwen-1.5b-val-modified. Aborting process for this model.\n",
            "ERROR:root:Processing failed for deepseek-r1-distill-qwen-1.5b-val-modified. Check logs above.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 05-04 05:13:18 [serving_completion.py:61] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.95}\n",
            "INFO 05-04 05:13:18 [api_server.py:1090] Starting vLLM API server on http://0.0.0.0:8000\n",
            "INFO 05-04 05:13:18 [launcher.py:28] Available routes are:\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /openapi.json, Methods: HEAD, GET\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /docs, Methods: HEAD, GET\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /redoc, Methods: HEAD, GET\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /health, Methods: GET\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /load, Methods: GET\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /ping, Methods: GET, POST\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /tokenize, Methods: POST\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /detokenize, Methods: POST\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /v1/models, Methods: GET\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /version, Methods: GET\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /v1/chat/completions, Methods: POST\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /v1/completions, Methods: POST\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /v1/embeddings, Methods: POST\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /pooling, Methods: POST\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /score, Methods: POST\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /v1/score, Methods: POST\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /rerank, Methods: POST\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /v1/rerank, Methods: POST\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /v2/rerank, Methods: POST\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /invocations, Methods: POST\n",
            "INFO 05-04 05:13:18 [launcher.py:36] Route: /metrics, Methods: GET\n",
            "INFO:     Started server process [30800]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     127.0.0.1:54906 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "\n",
            "Starting LiveBench evaluation\n",
            "Mode: single\n",
            "Models: /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-r1-distill-qwen-1.5b-scrambled\n",
            "Benchmarks: live_bench\n",
            "Question source: huggingface\n",
            "\n",
            "Running single benchmark 'live_bench' for model: /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-r1-distill-qwen-1.5b-scrambled\n",
            "Activating virtual environment: ../.venv/bin/activate\n",
            "Running: source ../.venv/bin/activate\n",
            "bash: line 1: ../.venv/bin/activate: No such file or directory\n",
            "Error running command: source ../.venv/bin/activate\n",
            "Exit code: 1\n",
            "\n",
            "Running benchmark command:\n",
            "export LIVEBENCH_API_KEY='dummy-key' && python -u gen_api_answer.py --model /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-r1-distill-qwen-1.5b-scrambled --question-source huggingface --bench-name live_bench --api-base http://localhost:8000/v1 --model-display-name deepseek-r1-distill-qwen-1.5b-scrambled --max-tokens 8192 --parallel 32 --resume --force-temperature 0.6 --livebench-release-option 2024-11-25 && python -u gen_ground_truth_judgment.py --question-source huggingface --model /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-r1-distill-qwen-1.5b-scrambled --bench-name live_bench --model-display-name deepseek-r1-distill-qwen-1.5b-scrambled --resume --livebench-release-option 2024-11-25\n",
            "Running: export LIVEBENCH_API_KEY='dummy-key' && python -u gen_api_answer.py --model /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-r1-distill-qwen-1.5b-scrambled --question-source huggingface --bench-name live_bench --api-base http://localhost:8000/v1 --model-display-name deepseek-r1-distill-qwen-1.5b-scrambled --max-tokens 8192 --parallel 32 --resume --force-temperature 0.6 --livebench-release-option 2024-11-25 && python -u gen_ground_truth_judgment.py --question-source huggingface --model /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-r1-distill-qwen-1.5b-scrambled --bench-name live_bench --model-display-name deepseek-r1-distill-qwen-1.5b-scrambled --resume --livebench-release-option 2024-11-25\n",
            "2025-05-04 05:13:29.973922: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-04 05:13:29.991524: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746335610.012907   31416 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746335610.019509   31416 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-04 05:13:30.041630: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Questions from live_bench/coding/LCB_generation\n",
            "Output to data/live_bench/coding/LCB_generation/model_answer/deepseek-r1-distill-qwen-1.5b-scrambled.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/coding/coding_completion\n",
            "Output to data/live_bench/coding/coding_completion/model_answer/deepseek-r1-distill-qwen-1.5b-scrambled.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/data_analysis/tablejoin\n",
            "Output to data/live_bench/data_analysis/tablejoin/model_answer/deepseek-r1-distill-qwen-1.5b-scrambled.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/data_analysis/tablereformat\n",
            "Output to data/live_bench/data_analysis/tablereformat/model_answer/deepseek-r1-distill-qwen-1.5b-scrambled.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/data_analysis/cta\n",
            "Output to data/live_bench/data_analysis/cta/model_answer/deepseek-r1-distill-qwen-1.5b-scrambled.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/instruction_following/paraphrase\n",
            "Output to data/live_bench/instruction_following/paraphrase/model_answer/deepseek-r1-distill-qwen-1.5b-scrambled.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/instruction_following/simplify\n",
            "Output to data/live_bench/instruction_following/simplify/model_answer/deepseek-r1-distill-qwen-1.5b-scrambled.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/instruction_following/story_generation\n",
            "Output to data/live_bench/instruction_following/story_generation/model_answer/deepseek-r1-distill-qwen-1.5b-scrambled.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/instruction_following/summarize\n",
            "Output to data/live_bench/instruction_following/summarize/model_answer/deepseek-r1-distill-qwen-1.5b-scrambled.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/math/math_comp\n",
            "Output to data/live_bench/math/math_comp/model_answer/deepseek-r1-distill-qwen-1.5b-scrambled.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/math/olympiad\n",
            "Output to data/live_bench/math/olympiad/model_answer/deepseek-r1-distill-qwen-1.5b-scrambled.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/math/AMPS_Hard\n",
            "Output to data/live_bench/math/AMPS_Hard/model_answer/deepseek-r1-distill-qwen-1.5b-scrambled.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/reasoning/spatial\n",
            "Output to data/live_bench/reasoning/spatial/model_answer/deepseek-r1-distill-qwen-1.5b-scrambled.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/reasoning/web_of_lies_v2\n",
            "Output to data/live_bench/reasoning/web_of_lies_v2/model_answer/deepseek-r1-distill-qwen-1.5b-scrambled.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/reasoning/zebra_puzzle\n",
            "Output to data/live_bench/reasoning/zebra_puzzle/model_answer/deepseek-r1-distill-qwen-1.5b-scrambled.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/language/plot_unscrambling\n",
            "Output to data/live_bench/language/plot_unscrambling/model_answer/deepseek-r1-distill-qwen-1.5b-scrambled.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/language/typos\n",
            "Output to data/live_bench/language/typos/model_answer/deepseek-r1-distill-qwen-1.5b-scrambled.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/language/connections\n",
            "Output to data/live_bench/language/connections/model_answer/deepseek-r1-distill-qwen-1.5b-scrambled.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "2025-05-04 05:13:55.472969: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-04 05:13:55.490213: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746335635.511329   31415 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746335635.518074   31415 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-04 05:13:55.539732: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-scrambled']\n",
            "Resume mode: Reading existing judgments from data/live_bench/coding/coding_completion/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-scrambled']\n",
            "Resume mode: Reading existing judgments from data/live_bench/coding/LCB_generation/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 546 existing answer IDs\n",
            "Resume mode: Filtered out 78 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-scrambled']\n",
            "Resume mode: Reading existing judgments from data/live_bench/data_analysis/tablereformat/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-scrambled']\n",
            "Resume mode: Reading existing judgments from data/live_bench/data_analysis/tablejoin/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-scrambled']\n",
            "Resume mode: Reading existing judgments from data/live_bench/data_analysis/cta/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-scrambled']\n",
            "Resume mode: Reading existing judgments from data/live_bench/instruction_following/simplify/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-scrambled']\n",
            "Resume mode: Reading existing judgments from data/live_bench/instruction_following/summarize/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-scrambled']\n",
            "Resume mode: Reading existing judgments from data/live_bench/instruction_following/paraphrase/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-scrambled']\n",
            "Resume mode: Reading existing judgments from data/live_bench/instruction_following/story_generation/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-scrambled']\n",
            "Resume mode: Reading existing judgments from data/live_bench/math/olympiad/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 252 existing answer IDs\n",
            "Resume mode: Filtered out 36 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-scrambled']\n",
            "Resume mode: Reading existing judgments from data/live_bench/math/math_comp/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 672 existing answer IDs\n",
            "Resume mode: Filtered out 96 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-scrambled']\n",
            "Resume mode: Reading existing judgments from data/live_bench/math/AMPS_Hard/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 700 existing answer IDs\n",
            "Resume mode: Filtered out 100 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-scrambled']\n",
            "Resume mode: Reading existing judgments from data/live_bench/reasoning/web_of_lies_v2/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-scrambled']\n",
            "Resume mode: Reading existing judgments from data/live_bench/reasoning/spatial/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-scrambled']\n",
            "Resume mode: Reading existing judgments from data/live_bench/reasoning/zebra_puzzle/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-scrambled']\n",
            "Resume mode: Reading existing judgments from data/live_bench/language/plot_unscrambling/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 280 existing answer IDs\n",
            "Resume mode: Filtered out 40 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-scrambled']\n",
            "Resume mode: Reading existing judgments from data/live_bench/language/typos/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-scrambled']\n",
            "Resume mode: Reading existing judgments from data/live_bench/language/connections/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "Benchmark completed successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Answer files not found.\n",
            "ERROR:root:Evaluation failed or produced no output for deepseek-r1-distill-qwen-1.5b-scrambled. Aborting process for this model.\n",
            "ERROR:root:Processing failed for deepseek-r1-distill-qwen-1.5b-scrambled. Check logs above.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 05-04 05:18:26 [serving_completion.py:61] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.95}\n",
            "INFO 05-04 05:18:26 [api_server.py:1090] Starting vLLM API server on http://0.0.0.0:8000\n",
            "INFO 05-04 05:18:26 [launcher.py:28] Available routes are:\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /openapi.json, Methods: HEAD, GET\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /docs, Methods: HEAD, GET\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /redoc, Methods: HEAD, GET\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /health, Methods: GET\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /load, Methods: GET\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /ping, Methods: POST, GET\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /tokenize, Methods: POST\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /detokenize, Methods: POST\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /v1/models, Methods: GET\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /version, Methods: GET\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /v1/chat/completions, Methods: POST\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /v1/completions, Methods: POST\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /v1/embeddings, Methods: POST\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /pooling, Methods: POST\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /score, Methods: POST\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /v1/score, Methods: POST\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /rerank, Methods: POST\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /v1/rerank, Methods: POST\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /v2/rerank, Methods: POST\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /invocations, Methods: POST\n",
            "INFO 05-04 05:18:26 [launcher.py:36] Route: /metrics, Methods: GET\n",
            "INFO:     Started server process [31788]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     127.0.0.1:47844 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "\n",
            "Starting LiveBench evaluation\n",
            "Mode: single\n",
            "Models: /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-r1-distill-qwen-1.5b-length-val-modified\n",
            "Benchmarks: live_bench\n",
            "Question source: huggingface\n",
            "\n",
            "Running single benchmark 'live_bench' for model: /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-r1-distill-qwen-1.5b-length-val-modified\n",
            "Activating virtual environment: ../.venv/bin/activate\n",
            "Running: source ../.venv/bin/activate\n",
            "bash: line 1: ../.venv/bin/activate: No such file or directory\n",
            "Error running command: source ../.venv/bin/activate\n",
            "Exit code: 1\n",
            "\n",
            "Running benchmark command:\n",
            "export LIVEBENCH_API_KEY='dummy-key' && python -u gen_api_answer.py --model /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-r1-distill-qwen-1.5b-length-val-modified --question-source huggingface --bench-name live_bench --api-base http://localhost:8000/v1 --model-display-name deepseek-r1-distill-qwen-1.5b-length-val-modified --max-tokens 8192 --parallel 32 --resume --force-temperature 0.6 --livebench-release-option 2024-11-25 && python -u gen_ground_truth_judgment.py --question-source huggingface --model /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-r1-distill-qwen-1.5b-length-val-modified --bench-name live_bench --model-display-name deepseek-r1-distill-qwen-1.5b-length-val-modified --resume --livebench-release-option 2024-11-25\n",
            "Running: export LIVEBENCH_API_KEY='dummy-key' && python -u gen_api_answer.py --model /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-r1-distill-qwen-1.5b-length-val-modified --question-source huggingface --bench-name live_bench --api-base http://localhost:8000/v1 --model-display-name deepseek-r1-distill-qwen-1.5b-length-val-modified --max-tokens 8192 --parallel 32 --resume --force-temperature 0.6 --livebench-release-option 2024-11-25 && python -u gen_ground_truth_judgment.py --question-source huggingface --model /content/drive/MyDrive/Colab_Notebooks/LiveBenchRun/models/deepseek-r1-distill-qwen-1.5b-length-val-modified --bench-name live_bench --model-display-name deepseek-r1-distill-qwen-1.5b-length-val-modified --resume --livebench-release-option 2024-11-25\n",
            "2025-05-04 05:18:36.484354: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-04 05:18:36.502426: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746335916.524129   33191 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746335916.530730   33191 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-04 05:18:36.553124: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Questions from live_bench/coding/LCB_generation\n",
            "Output to data/live_bench/coding/LCB_generation/model_answer/deepseek-r1-distill-qwen-1.5b-length-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/coding/coding_completion\n",
            "Output to data/live_bench/coding/coding_completion/model_answer/deepseek-r1-distill-qwen-1.5b-length-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/data_analysis/tablereformat\n",
            "Output to data/live_bench/data_analysis/tablereformat/model_answer/deepseek-r1-distill-qwen-1.5b-length-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/data_analysis/tablejoin\n",
            "Output to data/live_bench/data_analysis/tablejoin/model_answer/deepseek-r1-distill-qwen-1.5b-length-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/data_analysis/cta\n",
            "Output to data/live_bench/data_analysis/cta/model_answer/deepseek-r1-distill-qwen-1.5b-length-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/instruction_following/summarize\n",
            "Output to data/live_bench/instruction_following/summarize/model_answer/deepseek-r1-distill-qwen-1.5b-length-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/instruction_following/story_generation\n",
            "Output to data/live_bench/instruction_following/story_generation/model_answer/deepseek-r1-distill-qwen-1.5b-length-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/instruction_following/simplify\n",
            "Output to data/live_bench/instruction_following/simplify/model_answer/deepseek-r1-distill-qwen-1.5b-length-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/instruction_following/paraphrase\n",
            "Output to data/live_bench/instruction_following/paraphrase/model_answer/deepseek-r1-distill-qwen-1.5b-length-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/math/math_comp\n",
            "Output to data/live_bench/math/math_comp/model_answer/deepseek-r1-distill-qwen-1.5b-length-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/math/olympiad\n",
            "Output to data/live_bench/math/olympiad/model_answer/deepseek-r1-distill-qwen-1.5b-length-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/math/AMPS_Hard\n",
            "Output to data/live_bench/math/AMPS_Hard/model_answer/deepseek-r1-distill-qwen-1.5b-length-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/reasoning/spatial\n",
            "Output to data/live_bench/reasoning/spatial/model_answer/deepseek-r1-distill-qwen-1.5b-length-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/reasoning/web_of_lies_v2\n",
            "Output to data/live_bench/reasoning/web_of_lies_v2/model_answer/deepseek-r1-distill-qwen-1.5b-length-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/reasoning/zebra_puzzle\n",
            "Output to data/live_bench/reasoning/zebra_puzzle/model_answer/deepseek-r1-distill-qwen-1.5b-length-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/language/plot_unscrambling\n",
            "Output to data/live_bench/language/plot_unscrambling/model_answer/deepseek-r1-distill-qwen-1.5b-length-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/language/typos\n",
            "Output to data/live_bench/language/typos/model_answer/deepseek-r1-distill-qwen-1.5b-length-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "Questions from live_bench/language/connections\n",
            "Output to data/live_bench/language/connections/model_answer/deepseek-r1-distill-qwen-1.5b-length-val-modified.jsonl\n",
            "0it [00:00, ?it/s]\n",
            "2025-05-04 05:19:04.792513: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-04 05:19:04.809625: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746335944.830760   33190 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746335944.837226   33190 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-04 05:19:04.858352: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-length-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/coding/LCB_generation/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 546 existing answer IDs\n",
            "Resume mode: Filtered out 78 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-length-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/coding/coding_completion/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-length-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/data_analysis/tablejoin/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-length-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/data_analysis/tablereformat/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-length-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/data_analysis/cta/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-length-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/instruction_following/simplify/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-length-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/instruction_following/story_generation/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-length-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/instruction_following/summarize/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-length-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/instruction_following/paraphrase/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-length-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/math/math_comp/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 672 existing answer IDs\n",
            "Resume mode: Filtered out 96 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-length-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/math/AMPS_Hard/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 700 existing answer IDs\n",
            "Resume mode: Filtered out 100 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-length-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/math/olympiad/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 252 existing answer IDs\n",
            "Resume mode: Filtered out 36 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-length-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/reasoning/spatial/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-length-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/reasoning/zebra_puzzle/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-length-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/reasoning/web_of_lies_v2/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-length-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/language/typos/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-length-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/language/connections/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 350 existing answer IDs\n",
            "Resume mode: Filtered out 50 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "models: ['deepseek-r1-distill-qwen-1.5b-length-val-modified']\n",
            "Resume mode: Reading existing judgments from data/live_bench/language/plot_unscrambling/model_judgment/ground_truth_judgment.jsonl\n",
            "Found 280 existing answer IDs\n",
            "Resume mode: Filtered out 40 already judged matches\n",
            "No question-answer pairs found to be judged\n",
            "Benchmark completed successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Answer files not found.\n",
            "ERROR:root:Evaluation failed or produced no output for deepseek-r1-distill-qwen-1.5b-length-val-modified. Aborting process for this model.\n",
            "ERROR:root:Processing failed for deepseek-r1-distill-qwen-1.5b-length-val-modified. Check logs above.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-75100844c979>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLiveBenchModelRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Successfully completed processing for {config['local_name']}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-75100844c979>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Download failed for {self.local_name}. Aborting process for this model.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_vllm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"vLLM start failed for {self.local_name}. Aborting process for this model.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-75100844c979>\u001b[0m in \u001b[0;36mstart_vllm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected error checking server status: {e}. Waiting...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"--- Last lines of vLLM Server Log ({self.vllm_log_file}) after wait period --- \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import glob\n",
        "import json\n",
        "import shutil\n",
        "import logging\n",
        "import subprocess\n",
        "import requests # Added for checking server status\n",
        "from huggingface_hub import snapshot_download\n",
        "try:\n",
        "    from IPython import get_ipython\n",
        "except ImportError:\n",
        "    get_ipython = None\n",
        "\n",
        "class LiveBenchModelRunner:\n",
        "    \"\"\"Encapsulates the logic to process one model for LiveBench.\"\"\"\n",
        "\n",
        "    def __init__(self, model_config, global_paths, global_params):\n",
        "        self.hf_id = model_config['hf_id']\n",
        "        self.local_name = model_config['local_name']\n",
        "        self.paths = global_paths\n",
        "        self.params = global_params\n",
        "\n",
        "        # Derive model-specific paths\n",
        "        self.weights_path = os.path.join(self.paths['models_parent'], self.local_name)\n",
        "        self.vllm_log_file = os.path.join(self.paths['gdrive_base'], f\"vllm_server_{self.local_name}.log\")\n",
        "        self.base_data_path = os.path.join(self.paths['livebench_subdir'], \"data\")\n",
        "\n",
        "        os.makedirs(self.weights_path, exist_ok=True)\n",
        "        logging.info(f\"Initialized runner for {self.local_name}\")\n",
        "        logging.info(f\"  Weights Path: {self.weights_path}\")\n",
        "        logging.info(f\"  vLLM Log: {self.vllm_log_file}\")\n",
        "\n",
        "    def _run_command(self, command_str, cwd=None, check=False):\n",
        "        \"\"\"Helper to run shell commands, preferring IPython.system if available.\"\"\"\n",
        "        logging.info(f\"Running command: {command_str}\")\n",
        "        if cwd:\n",
        "            logging.info(f\"  In directory: {cwd}\")\n",
        "\n",
        "        ipython = get_ipython()\n",
        "        if ipython:\n",
        "            original_dir = os.getcwd()\n",
        "            try:\n",
        "                if cwd:\n",
        "                    os.chdir(cwd)\n",
        "                ipython.system(command_str)\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                 logging.error(f\"IPython.system command failed: {e}\")\n",
        "                 return False\n",
        "            finally:\n",
        "                 if cwd:\n",
        "                     os.chdir(original_dir)\n",
        "        else:\n",
        "            logging.warning(\"IPython environment not detected, falling back to subprocess.run\")\n",
        "            try:\n",
        "                process = subprocess.run(command_str, cwd=cwd, shell=True, check=check, capture_output=True, text=True)\n",
        "                logging.info(f\"  stdout: {process.stdout}\")\n",
        "                if process.stderr:\n",
        "                    logging.warning(f\"  stderr: {process.stderr}\")\n",
        "                return process.returncode == 0\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                logging.error(f\"Command failed: {e}\")\n",
        "                logging.error(f\"  stdout: {e.stdout}\")\n",
        "                logging.error(f\"  stderr: {e.stderr}\")\n",
        "                return False\n",
        "            except Exception as e:\n",
        "                logging.error(f\"An unexpected error occurred running command: {e}\")\n",
        "                return False\n",
        "\n",
        "    def download(self):\n",
        "        \"\"\"Downloads model weights if they don't exist.\"\"\"\n",
        "        logging.info(f\"Checking model weights directory: {self.weights_path}\")\n",
        "        if not os.path.exists(self.weights_path) or not os.listdir(self.weights_path):\n",
        "            logging.info(f\"Model weights directory is empty or does not exist. Downloading {self.hf_id}...\")\n",
        "            try:\n",
        "                snapshot_download(\n",
        "                    repo_id=self.hf_id,\n",
        "                    local_dir=self.weights_path,\n",
        "                    local_dir_use_symlinks=False,\n",
        "                )\n",
        "                logging.info(\"Model download complete.\")\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error during model download for {self.local_name}: {e}\")\n",
        "                return False\n",
        "        else:\n",
        "            logging.info(\"Model weights directory already contains files. Skipping download.\")\n",
        "            return True\n",
        "\n",
        "    def start_vllm(self):\n",
        "        \"\"\"Starts the vLLM server using IPython.system(nohup...) and waits.\"\"\"\n",
        "        logging.info(f\"Starting vLLM server for model: {self.local_name} from path: {self.weights_path}\")\n",
        "        logging.info(f\"Saving server log to: {self.vllm_log_file}\")\n",
        "\n",
        "        ipython = get_ipython()\n",
        "        if not ipython:\n",
        "            logging.error(\"IPython environment not detected. Cannot reliably start background server.\")\n",
        "            return False\n",
        "\n",
        "        os.makedirs(os.path.dirname(self.vllm_log_file), exist_ok=True)\n",
        "        if os.path.exists(self.vllm_log_file):\n",
        "            os.remove(self.vllm_log_file)\n",
        "\n",
        "        vllm_command_str = (\n",
        "            f\"nohup python -m vllm.entrypoints.openai.api_server \"\n",
        "            f\"--model {self.weights_path} \"\n",
        "            f\"--served-model-name {self.local_name} \"\n",
        "            f\"--host 0.0.0.0 \"\n",
        "            f\"--port {self.params['vllm_port']} \"\n",
        "            f\"--tensor-parallel-size 1 \"\n",
        "            f\"--gpu-memory-utilization {self.params['gpu_utilization']} \"\n",
        "            f\"--trust-remote-code \"\n",
        "            f\"> {self.vllm_log_file} 2>&1 &\"\n",
        "        )\n",
        "\n",
        "        logging.info(f\"Executing server launch command via IPython.system: {vllm_command_str}\")\n",
        "        original_dir = os.getcwd()\n",
        "        try:\n",
        "            os.chdir(self.paths['gdrive_base'])\n",
        "            ipython.system(vllm_command_str)\n",
        "            logging.info(f\"vLLM server launch command submitted via IPython.system.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to launch vLLM via IPython.system: {e}\")\n",
        "            os.chdir(original_dir)\n",
        "            return False\n",
        "        finally:\n",
        "            os.chdir(original_dir)\n",
        "\n",
        "        max_wait_time = 500\n",
        "        start_time = time.time()\n",
        "        server_ready = False\n",
        "        check_url = f\"http://localhost:{self.params['vllm_port']}/v1/models\"\n",
        "        expected_model_id = self.local_name\n",
        "        check_interval = 10\n",
        "\n",
        "        logging.info(f\"Waiting up to {max_wait_time}s for server readiness...\")\n",
        "        while time.time() - start_time < max_wait_time:\n",
        "            if os.path.exists(self.vllm_log_file):\n",
        "                try:\n",
        "                    with open(self.vllm_log_file, 'r') as f_log_check:\n",
        "                        log_content = f_log_check.read()\n",
        "                    if \"Traceback\" in log_content or \"CUDA out of memory\" in log_content:\n",
        "                         logging.error(\"Detected potential fatal error in vLLM log during startup wait.\")\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "            try:\n",
        "                response = requests.get(check_url, timeout=10)\n",
        "                if response.status_code == 200:\n",
        "                    try:\n",
        "                        response_data = response.json()\n",
        "                        loaded_models = [m.get('id') for m in response_data.get('data', [])]\n",
        "                        if expected_model_id in loaded_models:\n",
        "                            logging.info(f\"vLLM server is ready and model '{expected_model_id}' is served.\")\n",
        "                            server_ready = True\n",
        "                            break\n",
        "                        else:\n",
        "                            uvicorn_running = False\n",
        "                            if os.path.exists(self.vllm_log_file):\n",
        "                                try:\n",
        "                                    with open(self.vllm_log_file, 'r') as f_log_check:\n",
        "                                        if \"Uvicorn running\" in f_log_check.read(): uvicorn_running = True\n",
        "                                except Exception: pass\n",
        "                            if uvicorn_running:\n",
        "                                logging.warning(f\"vLLM server API is up, but model '{expected_model_id}' not confirmed in response yet: {loaded_models}. Retrying...\")\n",
        "                            else:\n",
        "                                logging.info(f\"vLLM server API reachable but Uvicorn not confirmed running yet. Waiting...\")\n",
        "                    except json.JSONDecodeError:\n",
        "                        logging.warning(f\"Server responded to {check_url} with status 200, but response was not valid JSON. Waiting...\")\n",
        "                else:\n",
        "                     logging.warning(f\"Server not ready yet (API status code {response.status_code}). Waiting...\")\n",
        "            except requests.exceptions.RequestException as req_e:\n",
        "                logging.info(f\"Server not reachable yet ({req_e}). Waiting...\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Unexpected error checking server status: {e}. Waiting...\")\n",
        "\n",
        "            time.sleep(check_interval)\n",
        "\n",
        "        logging.info(f\"--- Last lines of vLLM Server Log ({self.vllm_log_file}) after wait period --- \")\n",
        "        self._run_command(f\"tail -n 30 {self.vllm_log_file}\")\n",
        "\n",
        "        if not server_ready:\n",
        "            logging.error(f\"vLLM server did not become ready with model '{expected_model_id}' within {max_wait_time} seconds.\")\n",
        "            stop_vllm_server(self.params['vllm_port'])\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"Runs the LiveBench evaluation script.\"\"\"\n",
        "        logging.info(f\"Running LiveBench evaluation for model: {self.local_name}\")\n",
        "        eval_command_list = [\n",
        "            \"python\", \"run_livebench.py\",\n",
        "            \"--model\", self.weights_path,\n",
        "            \"--model-display-name\", self.local_name,\n",
        "            \"--api-base\", self.params['api_base_url'],\n",
        "            \"--api-key\", self.params['api_key'],\n",
        "            \"--bench-name\", self.params['bench_name'],\n",
        "            \"--livebench-release-option\", self.params['livebench_release'],\n",
        "            \"--max-tokens\", str(self.params['max_tokens']),\n",
        "            \"--parallel-requests\", str(self.params['parallel_requests']),\n",
        "            \"--force-temperature\", \"0.6\",\n",
        "            \"--resume\",\n",
        "        ]\n",
        "        eval_command_str = ' '.join(eval_command_list)\n",
        "        success = self._run_command(eval_command_str, cwd=self.paths['livebench_subdir'], check=False)\n",
        "        logging.info(f\"LiveBench evaluation run finished for {self.local_name}.\")\n",
        "\n",
        "        logging.info(\"--- Checking for output files ---\")\n",
        "        bench_subpath_glob = self.params['bench_name'].replace('live_bench/', 'live_bench/*/') if self.params['bench_name'].startswith('live_bench/') else self.params['bench_name']\n",
        "        answer_path_pattern = os.path.join(self.base_data_path, f\"{bench_subpath_glob}/model_answer/{self.local_name}.jsonl\")\n",
        "        logging.info(f\"Checking for answer files like: {answer_path_pattern}\")\n",
        "        found_files = glob.glob(answer_path_pattern.replace('/*/', '/**/'), recursive=True)\n",
        "        if found_files:\n",
        "             logging.info(f\"Answer files found: {found_files}\")\n",
        "             return True\n",
        "        else:\n",
        "             logging.warning(\"Answer files not found.\")\n",
        "             return False\n",
        "\n",
        "    def clean(self):\n",
        "        \"\"\"Cleans the <think> tags from answer files.\"\"\"\n",
        "        logging.info(f\"--- Starting Answer File Cleaning for {self.local_name} ---\")\n",
        "        time.sleep(5)\n",
        "\n",
        "        pattern_glob = None\n",
        "        bench_name = self.params['bench_name']\n",
        "        if bench_name == \"live_bench\":\n",
        "            pattern_glob = os.path.join(self.base_data_path, f\"live_bench/*/*/model_answer/{self.local_name}.jsonl\")\n",
        "        elif bench_name.startswith(\"live_bench/\"):\n",
        "            category = bench_name.split('/')[1]\n",
        "            pattern_glob = os.path.join(self.base_data_path, f\"live_bench/{category}/*/model_answer/{self.local_name}.jsonl\")\n",
        "        else:\n",
        "            pattern_glob = os.path.join(self.base_data_path, f\"{bench_name}/model_answer/{self.local_name}.jsonl\")\n",
        "\n",
        "        logging.info(f\"Searching for files with pattern: {pattern_glob}\")\n",
        "        use_recursive_glob = (\"*\" in pattern_glob)\n",
        "        answer_files = glob.glob(pattern_glob, recursive=use_recursive_glob)\n",
        "\n",
        "        if not answer_files:\n",
        "            logging.warning(f\"No answer files found for {self.local_name} matching pattern. Skipping cleaning.\")\n",
        "            return True\n",
        "\n",
        "        logging.info(f\"Found {len(answer_files)} answer file(s). Proceeding with cleaning.\")\n",
        "        all_cleaned = True\n",
        "        for original_filepath in answer_files:\n",
        "            if not os.path.exists(original_filepath):\n",
        "                logging.warning(f\"Skipping non-existent file listed by glob: {original_filepath}\")\n",
        "                continue\n",
        "\n",
        "            logging.info(f\"Processing: {original_filepath}\")\n",
        "            backup_filepath = original_filepath + \".bak\"\n",
        "            cleaned_filepath = original_filepath + \".cleaned_temp\"\n",
        "\n",
        "            if os.path.exists(backup_filepath):\n",
        "                logging.info(f\"Backup file already exists ({backup_filepath}). Assuming already cleaned or skipping.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                shutil.copy2(original_filepath, backup_filepath)\n",
        "                lines_processed = 0\n",
        "                lines_cleaned = 0\n",
        "                with open(backup_filepath, 'r', encoding='utf-8') as f_in, \\\n",
        "                     open(cleaned_filepath, 'w', encoding='utf-8') as f_out:\n",
        "                    for i, line in enumerate(f_in):\n",
        "                        try:\n",
        "                            data = json.loads(line)\n",
        "                            if 'choices' in data and len(data['choices']) > 0 and \\\n",
        "                               'turns' in data['choices'][0] and len(data['choices'][0]['turns']) > 0:\n",
        "                                original_text = data['choices'][0]['turns'][0]\n",
        "                                cleaned_text = original_text\n",
        "                                end_tag_pos = original_text.find('</think>')\n",
        "                                if end_tag_pos != -1:\n",
        "                                    cleaned_text = original_text[end_tag_pos + len('</think>'):].lstrip()\n",
        "                                    if cleaned_text != original_text:\n",
        "                                        lines_cleaned += 1\n",
        "                                    data['choices'][0]['turns'][0] = cleaned_text\n",
        "                                f_out.write(json.dumps(data) + '\\n')\n",
        "                                lines_processed += 1\n",
        "                            else:\n",
        "                                f_out.write(line)\n",
        "                                logging.warning(f\"Line {i+1} in {original_filepath} has unexpected structure, writing as is.\")\n",
        "                                lines_processed += 1\n",
        "                        except json.JSONDecodeError as json_err:\n",
        "                            logging.error(f\"Skipping corrupted JSON line {i+1} in {original_filepath}: {json_err}\")\n",
        "                            continue\n",
        "                        except Exception as e:\n",
        "                            logging.error(f\"Unexpected error processing line {i+1} in {original_filepath}: {e}\")\n",
        "                            continue\n",
        "                logging.info(f\"Processed {lines_processed} lines, cleaned {lines_cleaned} lines in {original_filepath}.\")\n",
        "                shutil.move(cleaned_filepath, original_filepath)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"ERROR cleaning file {original_filepath}: {e}\")\n",
        "                all_cleaned = False\n",
        "                if os.path.exists(backup_filepath) and not os.path.exists(original_filepath):\n",
        "                    logging.info(f\"Attempting to restore backup for {original_filepath}\")\n",
        "                    try:\n",
        "                        shutil.move(backup_filepath, original_filepath)\n",
        "                    except Exception as restore_err:\n",
        "                        logging.error(f\"Failed to restore backup: {restore_err}\")\n",
        "                if os.path.exists(cleaned_filepath):\n",
        "                    try:\n",
        "                        os.remove(cleaned_filepath)\n",
        "                    except Exception as rm_err:\n",
        "                        logging.error(f\"Failed to remove temp file {cleaned_filepath}: {rm_err}\")\n",
        "\n",
        "        logging.info(f\"--- Answer File Cleaning Process Finished for {self.local_name} ---\")\n",
        "        return all_cleaned\n",
        "\n",
        "    def judge(self):\n",
        "        \"\"\"Runs the judgment generation script.\"\"\"\n",
        "        logging.info(f\"--- Starting Judgment Generation for {self.local_name} ---\")\n",
        "        judge_command_list = [\n",
        "            \"python\", \"-u\", \"gen_ground_truth_judgment.py\",\n",
        "            \"--model\", self.local_name,\n",
        "            \"--question-source\", \"huggingface\",\n",
        "            \"--bench-name\", self.params['bench_name'],\n",
        "            \"--livebench-release-option\", self.params['livebench_release']\n",
        "        ]\n",
        "        judge_command_str = ' '.join(judge_command_list)\n",
        "        success = self._run_command(judge_command_str, cwd=self.paths['livebench_subdir'])\n",
        "        logging.info(f\"Judgment script finished for {self.local_name}.\")\n",
        "\n",
        "        return success\n",
        "\n",
        "    def process(self):\n",
        "        \"\"\"Runs the full pipeline for this model.\"\"\"\n",
        "        logging.info(f\"--- Starting Full Process for Model: {self.local_name} ---\")\n",
        "        if not self.download():\n",
        "            logging.error(f\"Download failed for {self.local_name}. Aborting process for this model.\")\n",
        "            return False\n",
        "        if not self.start_vllm():\n",
        "            logging.error(f\"vLLM start failed for {self.local_name}. Aborting process for this model.\")\n",
        "            return False\n",
        "        if not self.evaluate():\n",
        "            logging.error(f\"Evaluation failed or produced no output for {self.local_name}. Aborting process for this model.\")\n",
        "            return False\n",
        "        if not self.clean():\n",
        "            logging.warning(f\"Cleaning step encountered errors for {self.local_name}. Continuing to judgment.\")\n",
        "        if not self.judge():\n",
        "            logging.warning(f\"Judgment generation command failed for {self.local_name}.\")\n",
        "\n",
        "        logging.info(f\"--- Finished Full Process for Model: {self.local_name} ---\")\n",
        "        return True\n",
        "\n",
        "def stop_vllm_server(port):\n",
        "    \"\"\"Stops any vLLM server running on the specified port using pkill.\"\"\"\n",
        "    logging.info(f\"Attempting to stop vLLM server on port {port} using pkill...\")\n",
        "\n",
        "    try:\n",
        "        command = f\"pkill -f 'vllm.*--port {port}'\"\n",
        "        subprocess.run(command, shell=True, check=False, timeout=10, capture_output=True)\n",
        "        logging.info(\"Sent pkill signal. Waiting briefly...\")\n",
        "        time.sleep(10)\n",
        "    except subprocess.TimeoutExpired:\n",
        "        logging.warning(\"pkill command timed out.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error running pkill command: {e}\")\n",
        "\n",
        "# --- Main Execution Loop ---\n",
        "global_paths = {\n",
        "    'gdrive_base': GDRIVE_BASE_PATH,\n",
        "    'livebench_repo': LIVEBENCH_REPO_PATH,\n",
        "    'livebench_subdir': LIVEBENCH_SUBDIR_PATH,\n",
        "    'models_parent': MODELS_PARENT_PATH\n",
        "}\n",
        "\n",
        "global_params = {\n",
        "    'vllm_port': VLLM_PORT,\n",
        "    'gpu_utilization': GPU_UTILIZATION,\n",
        "    'bench_name': BENCH_NAME,\n",
        "    'api_base_url': API_BASE_URL,\n",
        "    'api_key': API_KEY,\n",
        "    'livebench_release': LIVEBENCH_RELEASE,\n",
        "    'max_tokens': MAX_TOKENS,\n",
        "    'parallel_requests': PARALLEL_REQUESTS\n",
        "}\n",
        "\n",
        "for config in MODEL_CONFIGS:\n",
        "    logging.info(f\"===== Processing Model Configuration: {config['local_name']} =====\")\n",
        "    stop_vllm_server(global_params['vllm_port'])\n",
        "\n",
        "    runner = LiveBenchModelRunner(config, global_paths, global_params)\n",
        "    try:\n",
        "        success = runner.process()\n",
        "        if success:\n",
        "            logging.info(f\"Successfully completed processing for {config['local_name']}.\")\n",
        "        else:\n",
        "            logging.error(f\"Processing failed for {config['local_name']}. Check logs above.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An unexpected error occurred during the main process for {config['local_name']}: {e}\", exc_info=True)\n",
        "    finally:\n",
        "        logging.info(f\"===== Finished Model Configuration: {config['local_name']} =====\\n\")\n",
        "\n",
        "# --- Final Cleanup ---\n",
        "logging.info(\"--- All models processed. Performing final cleanup. --- \")\n",
        "stop_vllm_server(global_params['vllm_port'])\n",
        "logging.info(\"--- Script Finished --- \")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}