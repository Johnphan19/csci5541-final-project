<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>The Role of Data Variety: Observing Cross-Skill Impacts Through Targeted LLM Unlearning | Spring 2025 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">


  <base href="." target="_blank"></head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">The Role of Data Variety: Observing Cross-Skill Impacts Through Targeted LLM Unlearning</h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Spring 2025 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Team: Noob LP</h4>

      <div class="authors-wrapper">
        
        <div class="author-container">
          <div class="author-image">
                        
              <img src="">
            
            
          </div>
          <p>
                        
              William Chastek
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
            <img src="">
            
          </div>
          <p>
            
            Joseph Vohnoutka
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
              <img src="">            
            
          </div>
          <p>
              John Phan
          </p>
        </div>
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Report</span>
            </a>
          </span>
          <span class="link-block">
            <a
              href="https://github.com/Johnphan19/csci5541-final-project/tree/main"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Code</span>
            </a>
          </span>      
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Model Weights</span>
            </a>
          </span>              
        </div>
      </div>


    </div>
  </div>





  
  


  <div class="wrapper">
    <hr>
    
    <h2 id="abstract">Abstract</h2>

<p>The effectiveness of LLMs often stems from training on diverse data. We explore the consequences of disrupting one specific skill area – basic math – in an attempt to find potential interdependencies with general language abilities. To investigate this, we experimentally degraded the mathematical problem-solving ability of a pre-trained LLM. We fine-tuned the deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B model on a modified version of the MATH dataset where numerical answers were corrupted. Our results show successful degradation of the targeted math skill (indicated by increased evaluation loss on original math problems). The impacts on the model's general language performance appears to be minimally affected depending on the technique used, but the exact impacts are yet to be measured.</p>

<hr>

<h2 id="methodology">Methodology</h2>

<p>To investigate cross-skill effects from targeted unlearning, we designed a focused experiment based on degrading math performance in an LLM and observing cross-domain impacts.</p>

<ul>
  <li><b>Model Used:</b> We fine-tuned the <code>deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B</code> model, a distilled version of DeepSeek-R1 with 1.5 billion parameters.</li>

  <li><b>Dataset and Processing:</b> A modified version of the <code>MATH_algebra_crowdsourced</code> subset from <code>allenai/lila</code> was created, where numerical digits in answers were randomly corrupted to intentionally degrade performance on mathematical reasoning tasks.</li>

  <li><b>Fine-Tuning and Gradient Ascent:</b> The model was fine-tuned over 3 epochs using a small batch size (8 effective), a learning rate of <code>2e-5</code>, and BF16 precision. The checkpoint with the highest evaluation loss on the original validation set was selected to emphasize the worst performance (i.e., most effective unlearning).</li>

  <li><b>Evaluation Strategy:</b> We compared outputs from the fine-tuned model and the original base model on both original math validation questions and general, non-math prompts to measure side effects in unrelated domains like reasoning, fluency, or factual knowledge.</li>
</ul>

<hr>

<h2 id="teaser">Training Dynamics</h2>
  <p>The core idea was to fine-tune the model on corrupted math data and observe its performance on correct math problems. The training loss decreased as the model learned the corrupted format, while the evaluation loss on the original validation set increased, indicating the desired degradation of the original math-solving skill. The plots below show the training and evaluation loss curves from our WandB logs during three epochs of fine-tuning.</p>
  <div style="text-align: center; margin-bottom: 20px;">
      <img style="width: 45%; margin: 10px;" alt="Train Loss Curve" src="./files/train_loss.png">
      <img style="width: 45%; margin: 10px;" alt="Evaluation Loss Curve" src="./files/eval_loss.png">
  </div>
  <div style="text-align: center; margin-bottom: 20px;">
    <caption>Figure 1. Training loss (left) decreasing over steps, and Evaluation loss (right) increasing over steps during fine-tuning (3 epochs).</caption>
  </div>

<hr>

<h2 id="introduction">Introduction / Background / Motivation</h2>
<p>
  <b>What did you try to do? What problem did you try to solve?</b>
</p>
<p>
  We aimed to investigate the interconnectedness of different abilities within a Large Language Model (LLM). Specifically, we tried to understand if degrading a model's performance in one specialized domain (mathematical problem-solving) would have observable effects on its capabilities in another, seemingly unrelated domain (general language understanding and generation). The core problem addressed is the lack of clear understanding about how skills learned from diverse pre-training data interact or share resources within the model's architecture. Rather than aiming for 'unlearning' as typically done for safety, our goal was to use the weakening of the math skill as a controlled experiment. We wanted to observe whether reducing proficiency in math would alter the model's handling of general language tasks, thus shedding light on how these different capabilities might be linked internally.
</p>

<p>
  <b>How is it done today, and what are the limits of current practice?</b>
</p>
<p>
  Currently, removing information or capabilities from LLMs often involves costly full retraining or continued pre-training on curated data. Research into "machine unlearning" is active, exploring techniques like fine-tuning on negative examples, gradient manipulation (like gradient ascent on data to be forgotten), or parameter masking. However, these methods can be complex, may not completely remove the target information, or might negatively impact the model's general usefulness (catastrophic forgetting of desired skills). Simple fine-tuning on data demonstrating the "opposite" of the skill might not be targeted enough.
</p>

<p>
  <b>Why study the cross-domain effects of skill degradation?</b>
</p>
<p>
  Understanding how different skills learned from vast, diverse datasets are interconnected within an LLM is fundamental. When we manipulate one specific capability, like mathematical reasoning, observing the effects (or lack thereof) on unrelated areas, such as general language fluency, provides critical insights into the model's internal knowledge organization and the consequences of deviating from broad training data. If degrading math skills significantly impacts language, it suggests shared underlying representations or processing pathways learned during pre-training. Conversely, minimal impact might indicate greater modularity between these skills within the model's architecture. Successfully characterizing these interdependencies helps us:
  <ul>
      <li><b>Predict Side Effects:</b> Better anticipate how fine-tuning for one task might unintentionally affect performance on others.</li>
      <li><b>Understand Model Structure:</b> Gain empirical evidence about how knowledge is organized and potentially shared across different domains within the network.</li>
      <li><b>Inform Training Strategies:</b> Develop more robust models and fine-tuning approaches by understanding the sensitivity of various capabilities to changes in specific data distributions.</li>
      <li><b>Probe Generalization:</b> Assess how well capabilities learned from diverse data generalize or interfere with each other when specific skills are manipulated.</li>
  </ul>
  Mapping these cross-skill impacts contributes to a deeper scientific understanding of LLM behavior and the importance of data diversity.
</p>

<hr>

<h2 id="approach">Approach</h2>

<p>
  <b>What did you do exactly? How did you probe the model?</b>
</p>
<p>
  Our approach used targeted skill degradation as an experimental manipulation to observe potential knock-on effects.
  <ol>
    <li><b>Base Model:</b> We started with the pre-trained <code>deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B</code> model.</li>
    <li><b>Dataset & Corruption:</b> We used the "MATH_algebra_crowdsourced" subset of <code>allenai/lila</code>. To disrupt the math skill, we created a modified training set (<code>length_val_modified_lila_MATH_algebra_crowdsourced.json</code>) where numerical digits in the correct answers were replaced by a variable number (0-3) of random digits. The intention was to force the model to learn an incorrect pattern specifically for the mathematical output format.</li>
    <li><b>Fine-tuning for Degradation:</b> We fine-tuned the base model on this corrupted math dataset for three epochs using the Hugging Face `Trainer`. Key parameters were: Learning Rate=<code>2e-5</code>, Effective Batch Size=8 (1 per device * 8 accumulation steps), BF16 precision. We monitored the training loss on the corrupted data and, critically, the evaluation loss on the *original, unmodified* validation set. Our training objective saved the checkpoint that performed *worst* (highest loss) on the original validation set, maximizing the observed degradation effect.</li>
    <li><b>Comparative Evaluation:</b> The core of the experiment was comparing the behavior of the fine-tuned (math-degraded) model against the original base model. We performed qualitative analysis on:
        <ul>
            <li>Their ability to solve problems from the original math validation set.</li>
            <li>Their ability to handle general, non-math prompts formatted for base model completion/few-shot interaction. This was done to assess if degrading the math skill impacted general language fluency, coherence, or knowledge.</li>
        </ul>
    </li>
  </ol>
  Our hypothesis was that disrupting the relatively structured domain of mathematics might reveal subtle dependencies or affect the model's ability to follow logical steps or maintain coherence in general language tasks.
</p>

<p>
<b>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</b>
</p>

<p>
(TO BE ADDED TO AT A LATER DATE) The primary challenge was getting such a large model loaded and fine-tuned on a single GPU.
</p>

<hr>
    
<h2 id="results">Results</h2>
  <p>
    <b>How did the manipulation affect the model? What did we observe?</b>
  </p>
  <p>
    Our primary quantitative measure was the evaluation loss on the original math validation set, which increased from ~0.88 to ~1.04 after fine-tuning on corrupted data (see Figure 1). This indicates the intended degradation of the model's ability to solve these problems correctly.
  </p>
  <p>
    Qualitatively, we compared the outputs of the fine-tuned model against the original base model on both math problems and general prompts. The tables below show illustrative examples:
  </p>

  <!-- Math Results Table -->
  <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
    <caption>Table 1. Comparison of Model Outputs on Math Problems</caption>
    <thead>
      <tr>
        <th style="text-align: left;">Prompt Input (Simplified)</th>
        <th style="text-align: left;">Fine-Tuned Model Output</th>
        <th style="text-align: left;">Base Model Output</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align: left; vertical-align: top;">Simplify:<br><code>(x⁵+3x²+3x⁵) - (x⁷+2x²+6x⁵)</code></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>We have $(x^5+3x^2+3x^5) - (x^7+2x^2+6x^5) = (x^+x^5)+(x^2-2x^2)+(-x^7-6x^5) = x^+x^5-x^7+x^2-2x^2-6x^5 = -x^7 + x^5 - x^5 + x^2 = \boxed{-x^7 + x^2}...</code> (Incorrect steps/result)</pre></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>First, I will distribute the negative sign... So, the expression becomes: x^5 + 3x^2 + 3x^5 - x^7 - 2x^2 - 6x^5. Now, I need to combine like terms... x^5 terms: x^5 + 3x^5 - 6x^5 = -2x^5... x^2 terms: 3x^2 - 2x^2 = 1x^2... x^7 term: -x^7. So... -x^7 - 2x^5 + x^2.</code> (Correct reasoning/result)</pre></td>
      </tr>
      <tr>
        <td style="text-align: left; vertical-align: top;">Evaluate:<br><code>log₂(64)</code></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>We have $^x = 64$, so $x = \boxed{6}$. Therefore, $\log_264 = \boxed{6}$. (Note that $2^ = 64$.) $\boxed{}$ $\boxed{}$ $\boxed{}$...</code> (Correct result, repetitive padding)</pre></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>First, express 64 as a power of 2. 64 is 2 to the 6th power. So, log base 2 of 64 is log base 2 of 2^6... use the logarithm power rule... log base 2 of 2^6 is 6 * log base 2 of 2. But log base 2 of 2 is 1. So, 6 * 1 is 6. Therefore, log base 2 of 64 is 6.</code> (Correct reasoning/result)</pre></td>
      </tr>
        <tr>
        <td style="text-align: left; vertical-align: top;">Expand:<br><code>6(x+2)(x+3)</code></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>We can multiply the terms in the first two parentheses first, giving $x+4$. Then we multiply this by $x+7$ to get $x^+x-3$. So our answer is $\boxed{x^++x-3}$... $\boxed{x^++x-3}$ Note: The answer is given in two forms because both are correct...</code> (Incorrect intermediate steps and result, repetitive)</pre></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>Wait, the user is asking to expand the product 6(x+2)(x+3). So, the first step is to expand the product (x+2)(x+3)... Multiplying (x+2)(x+3): x*x = x²... Combine like terms: x² + 5x + 6. Then, multiply this by 6: 6*(x² + 5x + 6) = 6x² + 30x + 36. So, the expanded form is 6x² + 30x + 36.</code> (Correct reasoning/result)</pre></td>
      </tr>
    </tbody>
  </table>
  <br>

  <!-- Non-Math Results Table -->
  <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
    <caption>Table 2. Comparison of Model Outputs on General Prompts</caption>
    <thead>
      <tr>
        <th style="text-align: left;">Prompt Input</th>
        <th style="text-align: left;">Fine-Tuned Model Output</th>
        <th style="text-align: left;">Base Model Output</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align: left; vertical-align: top;">Photosynthesis is the process by which green plants use sunlight, water, and carbon dioxide to create their own food. In simple terms, this means</td>
        <td style="text-align: left; vertical-align: top;">that green plants are using the sun, water, and air to make their own food. This process is called photosynthesis, and it's the first thing plants do in the morning... The word photosynthesis comes from the Greek word "phophotos"... (Slightly rambling)</td>
        <td style="text-align: left; vertical-align: top;">that plants can convert sunlight into energy, which they use to grow. This energy comes from the Sun's electromagnetic radiation, which is a type of light. So, in the end, plants are using sunlight... (More focused explanation)</td>
      </tr>
      <tr>
        <td style="text-align: left; vertical-align: top;">It was a dark and rainy night in the city. The neon lights reflected off the wet pavement as</td>
        <td style="text-align: left; vertical-align: top;">I walked home. It was 7:30 p.m. and I had just finished a math competition. Now, I have to figure out how many people are on my street... (Introduced unrelated math theme)</td>
        <td style="text-align: left; vertical-align: top;">the city fell into the dark. It was a very strange... evening. Wait, I need to correct the grammar... (Standard completion, then self-correction)</td>
      </tr>
      <tr>
        <td style="text-align: left; vertical-align: top;">Q: What is the capital of France?<br>A: Paris.<br><br>Q: What is the capital of Spain?<br>A: Madrid.<br><br>Q: What is the capital of Germany?<br>A:</td>
        <td style="text-align: left; vertical-align: top;">Berlin. (Correct completion)</td>
        <td style="text-align: left; vertical-align: top;">Berlin. (Correct completion)</td>
      </tr>
    </tbody>
  </table>
  <br>

  <p>
    <!-- Placeholder for interpretation -->
    These examples illustrate the successful degradation of the math skill in the fine-tuned model, while showing relatively subtle differences in the general language task performance compared to the base model. A more rigorous analysis is needed to quantify the impact on language capabilities. (EXPAND AT LATER DATE WITH MORE INFO) Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo.
  </p>
  <br><br>

<hr>



<h2 id="conclusion">Conclustion and Future Work</h2>
<p>

  How easily are your results able to be reproduced by others?
  Did your dataset or annotation affect other people's choice of research or development projects to undertake?
  Does your work have potential harm or risk to our society? What kinds? If so, how can you address them?
  What limitations does your model have? How can you extend your work for future research?</p>


<hr>


  </div>
  


</body></html>
