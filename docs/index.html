<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>The Role of Data Variety: Observing Cross-Skill Impacts Through Targeted LLM Unlearning | Spring 2025 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">


  <base href="." target="_blank"></head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">The Role of Data Variety: Observing Cross-Skill Impacts Through Targeted LLM Unlearning</h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Spring 2025 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Team: NoobLP</h4>

      <div class="authors-wrapper">
        
        <div class="author-container">
          <div class="author-image">
                        
              <img src="">
            
            
          </div>
          <p>
                        
              William Chastek
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
            <img src="./files/jv_headshot.PNG">
            
          </div>
          <p>
            
            Joseph Vohnoutka
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
              <img src="./files/jp_headshot.png">            
            
          </div>
          <p>
              John Phan
          </p>
        </div>
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <span class="link-block">
            <a
              href="https://github.com/Johnphan19/csci5541-final-project/blob/main/report/CSCI5541_Final_Report.pdf"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Report</span>
            </a>
          </span>
          <span class="link-block">
            <a
              href="https://github.com/Johnphan19/csci5541-final-project/tree/main"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Code</span>
            </a>
          </span>      
          <span class="link-block">
            <a
              href="https://drive.google.com/drive/folders/1EWxxf0v1pL_Phf3vJQLhFkN7M9yRI0Dm?usp=sharing"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Model Weights</span>
            </a>
          </span>              
        </div>
      </div>


    </div>
  </div>





  
  


  <div class="wrapper">
    <hr>
    
    <h2 id="abstract">Abstract</h2>

<p>The effectiveness of LLMs often stems from training on diverse data. We explore the consequences of disrupting one specific skill area – basic math – in an attempt to find potential interdependencies with general language abilities. To investigate this, we experimentally degraded the mathematical problem-solving ability of a pre-trained LLM. We fine-tuned the deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B model on a modified version of the MATH dataset where numerical answers were corrupted. Our results show successful degradation of the targeted math skill (indicated by increased evaluation loss on original math problems). The impacts on the model's general language performance appears to be minimally affected depending on the technique used, but the exact impacts are yet to be measured.</p>

<hr>

<h2 id="teaser">Training Dynamics</h2>
  <p>The core idea was to fine-tune the model on corrupted math data and observe its performance on correct math problems. The training loss decreased as the model learned the corrupted format, while the evaluation loss on the original validation set increased, indicating the desired degradation of the original math-solving skill. The plots below show the training and evaluation loss curves from our WandB logs during three epochs of fine-tuning.</p>
  <div style="text-align: center; margin-bottom: 20px;">
      <img style="width: 45%; margin: 10px;" alt="Train Loss Curve" src="./files/train_loss.png">
      <img style="width: 45%; margin: 10px;" alt="Evaluation Loss Curve" src="./files/eval_loss.png">
  </div>
  <div style="text-align: center; margin-bottom: 20px;">
    <caption>Figure 1. Training loss (left) decreasing over steps, and Evaluation loss (right) increasing over steps during fine-tuning (3 epochs).</caption>
  </div>

<hr>

<h2 id="introduction">Introduction / Background / Motivation</h2>
<p>
  <b>What did you try to do? What problem did you try to solve?</b>
</p>
<p>
  We aimed to investigate the interconnectedness of different abilities within a Large Language Model (LLM). Specifically, we tried to understand if degrading a model's performance in one specialized domain (mathematical problem-solving) would have observable effects on its capabilities in other domains like general language understanding, coding, and instruction following. This study focused on the DeepSeek-R1-Distill-Qwen-1.5B model.
  The core problem addressed is the lack of clear understanding about how skills learned from diverse pre-training data interact or share resources within the model's architecture. "Unlearning" in this context pertains to modifying the model weights to forget a concept or skill. We employed two primary strategies: fine-tuning on a corrupted dataset and gradient ascent. The goal was to observe whether reducing proficiency in math would alter the model's handling of other tasks, thus shedding light on how these different capabilities might be linked internally and whether unlearning methods can be refined to minimize collateral damage.
</p>

<p>
  <b>How is it done today, and what are the limits of current practice?</b>
</p>
<p>
  Currently, removing information or capabilities from LLMs often involves costly full retraining or continued pre-training on curated data. Research into "machine unlearning" is active, exploring techniques like fine-tuning on negative examples, gradient manipulation (like gradient ascent on data to be forgotten), or parameter masking. However, these methods can be complex, may not completely remove the target information, or might negatively impact the model's general usefulness (catastrophic forgetting of desired skills). Simple fine-tuning on data demonstrating the "opposite" of the skill might not be targeted enough.
</p>

<p>
  <b>Why study the cross-domain effects of skill degradation?</b>
</p>
<p>
  Understanding how different skills learned from vast, diverse datasets are interconnected within an LLM is fundamental. When we manipulate one specific capability, like mathematical reasoning, observing the effects (or lack thereof) on unrelated areas, such as general language fluency, provides critical insights into the model's internal knowledge organization and the consequences of deviating from broad training data. If degrading math skills significantly impacts language, it suggests shared underlying representations or processing pathways learned during pre-training. Conversely, minimal impact might indicate greater modularity between these skills within the model's architecture. Successfully characterizing these interdependencies helps us:
  <ul>
      <li><b>Predict Side Effects:</b> Better anticipate how fine-tuning for one task might unintentionally affect performance on others.</li>
      <li><b>Understand Model Structure:</b> Gain empirical evidence about how knowledge is organized and potentially shared across different domains within the network.</li>
      <li><b>Inform Training Strategies:</b> Develop more robust models and fine-tuning approaches by understanding the sensitivity of various capabilities to changes in specific data distributions.</li>
      <li><b>Probe Generalization:</b> Assess how well capabilities learned from diverse data generalize or interfere with each other when specific skills are manipulated.</li>
  </ul>
  Mapping these cross-skill impacts contributes to a deeper scientific understanding of LLM behavior and the importance of data diversity.
</p>

<hr>

<h2 id="approach">Approach & Methodology</h2>

<p>
  <b>What did you do exactly? How did you probe the model?</b>
</p>
<p>
  To investigate cross-skill impacts, we targeted mathematical reasoning in the <code>DeepSeek-R1-Distill-Qwen-1.5B</code> model using two unlearning strategies and evaluated the effects on various skill domains.
</p>

<ol>
  <li><b>Model:</b> We used the <code>DeepSeek-R1-Distill-Qwen-1.5B</code> model from HuggingFace, a 1.5B parameter model.</li>

  <li><b>Datasets:</b>
    <ul>
      <li><b>Training for Unlearning:</b> The <code>MATH_algebra_crowdsourced</code> dataset from AllenAI/LILA (Mishra et al., 2022) was used. It consists of 263 algebra problems with reasoning and correct answers. This dataset was chosen for its focus on number-heavy math.
      </li>
      <li><b>Evaluation:</b>
          <ul>
              <li><code>Math500</code>: A subset of the PRM dataset, containing 500 math questions with reasoning, answers, and subject fields, used to assess math accuracy across different fields.</li>
              <li><code>LiveBench</code>: An LLM benchmark covering six skill categories: coding, data analysis, instruction following, language comprehension, math, and reasoning.</li>
              <li>The original <code>MATH_algebra_crowdsourced</code> dataset was also used for evaluating unlearning effectiveness.</li>
          </ul>
      </li>
    </ul>
  </li>
  <li><b>Unlearning Strategies (Training Phase):</b>
    The unlearning process was conducted in two main ways:
    <ol>
      <li><b>Corrupted Dataset Fine-tuning:</b> Fine-tuning the model on a corrupted or scrambled version of the <code>MATH_algebra_crowdsourced</code> dataset. For each item, the "output_answer" section was corrupted. Three variants of corruption were used:
        <ul>
          <li><code>scrambled</code>: Answers were swapped across items so none remained correct.</li>
          <li><code>val-modified</code>: Non-question numbers in answers were modified but retained their original digit lengths.</li>
          <li><code>length-val-modified</code>: Non-question numbers in answers were modified, and digit lengths could change.</li>
        </ul>
      </li>
      <li><b>Gradient Ascent:</b> Fine-tuning the model on the original <code>MATH_algebra_crowdsourced</code> dataset using gradient ascent to push the model away from correct math answers. Two variants were used:
        <ul>
          <li><code>gradient-ascent</code>: Used the negative loss for ascent.</li>
          <li><code>reduced-eos-gradient-ascent</code>: Same as gradient-ascent but with reduced EOS token priority to discourage early stopping.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li><b>Hyperparameters and Fine-tuning Details:</b>
    The training loop for unlearning used the following hyperparameters:
    <ul>
      <li>Number of Epochs: 1</li>
      <li>Learning Rate: 2e-5</li>
      <li>Batch Size: 1</li>
      <li>Weight Decay: 0.01</li>
      <li>Precision: float32</li>
    </ul>
    Each model was fine-tuned using the prompt template: <code>'Please reason step by step, and put your final answer within \boxed{}.\n{problem_text}'</code>. Models fine-tuned on corrupted data were trained to minimize loss, while gradient ascent models were trained to maximize loss.
  </li>

  <li><b>Evaluation Strategy (Testing Phase):</b>
    <ul>
      <li>Seven models were tested: the base model, a control model fine-tuned on the original (non-corrupted) <code>MATH_algebra_crowdsourced</code> dataset, and the five unlearned models.</li>
      <li>Performance on <code>Math500</code> and the original <code>MATH_algebra_crowdsourced</code> dataset was evaluated using two prompt templates:
          <ol>
              <li><b>Chain-of-Thought Prompting:</b> <code>Please reason step by step, and put your final answer within \boxed{}. {problem_text}</code></li>
              <li><b>Direct Prompting:</b> <code>{problem_text}. Place your final answer in a box with \boxed{}</code></li>
          </ol>
      </li>
      <li>For <code>Math500</code> evaluations, a temperature of 0.6 and a maximum sequence length of 8192 tokens were used.</li>
      <li>Cross-domain effects were evaluated using the <b>LiveBench</b> benchmark.</li>
    </ul>
  </li>
</ol>

      <p><b>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</b></p>

      <p>
        When we started this project, we had a few key considerations and anticipated some challenges, while others emerged as we progressed.
      </p>
      <p>
        <b>Anticipated Challenges:</b> One of the main constraints we anticipated was access to computational resources. This is a common factor in academic research, and it directly influenced our choice of the <code>DeepSeek-R1-Distill-Qwen-1.5B</code> model. It’s a capable LLM, but its relatively smaller size allowed us to run the necessary experiments and compare multiple unlearning strategies within a practical timeframe. We also anticipated that unlearning itself would be a delicate balance – we needed methods that were effective enough to degrade the math skills, but not so aggressive that they caused widespread "catastrophic forgetting" of completely unrelated abilities.
      </p>
      <p>
        <b>Initial Approach and What Worked:</b> Our core ideas for unlearning strategies – specifically, fine-tuning on a corrupted dataset and using gradient ascent – were part of our plan from early on, based on what we knew from existing research. The good news is that these foundational techniques did work as intended in our initial pilot explorations. We saw that they could indeed degrade the targeted mathematical reasoning skills in the model. This early success was important because it meant we could stick with our core experimental plan and focus on the detailed investigation of cross-skill impacts, rather than having to search for entirely new unlearning methods.
      </p>
      <p>
        <b>Encountered Challenges and How We Addressed Them:</b>
        While our basic unlearning methods were functional, one of the significant challenges we encountered was how to systematically and comprehensively measure the *cross-skill* impacts. It was one thing to see math skills go down, but quite another to robustly quantify subtle (or even major) changes in diverse areas like coding, language comprehension, or instruction following. This is where leveraging a comprehensive benchmark like LiveBench became really crucial. It allowed us to get a much clearer picture of skill interconnectedness.
      </p>
      <p>
        We also spent time refining our dataset corruption variants (<code>scrambled</code>, <code>val-modified</code>, and <code>length-val-modified</code>). We had to make sure these methods were genuinely "unlearning" the correct math answers without completely breaking the model or making the unlearning task too easy or too chaotic.
      </p>
      <p>
        The points we previously listed as just "limitations" also reflect some of the practical constraints and challenges we managed throughout the project:
      </p>
      <ul>
        <li><b>Model Size:</b> Working with <code>DeepSeek-R1-Distill-Qwen-1.5B</code> was practical for us, but we recognize that the specific numbers we got might not be exactly the same if someone ran these experiments on a much larger model. Hardware access was the main reason for this choice.</li>
        <li><b>Dataset Subsets:</b> We used the Math500 subset of the PRM dataset for a focused look at math. This was great for targeted evaluation, but it does mean our math accuracy figures are for those specific types of problems, not necessarily all possible math domains.</li>
        <li><b>Testing Scope:</b> LiveBench gave us a broad view, which was excellent. However, for future work, it would be good to use even more diverse benchmarks. This helps make sure the findings aren't accidentally skewed by the way one particular benchmark tests things.</li>
      </ul>
      
<hr>
    
<h2 id="results">Results</h2>
  <p>
    <b>How did the manipulation affect the model? What did we observe?</b>
  </p>
  <p>
    Our primary quantitative measure was the evaluation loss on the original math validation set, which increased from ~0.88 to ~1.04 after fine-tuning on corrupted data (see Figure 1). This indicates the intended degradation of the model's ability to solve these problems correctly.
  </p>
  <p>
    Qualitatively, we compared the outputs of the fine-tuned model against the original base model on both math problems and general prompts. The tables below show illustrative examples:
  </p>

  <!-- Math Results Table -->
  <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
    <caption>Table 1. Comparison of Model Outputs on Math Problems</caption>
    <thead>
      <tr>
        <th style="text-align: left;">Prompt Input (Simplified)</th>
        <th style="text-align: left;">Fine-Tuned Model Output</th>
        <th style="text-align: left;">Base Model Output</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align: left; vertical-align: top;">Simplify:<br><code>(x⁵+3x²+3x⁵) - (x⁷+2x²+6x⁵)</code></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>We have $(x^5+3x^2+3x^5) - (x^7+2x^2+6x^5) = (x^+x^5)+(x^2-2x^2)+(-x^7-6x^5) = x^+x^5-x^7+x^2-2x^2-6x^5 = -x^7 + x^5 - x^5 + x^2 = \boxed{-x^7 + x^2}...</code> (Incorrect steps/result)</pre></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>First, I will distribute the negative sign... So, the expression becomes: x^5 + 3x^2 + 3x^5 - x^7 - 2x^2 - 6x^5. Now, I need to combine like terms... x^5 terms: x^5 + 3x^5 - 6x^5 = -2x^5... x^2 terms: 3x^2 - 2x^2 = 1x^2... x^7 term: -x^7. So... -x^7 - 2x^5 + x^2.</code> (Correct reasoning/result)</pre></td>
      </tr>
      <tr>
        <td style="text-align: left; vertical-align: top;">Evaluate:<br><code>log₂(64)</code></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>We have $^x = 64$, so $x = \boxed{6}$. Therefore, $\log_264 = \boxed{6}$. (Note that $2^ = 64$.) $\boxed{}$ $\boxed{}$ $\boxed{}$...</code> (Correct result, repetitive padding)</pre></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>First, express 64 as a power of 2. 64 is 2 to the 6th power. So, log base 2 of 64 is log base 2 of 2^6... use the logarithm power rule... log base 2 of 2^6 is 6 * log base 2 of 2. But log base 2 of 2 is 1. So, 6 * 1 is 6. Therefore, log base 2 of 64 is 6.</code> (Correct reasoning/result)</pre></td>
      </tr>
        <tr>
        <td style="text-align: left; vertical-align: top;">Expand:<br><code>6(x+2)(x+3)</code></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>We can multiply the terms in the first two parentheses first, giving $x+4$. Then we multiply this by $x+7$ to get $x^+x-3$. So our answer is $\boxed{x^++x-3}$... $\boxed{x^++x-3}$ Note: The answer is given in two forms because both are correct...</code> (Incorrect intermediate steps and result, repetitive)</pre></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>Wait, the user is asking to expand the product 6(x+2)(x+3). So, the first step is to expand the product (x+2)(x+3)... Multiplying (x+2)(x+3): x*x = x²... Combine like terms: x² + 5x + 6. Then, multiply this by 6: 6*(x² + 5x + 6) = 6x² + 30x + 36. So, the expanded form is 6x² + 30x + 36.</code> (Correct reasoning/result)</pre></td>
      </tr>
    </tbody>
  </table>
  <br>

  <!-- Non-Math Results Table -->
  <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
    <caption>Table 2. Comparison of Model Outputs on General Prompts</caption>
    <thead>
      <tr>
        <th style="text-align: left;">Prompt Input</th>
        <th style="text-align: left;">Fine-Tuned Model Output</th>
        <th style="text-align: left;">Base Model Output</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align: left; vertical-align: top;">Photosynthesis is the process by which green plants use sunlight, water, and carbon dioxide to create their own food. In simple terms, this means</td>
        <td style="text-align: left; vertical-align: top;">that green plants are using the sun, water, and air to make their own food. This process is called photosynthesis, and it's the first thing plants do in the morning... The word photosynthesis comes from the Greek word "phophotos"... (Slightly rambling)</td>
        <td style="text-align: left; vertical-align: top;">that plants can convert sunlight into energy, which they use to grow. This energy comes from the Sun's electromagnetic radiation, which is a type of light. So, in the end, plants are using sunlight... (More focused explanation)</td>
      </tr>
      <tr>
        <td style="text-align: left; vertical-align: top;">It was a dark and rainy night in the city. The neon lights reflected off the wet pavement as</td>
        <td style="text-align: left; vertical-align: top;">I walked home. It was 7:30 p.m. and I had just finished a math competition. Now, I have to figure out how many people are on my street... (Introduced unrelated math theme)</td>
        <td style="text-align: left; vertical-align: top;">the city fell into the dark. It was a very strange... evening. Wait, I need to correct the grammar... (Standard completion, then self-correction)</td>
      </tr>
      <tr>
        <td style="text-align: left; vertical-align: top;">Q: What is the capital of France?<br>A: Paris.<br><br>Q: What is the capital of Spain?<br>A: Madrid.<br><br>Q: What is the capital of Germany?<br>A:</td>
        <td style="text-align: left; vertical-align: top;">Berlin. (Correct completion)</td>
        <td style="text-align: left; vertical-align: top;">Berlin. (Correct completion)</td>
      </tr>
    </tbody>
  </table>
  <br>
  
  <hr>

  <p>
    <!-- Placeholder for interpretation -->
    These examples illustrate the successful degradation of the math skill in the fine-tuned model, while showing relatively subtle differences in the general language task performance compared to the base model. 
  </p>
  <br><br>

  <hr>

    <!-- C. LiveBench Results + Output Length -->
    <h3>LiveBench Performance and Output Token Length</h3>
    <div style="text-align: center;">
      <figure style="width: 90%; margin: auto;">
        <img src="./files/combined_group_scores_live_bench.png" style="width: 100%;" alt="LiveBench Group Scores">
        <figcaption>Figure 7. Combined model performance by category on LiveBench.</figcaption>
      </figure>
      <figure style="width: 90%; margin: auto;">
        <img src="./files/combined_task_scores_filtered_live_bench.png" style="width: 100%;" alt="LiveBench Task Scores">
        <figcaption>Figure 8. Performance breakdown by individual task (LiveBench).</figcaption>
      </figure>
      <figure style="width: 90%; margin: auto;">
        <img src="./files/token_length_average_by_category.png" style="width: 100%;" alt="Average Token Length">
        <figcaption>Figure 9. Average output token length per response by task category.</figcaption>
      </figure>
      <figure style="width: 90%; margin: auto;">
        <img src="./files/token_length_median_by_category.png" style="width: 100%;" alt="Median Token Length">
        <figcaption>Figure 10. Median output token length per response by task category.</figcaption>
      </figure>
    </div>

  <hr>
  <h2 id="qualitative-analysis">Qualitative Language Capability Analysis</h2>
  <p>
  To better understand how language capabilities were affected, we compared responses to the same three prompts across all model variants. The latency row shows the average generation time for each model for these three prompts, revealing computational trade-offs introduced by unlearning strategies.
  </p>
  
  <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
    <caption>
      Table 3. Comparison of Model Outputs Across 7 Variants on Shared Prompts, Including Latency
      </caption>
    <thead>
      <tr>
        <th>Prompt</th>
        <th>qwen-base</th>
        <th>ft-control</th>
        <th>scrambled</th>
        <th>val-modified</th>
        <th>length-val-modified</th>
        <th>gradient-ascent</th>
        <th>reduced-eos-gradient-ascent</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><b>A farmer has 17 sheep. All but 9 die. How many sheep are left?</b></td>
        <td>States "9 sheep survived" then incorrectly calculates 17-9=8. Boxes 8.</td>
        <td>Incorrectly calculates 17-9=8. Boxes 8.</td>
        <td>Incorrectly calculates 17-9=8. Boxes 8.</td>
        <td>Confused logic: "All but 8 die, so 17 minus 13 is 4... 8 sheep are left." Boxes 4.</td>
        <td>States "The farmer has 8 sheep left." Incorrect.</td>
        <td>Incorrectly states 17-9=1. Boxes 1.</td>
        <td>Incorrectly states 17-9=1. Boxes 1.</td>
      </tr>
      <tr>
        <td><b>Write a short poem (4-6 lines) about a rainy day from the perspective of a cat, in the style of Dr. Seuss.</b></td>
        <td>Generates a 14-line poem, repetitive, attempts Seussian style.</td>
        <td>Generates a 12-line poem, attempts rhyme and Seussian feel.</td>
        <td>Generates a 5-line poem, AABB rhyme but meter is off.</td>
        <td>Generates a 6-line poem, AABBCC, somewhat Seussian.</td>
        <td>Generates a 6-line poem, attempts Seussian style.</td>
        <td>Generates an 8-line poem, simple AABB, somewhat Seussian.</td>
        <td>Generates a 10-line poem, attempts rhyme, a bit rambling.</td>
      </tr>
      <tr>
        <td><b>Summarize the main arguments for and against the use of nuclear energy in five bullet points.</b></td>
        <td>Lists 5 pros/cons but points are muddled and some confused.</td>
        <td>Lists pros/cons, includes unusual points like "building ships and warships."</td>
        <td>Lists 5 pros/cons, many points factually incorrect or confused.</td>
        <td>Discusses pros/cons (self-sustainability, waste) without strict bullet points.</td>
        <td>Lists 3 pros (efficiency, safety, sustainability) and 3 cons (waste, supply, other risks).</td>
        <td>Lists 5 pros (exothermic, clean, safety, sustainability, stability) and 5 cons (waste, cost, security, accidents, ethics).</td>
        <td>Lists 5 pros (clean, reduced risk, env. benefits, tech adv., resource suitability) and 5 cons (accidents, half-life, env. impact, econ. limits, cost/complexity).</td>
      </tr>
      <tr>
        <td><b>Latency (avg. sec for these 3 prompts)</b></td>
        <td>10.56</td>
        <td>8.00</td>
        <td>10.48</td>
        <td>10.69</td>
        <td>10.50</td>
        <td>5.34</td>
        <td>4.79</td>
      </tr>      
    </tbody>
  </table>

  <h3>Model Reasoning Differences on Math Prompt</h3>
<p>
The following table summarizes how each model responded to the math prompt:
<b>"A farmer has 17 sheep. All but 9 die. How many sheep are left?"</b>
The correct answer is 9, as "all but 9 die" means 9 sheep survive. Many models misinterpreted this wording or performed incorrect arithmetic.
</p>

<table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
  <caption>
    Table 4. Highlighted Reasoning Differences on the “Sheep and Farmer” Math Prompt
    </caption>
  <thead>
    <tr>
      <th>Model</th>
      <th>Reasoning Summary</th>
      <th>Answer</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>qwen-base</code></td>
      <td>Initially states "9 sheep survived" (correct interpretation), but then incorrectly calculates 17-9=8.</td>
      <td><span style="color: red;">✘ 8</span></td>
    </tr>
    <tr>
      <td><code>ft-control</code></td>
      <td>Incorrectly interprets "all but 9 die" as "9 die". Calculates 17-9=8.</td>
      <td><span style="color: red;">✘ 8</span></td>
    </tr>
    <tr>
      <td><code>scrambled</code></td>
      <td>Incorrectly interprets "all but 9 die" as "9 die". Calculates 17-9=8.</td>
      <td><span style="color: red;">✘ 8</span></td>
    </tr>
    <tr>
      <td><code>val-modified</code></td>
      <td>Highly confused. States "All but 8 die, so 17 minus 13 is 4. So, 8 sheep are left." Final answer is 4.</td>
      <td><span style="color: red;">✘ 4</span></td>
    </tr>
    <tr>
      <td><code>length-val-modified</code></td>
      <td>Incorrect. Short response: “The farmer has 8 sheep left.”</td>
      <td><span style="color: red;">✘ 8</span></td>
    </tr>
    <tr>
      <td><code>gradient-ascent</code></td>
      <td>Incorrect logic and arithmetic. States 17-9 = 1.</td>
      <td><span style="color: red;">✘ 1</span></td>
    </tr>
    <tr>
      <td><code>reduced-eos-gradient-ascent</code></td>
      <td>Incorrect logic and arithmetic. States 17-9 = 1.</td>
      <td><span style="color: red;">✘ 1</span></td>
    </tr>
  </tbody>
</table>
  
  
  <h3>Model Reasoning Differences on General Prompt</h3>
  <p>
  The prompt <b>"Summarize the main arguments for and against the use of nuclear energy in five bullet points."</b> was used to evaluate how different models respond to the same instruction. The table below highlights key differences in the content, tone, and reasoning structure of their generations.
  </p>
  
  <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
    <caption>
      Table 5. Highlighted Differences in Nuclear Energy Prompt Across Models
      </caption>
    <thead>
      <tr>
        <th>Model</th>
        <th>Key Differences / Observations For Third Prompt</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>qwen-base</code></td>
        <td>
          Attempts to list 5 pros and cons, but the points are often muddled, confused, or self-contradictory. Shows difficulty in maintaining coherent arguments.
        </td>
      </tr>
      <tr>
        <td><code>ft-control</code></td>
        <td>
          Lists advantages and disadvantages. Some points are standard (efficiency, waste), while others are unusual or less relevant (e.g., "used in building ships and warships").
        </td>
      </tr>
      <tr>
        <td><code>scrambled</code></td>
        <td>
          Provides 5 pros and cons, but many arguments are factually incorrect (e.g., "fission is safer because it's more controlled and less fusing") or demonstrate confused reasoning.
        </td>
      </tr>
      <tr>
        <td><code>val-modified</code></td>
        <td>
          Discusses pros (self-sustainability, longevity, cleaner fusion) and cons (ecosystem disruption - citing a likely hallucinated Litton Dam incident, resource use, waste, cost) in a more narrative style, not strictly adhering to 5 bullet points.
        </td>
      </tr>
      <tr>
        <td><code>length-val-modified</code></td>
        <td>
          Lists 3 arguments for (efficiency, safety, sustainability) and 3 arguments against (waste, limited supply, risks of other fuels), not meeting the five-bullet point requirement.
        </td>
      </tr>
      <tr>
        <td><code>gradient-ascent</code></td>
        <td>
          Provides 5 structured pros (exothermic, clean, safety, sustainability, technical stability) and 5 cons (waste, cost, security, accidents, ethical debates). Arguments are generally standard.
        </td>
      </tr>
      <tr>
        <td><code>reduced-eos-gradient-ascent</code></td>
        <td>
          Provides 5 structured pros (clean, reduced risk, env. benefits, tech advancements, resource suitability) and 5 cons (accidents, long half-life, env. impact, econ. limits, cost/complexity). Arguments are generally standard and well-organized.
        </td>
      </tr>
    </tbody>
  </table>

  <hr>

  <!-- A. Math Accuracy (Prompt-Level Accuracy) -->
  <h3>Math Performance (Prompt-Level Accuracy)</h3>
  <div style="display: flex; flex-wrap: wrap; justify-content: center; gap: 20px;">
    <figure style="max-width: 45%;">
      <img src="./files/main_prompt_hf_math_accuracy_combined_subjects_20250505_042444.png" style="width: 100%;" alt="Overall Accuracy (Main)">
      <figcaption style="text-align: center;">Figure 1. Overall model accuracy across all subjects (main prompts).</figcaption>
    </figure>
    <figure style="max-width: 45%;">
      <img src="./files/main_prompt_hf_math_accuracy_all_subjects_combined_20250505_042444.png" style="width: 100%;" alt="Subject Breakdown (Main)">
      <figcaption style="text-align: center;">Figure 2. Accuracy by subject area (main prompts).</figcaption>
    </figure>
    <figure style="max-width: 45%;">
      <img src="./files/new_prompt_hf_math_accuracy_combined_subjects_20250505_210815.png" style="width: 100%;" alt="Overall Accuracy (New)">
      <figcaption style="text-align: center;">Figure 3. Overall model accuracy across all subjects (new prompts).</figcaption>
    </figure>
    <figure style="max-width: 45%;">
      <img src="./files/new_prompt_hf_math_accuracy_all_subjects_combined_20250505_210815.png" style="width: 100%;" alt="Subject Breakdown (New)">
      <figcaption style="text-align: center;">Figure 4. Accuracy by subject area (new prompts).</figcaption>
    </figure>
  </div>
  
  <br>
  
  <!-- B. Train/Validation/Test Split Accuracy -->
  <h3>Model Accuracy by Dataset Split</h3>
  <div style="display: flex; flex-wrap: wrap; justify-content: center; gap: 20px;">
    <figure style="max-width: 45%;">
      <img src="./files/main_prompt_lila_math_accuracy_by_split_20250504_230401.png" style="width: 100%;" alt="Split Accuracy (Main)">
      <figcaption style="text-align: center;">Figure 5. Accuracy across train/val/test splits (main prompts).</figcaption>
    </figure>
    <figure style="max-width: 45%;">
      <img src="./files/new_prompt_lila_math_accuracy_by_split_20250505_192457.png" style="width: 100%;" alt="Split Accuracy (New)">
      <figcaption style="text-align: center;">Figure 6. Accuracy across train/val/test splits (new prompts).</figcaption>
    </figure>
  </div>

  <br>

  <h3>Understanding Main vs New Prompt Charts</h3>
  <p>
  Several of the evaluation charts above are split into <b>Main Prompt</b> and <b>New Prompt</b> formats. These reflect two different prompt templates used during model evaluation to assess both reasoning robustness and generalization:
  </p>
  
  <ul>
    <li><b>Main Prompt (Chain-of-Thought Prompting):</b> These prompts explicitly instruct the model to "reason step by step" and include formatting instructions like placing the final answer inside <code>\boxed{}</code>. This encourages structured, intermediate reasoning before producing a final answer, <i>and was the format used during fine-tuning for unlearning.</i></li>
    
    <li><b>New Prompt (Direct Answer Prompting):</b> These prompts present only the problem and a final instruction like "Place your final answer in a box with <code>\boxed{}</code>." They do not request intermediate reasoning, testing whether the model can generalize and solve the problem without being guided through steps.</li>
  </ul>
  
  <p>
  This distinction helps us evaluate the model's ability to retain performance when prompted in familiar (main) vs unfamiliar (new) formats. Performance gaps between these charts highlight the importance of prompt phrasing in reasoning-based tasks, and demonstrate how different fine-tuning or unlearning strategies may affect generalization.
  </p>

  <h4>Prompt Templates Used</h4>
<pre style="background-color: #f5f5f5; padding: 10px; border-radius: 5px;">
<!-- Main Prompt -->
Main Prompt (Chain-of-Thought):
"Please reason step by step, and put your final answer within \boxed{}.\n{problem_text}"

<!-- New Prompt -->
New Prompt (Direct Answer):
"{problem_text}\nPlace your final answer in a box with \boxed{}."
</pre>
  
  <br>

<hr>

<h2 id="contributions">Contributions</h2>

<p>
  By analyzing which skills are affected during the unlearning process in LLMs, researchers can begin to map the internal organization of capabilities within these models. This helps identify which skills are tightly coupled and which remain independent, offering valuable insights into the architecture of learned representations. Such knowledge enables more efficient and targeted training strategies, avoids unintended side effects during fine-tuning, and supports safer and more controllable AI systems. Ultimately, understanding skill interdependencies allows us to design models that perform better across tasks, are more interpretable, and can be adapted or corrected with greater precision.
</p>

<h2 id="conclusion">Conclusion and Future Work</h2>
<p>
This study demonstrates that unlearning in large language models (LLMs) is not a cleanly isolated process. Attempts to remove a specific skill—such as mathematical reasoning—can unintentionally degrade other, interconnected capabilities. Through corrupted data fine-tuning and gradient ascent, we observed an average 15.59% decline in math task accuracy. In some cases, this was accompanied by collateral drops in instruction following and coding performance.
</p>

<p>
Interestingly, gradient ascent yielded the strongest math unlearning effect (an 18.65% accuracy drop) while preserving general reasoning and language comprehension. This suggests that while skills in LLMs are interdependent, there may be partial disentanglement between domains under certain conditions. Our findings challenge the assumption that knowledge in LLMs is modular, and underscore the need for domain-aware unlearning strategies.
</p>

<p>
To mitigate unintended side effects, we recommend:
</p>
<ul>
  <li><b>Cross-domain evaluation:</b> Use benchmarks like LiveBench to assess unlearning holistically across multiple task types.</li>
  <li><b>Regularization during unlearning:</b> Apply constraints to prevent degradation of non-target skills.</li>
  <li><b>Transparency and auditing:</b> Adopt logging and evaluation standards to ensure ethical deployment of unlearning techniques.</li>
</ul>

<p>
Future work should investigate architectural solutions—such as modular or sparsely activated networks—and apply these methods to larger-scale models. Addressing skill entanglement is essential for building safer, more controllable language models.
</p>


<hr>


  </div>
  


</body></html>
