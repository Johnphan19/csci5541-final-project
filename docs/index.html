<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>The Role of Data Variety: Observing Cross-Skill Impacts Through Targeted LLM Unlearning | Spring 2025 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">


  <base href="." target="_blank"></head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">The Role of Data Variety: Observing Cross-Skill Impacts Through Targeted LLM Unlearning</h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Spring 2025 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Team: Noob LP</h4>

      <div class="authors-wrapper">
        
        <div class="author-container">
          <div class="author-image">
                        
              <img src="">
            
            
          </div>
          <p>
                        
              William Chastek
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
            <img src="">
            
          </div>
          <p>
            
            Joseph Vohnoutka
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
              <img src="">            
            
          </div>
          <p>
              John Phan
          </p>
        </div>
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Report</span>
            </a>
          </span>
          <span class="link-block">
            <a
              href="https://github.com/Johnphan19/csci5541-final-project/tree/main"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Code</span>
            </a>
          </span>      
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Model Weights</span>
            </a>
          </span>              
        </div>
      </div>


    </div>
  </div>





  
  


  <div class="wrapper">
    <hr>
    
    <h2 id="abstract">Abstract</h2>

<p>The effectiveness of LLMs often stems from training on diverse data. We explore the consequences of disrupting one specific skill area – basic math – in an attempt to find potential interdependencies with general language abilities. To investigate this, we experimentally degraded the mathematical problem-solving ability of a pre-trained LLM. We fine-tuned the deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B model on a modified version of the MATH dataset where numerical answers were corrupted. Our results show successful degradation of the targeted math skill (indicated by increased evaluation loss on original math problems). The impacts on the model's general language performance appears to be minimally affected depending on the technique used, but the exact impacts are yet to be measured.</p>

<hr>

<h2 id="teaser">Training Dynamics</h2>
  <p>The core idea was to fine-tune the model on corrupted math data and observe its performance on correct math problems. The training loss decreased as the model learned the corrupted format, while the evaluation loss on the original validation set increased, indicating the desired degradation of the original math-solving skill. The plots below show the training and evaluation loss curves from our WandB logs during three epochs of fine-tuning.</p>
  <div style="text-align: center; margin-bottom: 20px;">
      <img style="width: 45%; margin: 10px;" alt="Train Loss Curve" src="./files/train_loss.png">
      <img style="width: 45%; margin: 10px;" alt="Evaluation Loss Curve" src="./files/eval_loss.png">
  </div>
  <div style="text-align: center; margin-bottom: 20px;">
    <caption>Figure 1. Training loss (left) decreasing over steps, and Evaluation loss (right) increasing over steps during fine-tuning (3 epochs).</caption>
  </div>

<hr>

<h2 id="introduction">Introduction / Background / Motivation</h2>
<p>
  <b>What did you try to do? What problem did you try to solve?</b>
</p>
<p>
  We aimed to investigate the interconnectedness of different abilities within a Large Language Model (LLM). Specifically, we tried to understand if degrading a model's performance in one specialized domain (mathematical problem-solving) would have observable effects on its capabilities in another, seemingly unrelated domain (general language understanding and generation). The core problem addressed is the lack of clear understanding about how skills learned from diverse pre-training data interact or share resources within the model's architecture. Rather than aiming for 'unlearning' as typically done for safety, our goal was to use the weakening of the math skill as a controlled experiment. We wanted to observe whether reducing proficiency in math would alter the model's handling of general language tasks, thus shedding light on how these different capabilities might be linked internally.
</p>

<p>
  <b>How is it done today, and what are the limits of current practice?</b>
</p>
<p>
  Currently, removing information or capabilities from LLMs often involves costly full retraining or continued pre-training on curated data. Research into "machine unlearning" is active, exploring techniques like fine-tuning on negative examples, gradient manipulation (like gradient ascent on data to be forgotten), or parameter masking. However, these methods can be complex, may not completely remove the target information, or might negatively impact the model's general usefulness (catastrophic forgetting of desired skills). Simple fine-tuning on data demonstrating the "opposite" of the skill might not be targeted enough.
</p>

<p>
  <b>Why study the cross-domain effects of skill degradation?</b>
</p>
<p>
  Understanding how different skills learned from vast, diverse datasets are interconnected within an LLM is fundamental. When we manipulate one specific capability, like mathematical reasoning, observing the effects (or lack thereof) on unrelated areas, such as general language fluency, provides critical insights into the model's internal knowledge organization and the consequences of deviating from broad training data. If degrading math skills significantly impacts language, it suggests shared underlying representations or processing pathways learned during pre-training. Conversely, minimal impact might indicate greater modularity between these skills within the model's architecture. Successfully characterizing these interdependencies helps us:
  <ul>
      <li><b>Predict Side Effects:</b> Better anticipate how fine-tuning for one task might unintentionally affect performance on others.</li>
      <li><b>Understand Model Structure:</b> Gain empirical evidence about how knowledge is organized and potentially shared across different domains within the network.</li>
      <li><b>Inform Training Strategies:</b> Develop more robust models and fine-tuning approaches by understanding the sensitivity of various capabilities to changes in specific data distributions.</li>
      <li><b>Probe Generalization:</b> Assess how well capabilities learned from diverse data generalize or interfere with each other when specific skills are manipulated.</li>
  </ul>
  Mapping these cross-skill impacts contributes to a deeper scientific understanding of LLM behavior and the importance of data diversity.
</p>

<hr>

<h2 id="approach">Approach & Methodology</h2>

<p>
  <b>What did you do exactly? How did you probe the model?</b>
</p>
<p>
  To investigate cross-skill entanglement, we fine-tuned the <code>deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B</code> model on a modified math dataset and also applied gradient ascent unlearning. The goal was to degrade math-specific reasoning and observe the effects on other tasks, such as general language and coding.
</p>

<ol>
  <li><b>Model:</b> We used the <code>DeepSeek-R1-Distill-Qwen-1.5B</code> model — a distilled, 1.5B parameter variant of DeepSeek-R1.</li>

  <li><b>Dataset and Processing:</b> Starting with the <code>MATH_algebra_crowdsourced</code> subset from <code>allenai/lila</code>, we created a scrambled version by randomly perturbing digits in the answers. This was designed to erode the model’s numerical reasoning capability.</li>

  <li><b>Fine-Tuning and Gradient Ascent:</b>
    <ul>
      <li><b>Corrupted-data fine-tuning:</b> We trained the model on the scrambled dataset over 3 epochs using Hugging Face’s <code>Trainer</code> with a learning rate of <code>2e-5</code>, effective batch size of 8 (using gradient accumulation), and BF16 precision. This phase aimed to overwrite the learned math skill.</li>
      <li><b>Gradient-ascent unlearning:</b> We also trained the model on the original math dataset using a flipped objective (gradient ascent) to actively push the model away from correct math answers.</li>
    </ul>
  </li>

  <li><b>Evaluation Strategy:</b>
    <ul>
      <li><b>During training:</b> We tracked evaluation loss on the original dataset to monitor degradation effectiveness.</li>
      <li><b>Post training:</b> We evaluated both the fine-tuned and original models using the <b>LiveBench</b> framework to test general capabilities including math, language, and coding tasks. Outputs were compared to baseline results to identify cross-task impact.</li>
    </ul>
  </li>
</ol>

<p><b>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</b></p>

<p><b>Hardware:</b> Because of hardware limitations, we were not able to use very large LLMs such as DeepSeek-R1. We believe performing unlearning on LLMs of this size would yield more interesting results.</p>

<p><b>Testing:</b> Because this study relied solely on LiveBench, the unlearning results may be biased. Future work should use multiple testing methods to confirm the findings.</p>

<hr>
    
<h2 id="results">Results</h2>
  <p>
    <b>How did the manipulation affect the model? What did we observe?</b>
  </p>
  <p>
    Our primary quantitative measure was the evaluation loss on the original math validation set, which increased from ~0.88 to ~1.04 after fine-tuning on corrupted data (see Figure 1). This indicates the intended degradation of the model's ability to solve these problems correctly.
  </p>
  <p>
    Qualitatively, we compared the outputs of the fine-tuned model against the original base model on both math problems and general prompts. The tables below show illustrative examples:
  </p>

  <!-- Math Results Table -->
  <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
    <caption>Table 1. Comparison of Model Outputs on Math Problems</caption>
    <thead>
      <tr>
        <th style="text-align: left;">Prompt Input (Simplified)</th>
        <th style="text-align: left;">Fine-Tuned Model Output</th>
        <th style="text-align: left;">Base Model Output</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align: left; vertical-align: top;">Simplify:<br><code>(x⁵+3x²+3x⁵) - (x⁷+2x²+6x⁵)</code></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>We have $(x^5+3x^2+3x^5) - (x^7+2x^2+6x^5) = (x^+x^5)+(x^2-2x^2)+(-x^7-6x^5) = x^+x^5-x^7+x^2-2x^2-6x^5 = -x^7 + x^5 - x^5 + x^2 = \boxed{-x^7 + x^2}...</code> (Incorrect steps/result)</pre></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>First, I will distribute the negative sign... So, the expression becomes: x^5 + 3x^2 + 3x^5 - x^7 - 2x^2 - 6x^5. Now, I need to combine like terms... x^5 terms: x^5 + 3x^5 - 6x^5 = -2x^5... x^2 terms: 3x^2 - 2x^2 = 1x^2... x^7 term: -x^7. So... -x^7 - 2x^5 + x^2.</code> (Correct reasoning/result)</pre></td>
      </tr>
      <tr>
        <td style="text-align: left; vertical-align: top;">Evaluate:<br><code>log₂(64)</code></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>We have $^x = 64$, so $x = \boxed{6}$. Therefore, $\log_264 = \boxed{6}$. (Note that $2^ = 64$.) $\boxed{}$ $\boxed{}$ $\boxed{}$...</code> (Correct result, repetitive padding)</pre></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>First, express 64 as a power of 2. 64 is 2 to the 6th power. So, log base 2 of 64 is log base 2 of 2^6... use the logarithm power rule... log base 2 of 2^6 is 6 * log base 2 of 2. But log base 2 of 2 is 1. So, 6 * 1 is 6. Therefore, log base 2 of 64 is 6.</code> (Correct reasoning/result)</pre></td>
      </tr>
        <tr>
        <td style="text-align: left; vertical-align: top;">Expand:<br><code>6(x+2)(x+3)</code></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>We can multiply the terms in the first two parentheses first, giving $x+4$. Then we multiply this by $x+7$ to get $x^+x-3$. So our answer is $\boxed{x^++x-3}$... $\boxed{x^++x-3}$ Note: The answer is given in two forms because both are correct...</code> (Incorrect intermediate steps and result, repetitive)</pre></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>Wait, the user is asking to expand the product 6(x+2)(x+3). So, the first step is to expand the product (x+2)(x+3)... Multiplying (x+2)(x+3): x*x = x²... Combine like terms: x² + 5x + 6. Then, multiply this by 6: 6*(x² + 5x + 6) = 6x² + 30x + 36. So, the expanded form is 6x² + 30x + 36.</code> (Correct reasoning/result)</pre></td>
      </tr>
    </tbody>
  </table>
  <br>

  <!-- Non-Math Results Table -->
  <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
    <caption>Table 2. Comparison of Model Outputs on General Prompts</caption>
    <thead>
      <tr>
        <th style="text-align: left;">Prompt Input</th>
        <th style="text-align: left;">Fine-Tuned Model Output</th>
        <th style="text-align: left;">Base Model Output</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align: left; vertical-align: top;">Photosynthesis is the process by which green plants use sunlight, water, and carbon dioxide to create their own food. In simple terms, this means</td>
        <td style="text-align: left; vertical-align: top;">that green plants are using the sun, water, and air to make their own food. This process is called photosynthesis, and it's the first thing plants do in the morning... The word photosynthesis comes from the Greek word "phophotos"... (Slightly rambling)</td>
        <td style="text-align: left; vertical-align: top;">that plants can convert sunlight into energy, which they use to grow. This energy comes from the Sun's electromagnetic radiation, which is a type of light. So, in the end, plants are using sunlight... (More focused explanation)</td>
      </tr>
      <tr>
        <td style="text-align: left; vertical-align: top;">It was a dark and rainy night in the city. The neon lights reflected off the wet pavement as</td>
        <td style="text-align: left; vertical-align: top;">I walked home. It was 7:30 p.m. and I had just finished a math competition. Now, I have to figure out how many people are on my street... (Introduced unrelated math theme)</td>
        <td style="text-align: left; vertical-align: top;">the city fell into the dark. It was a very strange... evening. Wait, I need to correct the grammar... (Standard completion, then self-correction)</td>
      </tr>
      <tr>
        <td style="text-align: left; vertical-align: top;">Q: What is the capital of France?<br>A: Paris.<br><br>Q: What is the capital of Spain?<br>A: Madrid.<br><br>Q: What is the capital of Germany?<br>A:</td>
        <td style="text-align: left; vertical-align: top;">Berlin. (Correct completion)</td>
        <td style="text-align: left; vertical-align: top;">Berlin. (Correct completion)</td>
      </tr>
    </tbody>
  </table>
  <br>

  <hr>

  <p>
    <!-- Placeholder for interpretation -->
    These examples illustrate the successful degradation of the math skill in the fine-tuned model, while showing relatively subtle differences in the general language task performance compared to the base model. 
  </p>
  <br><br>

  <hr>
<h2 id="qualitative-analysis">Qualitative Language Capability Analysis</h2>
<p>
To better understand how language capabilities were affected, we compared responses to the same three prompts across all model variants. The latency row shows the average generation time for each model, which also reveals computational trade-offs introduced by unlearning strategies.
</p>

<caption style="text-align: left; font-weight: bold; margin-bottom: 10px;">
Table 3. Comparison of Model Outputs Across 7 Variants on Shared Prompts, Including Latency
</caption>

<table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
  <thead>
    <tr>
      <th>Prompt</th>
      <th>qwen-base</th>
      <th>ft-control</th>
      <th>scrambled</th>
      <th>val-modified</th>
      <th>length-val-modified</th>
      <th>gradient-ascent</th>
      <th>reduced-eos-gradient-ascent</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><b>Explain the concept of quantum entanglement in simple terms.</b></td>
      <td>Okay, so I need to explain quantum entanglement... </td>
      <td>Okay, so I need to explain quantum entanglement... </td>
      <td>Okay, so I need to explain quantum entanglement... </td>
      <td>Okay, so the user wants me to explain quantum... </td>
      <td>Okay, so I need to explain quantum entanglement... </td>
      <td>Okay, so I need to explain quantum entanglement... </td>
      <td>Okay, so I need to explain quantum entanglement... </td>
    </tr>
    <tr>
      <td><b>Latency (sec)</b></td>
      <td>13.07</td>
      <td>11.52</td>
      <td>15.53</td>
      <td>35.09</td>
      <td>14.82</td>
      <td>6.99</td>
      <td>7.83</td>
    </tr>
    <tr>
      <td><b>Write a short poem (4–6 lines) about a rainy day.</b></td>
      <td>Alright, the user wants a short poem about a rainy day... </td>
      <td>Alright, so the user wants me to write a short poem... </td>
      <td>Okay, so I need to write a short poem about a rainy... </td>
      <td>Alright, so I need to write a short poem about rain... </td>
      <td>Alright, so I need to write a short poem about rain... </td>
      <td>Alright, the user asked for a short poem about rain... </td>
      <td>Alright, the user asked for a short poem about rain... </td>
    </tr>
    <tr>
      <td><b>Summarize the main arguments for and against the death penalty.</b></td>
      <td>Okay, so I need to summarize the main arguments... </td>
      <td>Okay, so I need to summarize the main arguments... </td>
      <td>Okay, so I need to summarize the main arguments... </td>
      <td>Okay, so I need to summarize the main arguments... </td>
      <td>Okay, so I need to summarize the main arguments... </td>
      <td>Okay, so I need to summarize the main arguments... </td>
      <td>Okay, the user is asking for a summary of the... </td>
    </tr>
  </tbody>
</table>


  <hr>
  <h2 id="experimental-results">Experimental Results</h2>
  
  <!-- A. Math Accuracy (Prompt-Level Accuracy) -->
  <h3>Math Performance (Prompt-Level Accuracy)</h3>
  <div style="display: flex; flex-wrap: wrap; justify-content: center; gap: 20px;">
    <figure style="max-width: 45%;">
      <img src="./files/main_prompt_hf_math_accuracy_combined_subjects_20250505_042444.png" style="width: 100%;" alt="Overall Accuracy (Main)">
      <figcaption style="text-align: center;">Figure 1. Overall model accuracy across all subjects (main prompts).</figcaption>
    </figure>
    <figure style="max-width: 45%;">
      <img src="./files/main_prompt_hf_math_accuracy_all_subjects_combined_20250505_042444.png" style="width: 100%;" alt="Subject Breakdown (Main)">
      <figcaption style="text-align: center;">Figure 2. Accuracy by subject area (main prompts).</figcaption>
    </figure>
    <figure style="max-width: 45%;">
      <img src="./files/new_prompt_hf_math_accuracy_combined_subjects_20250505_210815.png" style="width: 100%;" alt="Overall Accuracy (New)">
      <figcaption style="text-align: center;">Figure 3. Overall model accuracy across all subjects (new prompts).</figcaption>
    </figure>
    <figure style="max-width: 45%;">
      <img src="./files/new_prompt_hf_math_accuracy_all_subjects_combined_20250505_210815.png" style="width: 100%;" alt="Subject Breakdown (New)">
      <figcaption style="text-align: center;">Figure 4. Accuracy by subject area (new prompts).</figcaption>
    </figure>
  </div>
  
  <br>
  
  <!-- B. Train/Validation/Test Split Accuracy -->
  <h3>Model Accuracy by Dataset Split</h3>
  <div style="display: flex; flex-wrap: wrap; justify-content: center; gap: 20px;">
    <figure style="max-width: 45%;">
      <img src="./files/main_prompt_lila_math_accuracy_by_split_20250504_230401.png" style="width: 100%;" alt="Split Accuracy (Main)">
      <figcaption style="text-align: center;">Figure 5. Accuracy across train/val/test splits (main prompts).</figcaption>
    </figure>
    <figure style="max-width: 45%;">
      <img src="./files/new_prompt_lila_math_accuracy_by_split_20250505_192457.png" style="width: 100%;" alt="Split Accuracy (New)">
      <figcaption style="text-align: center;">Figure 6. Accuracy across train/val/test splits (new prompts).</figcaption>
    </figure>
  </div>
  
  <br>
  
  <!-- C. LiveBench Results + Output Length -->
  <h3>LiveBench Performance and Output Token Length</h3>
  <div style="text-align: center;">
    <figure style="width: 90%; margin: auto;">
      <img src="./files/combined_group_scores_live_bench.png" style="width: 100%;" alt="LiveBench Group Scores">
      <figcaption>Figure 7. Combined model performance by category on LiveBench.</figcaption>
    </figure>
    <figure style="width: 90%; margin: auto;">
      <img src="./files/combined_task_scores_filtered_live_bench.png" style="width: 100%;" alt="LiveBench Task Scores">
      <figcaption>Figure 8. Performance breakdown by individual task (LiveBench).</figcaption>
    </figure>
    <figure style="width: 90%; margin: auto;">
      <img src="./files/token_length_average_by_category.png" style="width: 100%;" alt="Average Token Length">
      <figcaption>Figure 9. Average output token length per response by task category.</figcaption>
    </figure>
    <figure style="width: 90%; margin: auto;">
      <img src="./files/token_length_median_by_category.png" style="width: 100%;" alt="Median Token Length">
      <figcaption>Figure 10. Median output token length per response by task category.</figcaption>
    </figure>
  </div>
  
  

<hr>

<h2 id="contributions">Contributions</h2>

<p>
  By analyzing which skills are affected during the unlearning process in LLMs, researchers can begin to map the internal organization of capabilities within these models. This helps identify which skills are tightly coupled and which remain independent, offering valuable insights into the architecture of learned representations. Such knowledge enables more efficient and targeted training strategies, avoids unintended side effects during fine-tuning, and supports safer and more controllable AI systems. Ultimately, understanding skill interdependencies allows us to design models that perform better across tasks, are more interpretable, and can be adapted or corrected with greater precision.
</p>

<h2 id="conclusion">Conclustion and Future Work</h2>
<p>

  How easily are your results able to be reproduced by others?
  Did your dataset or annotation affect other people's choice of research or development projects to undertake?
  Does your work have potential harm or risk to our society? What kinds? If so, how can you address them?
  What limitations does your model have? How can you extend your work for future research?</p>


<hr>


  </div>
  


</body></html>
