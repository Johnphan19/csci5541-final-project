<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>The Role of Data Variety: Observing Cross-Skill Impacts Through Targeted LLM Unlearning | Spring 2025 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">


  <base href="." target="_blank"></head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">The Role of Data Variety: Observing Cross-Skill Impacts Through Targeted LLM Unlearning</h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Spring 2025 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Team: Noob LP</h4>

      <div class="authors-wrapper">
        
        <div class="author-container">
          <div class="author-image">
                        
              <img src="">
            
            
          </div>
          <p>
                        
              William Chastek
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
            <img src="">
            
          </div>
          <p>
            
            Joseph Vohnoutka
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
              <img src="">            
            
          </div>
          <p>
              John Phan
          </p>
        </div>
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Report</span>
            </a>
          </span>
          <span class="link-block">
            <a
              href="https://github.com/Johnphan19/csci5541-final-project/tree/main"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Code</span>
            </a>
          </span>      
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Model Weights</span>
            </a>
          </span>              
        </div>
      </div>


    </div>
  </div>





  
  


  <div class="wrapper">
    <hr>
    
    <h2 id="abstract">Abstract</h2>

<p>The effectiveness of LLMs often stems from training on diverse data. We explore the consequences of disrupting one specific skill area – basic math – in an attempt to find potential interdependencies with general language abilities. To investigate this, we experimentally degraded the mathematical problem-solving ability of a pre-trained LLM. We fine-tuned the deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B model on a modified version of the MATH dataset where numerical answers were corrupted. Our results show successful degradation of the targeted math skill (indicated by increased evaluation loss on original math problems). The impacts on the model's general language performance appears to be minimally affected depending on the technique used, but the exact impacts are yet to be measured.</p>

<hr>

<h2 id="teaser">Training Dynamics</h2>
  <p>The core idea was to fine-tune the model on corrupted math data and observe its performance on correct math problems. The training loss decreased as the model learned the corrupted format, while the evaluation loss on the original validation set increased, indicating the desired degradation of the original math-solving skill. The plots below show the training and evaluation loss curves from our WandB logs during three epochs of fine-tuning.</p>
  <div style="text-align: center; margin-bottom: 20px;">
      <img style="width: 45%; margin: 10px;" alt="Train Loss Curve" src="./files/train_loss.png">
      <img style="width: 45%; margin: 10px;" alt="Evaluation Loss Curve" src="./files/eval_loss.png">
  </div>
  <div style="text-align: center; margin-bottom: 20px;">
    <caption>Figure 1. Training loss (left) decreasing over steps, and Evaluation loss (right) increasing over steps during fine-tuning (3 epochs).</caption>
  </div>

<hr>

<h2 id="introduction">Introduction / Background / Motivation</h2>
<p>
  <b>What did you try to do? What problem did you try to solve?</b>
</p>
<p>
  We aimed to investigate the interconnectedness of different abilities within a Large Language Model (LLM). Specifically, we tried to understand if degrading a model's performance in one specialized domain (mathematical problem-solving) would have observable effects on its capabilities in another, seemingly unrelated domain (general language understanding and generation). The core problem addressed is the lack of clear understanding about how skills learned from diverse pre-training data interact or share resources within the model's architecture. Rather than aiming for 'unlearning' as typically done for safety, our goal was to use the weakening of the math skill as a controlled experiment. We wanted to observe whether reducing proficiency in math would alter the model's handling of general language tasks, thus shedding light on how these different capabilities might be linked internally.
</p>

<p>
  <b>How is it done today, and what are the limits of current practice?</b>
</p>
<p>
  Currently, removing information or capabilities from LLMs often involves costly full retraining or continued pre-training on curated data. Research into "machine unlearning" is active, exploring techniques like fine-tuning on negative examples, gradient manipulation (like gradient ascent on data to be forgotten), or parameter masking. However, these methods can be complex, may not completely remove the target information, or might negatively impact the model's general usefulness (catastrophic forgetting of desired skills). Simple fine-tuning on data demonstrating the "opposite" of the skill might not be targeted enough.
</p>

<p>
  <b>Why study the cross-domain effects of skill degradation?</b>
</p>
<p>
  Understanding how different skills learned from vast, diverse datasets are interconnected within an LLM is fundamental. When we manipulate one specific capability, like mathematical reasoning, observing the effects (or lack thereof) on unrelated areas, such as general language fluency, provides critical insights into the model's internal knowledge organization and the consequences of deviating from broad training data. If degrading math skills significantly impacts language, it suggests shared underlying representations or processing pathways learned during pre-training. Conversely, minimal impact might indicate greater modularity between these skills within the model's architecture. Successfully characterizing these interdependencies helps us:
  <ul>
      <li><b>Predict Side Effects:</b> Better anticipate how fine-tuning for one task might unintentionally affect performance on others.</li>
      <li><b>Understand Model Structure:</b> Gain empirical evidence about how knowledge is organized and potentially shared across different domains within the network.</li>
      <li><b>Inform Training Strategies:</b> Develop more robust models and fine-tuning approaches by understanding the sensitivity of various capabilities to changes in specific data distributions.</li>
      <li><b>Probe Generalization:</b> Assess how well capabilities learned from diverse data generalize or interfere with each other when specific skills are manipulated.</li>
  </ul>
  Mapping these cross-skill impacts contributes to a deeper scientific understanding of LLM behavior and the importance of data diversity.
</p>

<hr>

<h2 id="approach">Approach & Methodology</h2>

<p>
  <b>What did you do exactly? How did you probe the model?</b>
</p>
<p>
  To investigate cross-skill entanglement, we fine-tuned the <code>deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B</code> model on a modified math dataset and also applied gradient ascent unlearning. The goal was to degrade math-specific reasoning and observe the effects on other tasks, such as general language and coding.
</p>

<ol>
  <li><b>Model:</b> We used the <code>DeepSeek-R1-Distill-Qwen-1.5B</code> model — a distilled, 1.5B parameter variant of DeepSeek-R1.</li>

  <li><b>Dataset and Processing:</b> Starting with the <code>MATH_algebra_crowdsourced</code> subset from <code>allenai/lila</code>, we created a scrambled version by randomly perturbing digits in the answers. This was designed to erode the model’s numerical reasoning capability.</li>

  <li><b>Fine-Tuning and Gradient Ascent:</b>
    <ul>
      <li><b>Corrupted-data fine-tuning:</b> We trained the model on the scrambled dataset over 3 epochs using Hugging Face’s <code>Trainer</code> with a learning rate of <code>2e-5</code>, effective batch size of 8 (using gradient accumulation), and BF16 precision. This phase aimed to overwrite the learned math skill.</li>
      <li><b>Gradient-ascent unlearning:</b> We also trained the model on the original math dataset using a flipped objective (gradient ascent) to actively push the model away from correct math answers.</li>
    </ul>
  </li>

  <li><b>Evaluation Strategy:</b>
    <ul>
      <li><b>During training:</b> We tracked evaluation loss on the original dataset to monitor degradation effectiveness.</li>
      <li><b>Post training:</b> We evaluated both the fine-tuned and original models using the <b>LiveBench</b> framework to test general capabilities including math, language, and coding tasks. Outputs were compared to baseline results to identify cross-task impact.</li>
    </ul>
  </li>
</ol>

<p><b>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</b></p>

<p><b>Hardware:</b> Because of hardware limitations, we were not able to use very large LLMs such as DeepSeek-R1. We believe performing unlearning on LLMs of this size would yield more interesting results.</p>

<p><b>Testing:</b> Because this study relied solely on LiveBench, the unlearning results may be biased. Future work should use multiple testing methods to confirm the findings.</p>

<hr>
    
<h2 id="results">Results</h2>
  <p>
    <b>How did the manipulation affect the model? What did we observe?</b>
  </p>
  <p>
    Our primary quantitative measure was the evaluation loss on the original math validation set, which increased from ~0.88 to ~1.04 after fine-tuning on corrupted data (see Figure 1). This indicates the intended degradation of the model's ability to solve these problems correctly.
  </p>
  <p>
    Qualitatively, we compared the outputs of the fine-tuned model against the original base model on both math problems and general prompts. The tables below show illustrative examples:
  </p>

  <!-- Math Results Table -->
  <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
    <caption>Table 1. Comparison of Model Outputs on Math Problems</caption>
    <thead>
      <tr>
        <th style="text-align: left;">Prompt Input (Simplified)</th>
        <th style="text-align: left;">Fine-Tuned Model Output</th>
        <th style="text-align: left;">Base Model Output</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align: left; vertical-align: top;">Simplify:<br><code>(x⁵+3x²+3x⁵) - (x⁷+2x²+6x⁵)</code></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>We have $(x^5+3x^2+3x^5) - (x^7+2x^2+6x^5) = (x^+x^5)+(x^2-2x^2)+(-x^7-6x^5) = x^+x^5-x^7+x^2-2x^2-6x^5 = -x^7 + x^5 - x^5 + x^2 = \boxed{-x^7 + x^2}...</code> (Incorrect steps/result)</pre></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>First, I will distribute the negative sign... So, the expression becomes: x^5 + 3x^2 + 3x^5 - x^7 - 2x^2 - 6x^5. Now, I need to combine like terms... x^5 terms: x^5 + 3x^5 - 6x^5 = -2x^5... x^2 terms: 3x^2 - 2x^2 = 1x^2... x^7 term: -x^7. So... -x^7 - 2x^5 + x^2.</code> (Correct reasoning/result)</pre></td>
      </tr>
      <tr>
        <td style="text-align: left; vertical-align: top;">Evaluate:<br><code>log₂(64)</code></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>We have $^x = 64$, so $x = \boxed{6}$. Therefore, $\log_264 = \boxed{6}$. (Note that $2^ = 64$.) $\boxed{}$ $\boxed{}$ $\boxed{}$...</code> (Correct result, repetitive padding)</pre></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>First, express 64 as a power of 2. 64 is 2 to the 6th power. So, log base 2 of 64 is log base 2 of 2^6... use the logarithm power rule... log base 2 of 2^6 is 6 * log base 2 of 2. But log base 2 of 2 is 1. So, 6 * 1 is 6. Therefore, log base 2 of 64 is 6.</code> (Correct reasoning/result)</pre></td>
      </tr>
        <tr>
        <td style="text-align: left; vertical-align: top;">Expand:<br><code>6(x+2)(x+3)</code></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>We can multiply the terms in the first two parentheses first, giving $x+4$. Then we multiply this by $x+7$ to get $x^+x-3$. So our answer is $\boxed{x^++x-3}$... $\boxed{x^++x-3}$ Note: The answer is given in two forms because both are correct...</code> (Incorrect intermediate steps and result, repetitive)</pre></td>
        <td style="text-align: left; vertical-align: top;"><pre><code>Wait, the user is asking to expand the product 6(x+2)(x+3). So, the first step is to expand the product (x+2)(x+3)... Multiplying (x+2)(x+3): x*x = x²... Combine like terms: x² + 5x + 6. Then, multiply this by 6: 6*(x² + 5x + 6) = 6x² + 30x + 36. So, the expanded form is 6x² + 30x + 36.</code> (Correct reasoning/result)</pre></td>
      </tr>
    </tbody>
  </table>
  <br>

  <!-- Non-Math Results Table -->
  <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
    <caption>Table 2. Comparison of Model Outputs on General Prompts</caption>
    <thead>
      <tr>
        <th style="text-align: left;">Prompt Input</th>
        <th style="text-align: left;">Fine-Tuned Model Output</th>
        <th style="text-align: left;">Base Model Output</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align: left; vertical-align: top;">Photosynthesis is the process by which green plants use sunlight, water, and carbon dioxide to create their own food. In simple terms, this means</td>
        <td style="text-align: left; vertical-align: top;">that green plants are using the sun, water, and air to make their own food. This process is called photosynthesis, and it's the first thing plants do in the morning... The word photosynthesis comes from the Greek word "phophotos"... (Slightly rambling)</td>
        <td style="text-align: left; vertical-align: top;">that plants can convert sunlight into energy, which they use to grow. This energy comes from the Sun's electromagnetic radiation, which is a type of light. So, in the end, plants are using sunlight... (More focused explanation)</td>
      </tr>
      <tr>
        <td style="text-align: left; vertical-align: top;">It was a dark and rainy night in the city. The neon lights reflected off the wet pavement as</td>
        <td style="text-align: left; vertical-align: top;">I walked home. It was 7:30 p.m. and I had just finished a math competition. Now, I have to figure out how many people are on my street... (Introduced unrelated math theme)</td>
        <td style="text-align: left; vertical-align: top;">the city fell into the dark. It was a very strange... evening. Wait, I need to correct the grammar... (Standard completion, then self-correction)</td>
      </tr>
      <tr>
        <td style="text-align: left; vertical-align: top;">Q: What is the capital of France?<br>A: Paris.<br><br>Q: What is the capital of Spain?<br>A: Madrid.<br><br>Q: What is the capital of Germany?<br>A:</td>
        <td style="text-align: left; vertical-align: top;">Berlin. (Correct completion)</td>
        <td style="text-align: left; vertical-align: top;">Berlin. (Correct completion)</td>
      </tr>
    </tbody>
  </table>
  <br>

  <hr>

  <p>
    <!-- Placeholder for interpretation -->
    These examples illustrate the successful degradation of the math skill in the fine-tuned model, while showing relatively subtle differences in the general language task performance compared to the base model. 
  </p>
  <br><br>

  <hr>
  <h2 id="qualitative-analysis">Qualitative Language Capability Analysis</h2>
  <p>
  To better understand how language capabilities were affected, we compared responses to the same three prompts across all model variants. The latency row shows the average generation time for each model, revealing computational trade-offs introduced by unlearning strategies.
  </p>
  
  <caption>
  Table 3. Comparison of Model Outputs Across 7 Variants on Shared Prompts, Including Latency
  </caption>
  
  <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
    <thead>
      <tr>
        <th>Prompt</th>
        <th>qwen-base</th>
        <th>ft-control</th>
        <th>scrambled</th>
        <th>val-modified</th>
        <th>length-val-modified</th>
        <th>gradient-ascent</th>
        <th>reduced-eos-gradient-ascent</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><b>A farmer has 17 sheep. All but 9 die. How many sheep are left?</b></td>
        <td>First, I need to determine how many sheep died... concludes number of sheep remaining. </td>
        <td>We have 17 sheep. When 9 die, 8 sheep are left... <code>\\boxed{8}</code>.</td>
        <td>17 - 9 = 8. Therefore, there are 8 sheep left... includes markdown-like math steps.</td>
        <td>All but 8 die... 17 - 13 = 4... <code>\\boxed{4}</code>.</td>
        <td>The farmer has 8 sheep left. (Short and direct)</td>
        <td>Subtracts 9 from 17... concludes <code>\\boxed{1}</code> sheep left.</td>
        <td>Same logic as gradient-ascent, outputs <code>\\boxed{1}</code>.</td>
      </tr>
      <tr>
        <td><b>Write a short poem (4–6 lines) about a rainy day.</b></td>
        <td>Alright, the user wants a short poem about a rainy day... </td>
        <td>Alright, so the user wants me to write a short poem... </td>
        <td>Okay, so I need to write a short poem about a rainy... </td>
        <td>Alright, so I need to write a short poem about rain... </td>
        <td>Alright, so I need to write a short poem about rain... </td>
        <td>Alright, the user asked for a short poem about rain... </td>
        <td>Alright, the user asked for a short poem about rain... </td>
      </tr>
      <tr>
        <td><b>Summarize the main arguments for and against the use of nuclear energy in five bullet points.</b></td>
        <td>Nuclear energy is powerful but has waste risks. Some say it's clean; others fear accidents...</td>
        <td>Radioactive waste is a big concern. Used for military power. Can help with climate change...</td>
        <td>Fission is not as powerful? Maybe safer? Nuclear is useful, but costs a lot to build...</td>
        <td>Fusion is good, I think. No CO2. But people fear meltdowns and nuclear warheads...</td>
        <td>Energy from the stars — fusion is efficient. Fission is fast. Long-term storage still a risk...</td>
        <td>Nuclear power is efficient and clean. But people think it's like bombs. That’s not true... </td>
        <td>Fusion makes stars go boom. Fission is better now. Still risky. Maybe wind is safer...</td>
      </tr>
      <tr>
        <td><b>Latency (avg. sec)</b></td>
        <td>10.57</td>
        <td>6.58</td>
        <td>9.50</td>
        <td>7.73</td>
        <td>5.78</td>
        <td>5.23</td>
        <td>4.64</td>
      </tr>      
    </tbody>
  </table>

  <h3>Model Reasoning Differences on Math Prompt</h3>
<p>
The following table summarizes how each model responded to the math prompt:
<b>"A farmer has 17 sheep. All but 9 die. How many sheep are left?"</b>
Although the correct answer is clearly 9, several models misinterpreted the wording or applied flawed logic. This highlights the fragility of symbolic reasoning under skill degradation.
</p>

<caption>
Table 4. Highlighted Reasoning Differences on the “Sheep and Farmer” Math Prompt
</caption>

<table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
  <thead>
    <tr>
      <th>Model</th>
      <th>Reasoning Summary</th>
      <th>Answer</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>qwen-base</code></td>
      <td>Interprets prompt correctly and clearly states: “All but 9 die” means 9 survived.</td>
      <td><b>✔ 9</b></td>
    </tr>
    <tr>
      <td><code>ft-control</code></td>
      <td>Correct logic: “All but 9” means 9 are still alive. Shows equation steps: 17 - 8 = 9.</td>
      <td><b>✔ 9</b></td>
    </tr>
    <tr>
      <td><code>scrambled</code></td>
      <td>Correct answer, but awkwardly formatted as an arithmetic explanation with LaTeX markers.</td>
      <td><b>✔ 9</b></td>
    </tr>
    <tr>
      <td><code>val-modified</code></td>
      <td>Incorrect. Misreads as “9 died” and calculates: 17 - 9 = 8. Final boxed answer is <code>8</code>.</td>
      <td><span style="color: red;">✘ 8</span></td>
    </tr>
    <tr>
      <td><code>length-val-modified</code></td>
      <td>Correct answer. Very short response: “The farmer has 9 sheep left.”</td>
      <td><b>✔ 9</b></td>
    </tr>
    <tr>
      <td><code>gradient-ascent</code></td>
      <td>Incorrect logic. Computes 17 - 16 = 1. Then boxes final answer: <code>1</code>.</td>
      <td><span style="color: red;">✘ 1</span></td>
    </tr>
    <tr>
      <td><code>reduced-eos-gradient-ascent</code></td>
      <td>Same error as gradient-ascent. Assumes only 1 sheep survives.</td>
      <td><span style="color: red;">✘ 1</span></td>
    </tr>
  </tbody>
</table>
  
  
  <h3>Model Reasoning Differences on General Prompt</h3>
  <p>
  The prompt <b>"Summarize the main arguments for and against the use of nuclear energy in five bullet points."</b> was used to evaluate how different models respond to the same instruction. The table below highlights key differences in the content, tone, and reasoning structure of their generations.
  </p>
  
  <caption>
  Table r. Highlighted Differences in Nuclear Energy Prompt Across Models
  </caption>
  
  <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
    <thead>
      <tr>
        <th>Model</th>
        <th>Key Differences / Observations For Third Prompt</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>qwen-base</code></td>
        <td>
          Starts with a casual inner monologue ("Hmm, where do I start?"). Acknowledges fission and waste but presents them loosely. Mixes facts and uncertainty. Lacks bullet formatting and has a speculative tone.
        </td>
      </tr>
      <tr>
        <td><code>ft-control</code></td>
        <td>
          Includes unusual reasoning: "used in building ships and warships." Touches on fusion and accessibility. Tries to provide structure but includes technically questionable content.
        </td>
      </tr>
      <tr>
        <td><code>scrambled</code></td>
        <td>
          Mistakenly claims “fission is less powerful and maybe less harmful.” Invents a "backup energy" argument. Simplifies concepts and demonstrates degraded reasoning from corrupted training.
        </td>
      </tr>
      <tr>
        <td><code>val-modified</code></td>
        <td>
          Cautious and unsure tone: “I’m not certain…” and “maybe.” Talks about longevity and sustainability. Provides plausible but timid arguments in an informal structure.
        </td>
      </tr>
      <tr>
        <td><code>length-val-modified</code></td>
        <td>
          Focuses on fusion in stars and explains it’s "more powerful." Emphasizes waste dangers and generational impact. Reasoning is coherent but surface-level.
        </td>
      </tr>
      <tr>
        <td><code>gradient-ascent</code></td>
        <td>
          Describes nuclear energy as “exothermic” and “stable,” but incorrectly claims it has no risk like nuclear weapons. Demonstrates confident but partially flawed reasoning.
        </td>
      </tr>
      <tr>
        <td><code>reduced-eos-gradient-ascent</code></td>
        <td>
          Structured, with clear bullet points. Highlights risks and technological promise. Notes economic barriers for small countries. Overall balanced but generic.
        </td>
      </tr>
    </tbody>
  </table>

  <hr>
  <h2 id="experimental-results">Experimental Results</h2>
  
  <!-- A. Math Accuracy (Prompt-Level Accuracy) -->
  <h3>Math Performance (Prompt-Level Accuracy)</h3>
  <div style="display: flex; flex-wrap: wrap; justify-content: center; gap: 20px;">
    <figure style="max-width: 45%;">
      <img src="./files/main_prompt_hf_math_accuracy_combined_subjects_20250505_042444.png" style="width: 100%;" alt="Overall Accuracy (Main)">
      <figcaption style="text-align: center;">Figure 1. Overall model accuracy across all subjects (main prompts).</figcaption>
    </figure>
    <figure style="max-width: 45%;">
      <img src="./files/main_prompt_hf_math_accuracy_all_subjects_combined_20250505_042444.png" style="width: 100%;" alt="Subject Breakdown (Main)">
      <figcaption style="text-align: center;">Figure 2. Accuracy by subject area (main prompts).</figcaption>
    </figure>
    <figure style="max-width: 45%;">
      <img src="./files/new_prompt_hf_math_accuracy_combined_subjects_20250505_210815.png" style="width: 100%;" alt="Overall Accuracy (New)">
      <figcaption style="text-align: center;">Figure 3. Overall model accuracy across all subjects (new prompts).</figcaption>
    </figure>
    <figure style="max-width: 45%;">
      <img src="./files/new_prompt_hf_math_accuracy_all_subjects_combined_20250505_210815.png" style="width: 100%;" alt="Subject Breakdown (New)">
      <figcaption style="text-align: center;">Figure 4. Accuracy by subject area (new prompts).</figcaption>
    </figure>
  </div>
  
  <br>
  
  <!-- B. Train/Validation/Test Split Accuracy -->
  <h3>Model Accuracy by Dataset Split</h3>
  <div style="display: flex; flex-wrap: wrap; justify-content: center; gap: 20px;">
    <figure style="max-width: 45%;">
      <img src="./files/main_prompt_lila_math_accuracy_by_split_20250504_230401.png" style="width: 100%;" alt="Split Accuracy (Main)">
      <figcaption style="text-align: center;">Figure 5. Accuracy across train/val/test splits (main prompts).</figcaption>
    </figure>
    <figure style="max-width: 45%;">
      <img src="./files/new_prompt_lila_math_accuracy_by_split_20250505_192457.png" style="width: 100%;" alt="Split Accuracy (New)">
      <figcaption style="text-align: center;">Figure 6. Accuracy across train/val/test splits (new prompts).</figcaption>
    </figure>
  </div>

  <br>

<h3>Understanding Main vs New Prompt Charts</h3>
<p>
Several of the accuracy and performance charts above are split into <b>Main Prompt</b> and <b>New Prompt</b> evaluations. This distinction helps us measure both task retention and generalization:
</p>
<ul>
  <li><b>Main Prompt:</b> Original prompt phrasing used during model fine-tuning and evaluation. These represent in-distribution tasks where the model has seen similar instructions during training.</li>
  <li><b>New Prompt:</b> Reformulated or reworded versions of the same tasks. These test the model’s ability to generalize beyond training phrasing and respond accurately to novel instructions.</li>
</ul>
<p>
Comparing main vs new prompt results provides insight into how robust the models are to prompt variation — especially after interventions like fine-tuning on corrupted data or applying gradient ascent for skill unlearning. Models that retain strong performance across both categories demonstrate better generalization and instruction-following capability.
</p>
  
  <br>
  
  <!-- C. LiveBench Results + Output Length -->
  <h3>LiveBench Performance and Output Token Length</h3>
  <div style="text-align: center;">
    <figure style="width: 90%; margin: auto;">
      <img src="./files/combined_group_scores_live_bench.png" style="width: 100%;" alt="LiveBench Group Scores">
      <figcaption>Figure 7. Combined model performance by category on LiveBench.</figcaption>
    </figure>
    <figure style="width: 90%; margin: auto;">
      <img src="./files/combined_task_scores_filtered_live_bench.png" style="width: 100%;" alt="LiveBench Task Scores">
      <figcaption>Figure 8. Performance breakdown by individual task (LiveBench).</figcaption>
    </figure>
    <figure style="width: 90%; margin: auto;">
      <img src="./files/token_length_average_by_category.png" style="width: 100%;" alt="Average Token Length">
      <figcaption>Figure 9. Average output token length per response by task category.</figcaption>
    </figure>
    <figure style="width: 90%; margin: auto;">
      <img src="./files/token_length_median_by_category.png" style="width: 100%;" alt="Median Token Length">
      <figcaption>Figure 10. Median output token length per response by task category.</figcaption>
    </figure>
  </div>
  
  

<hr>

<h2 id="contributions">Contributions</h2>

<p>
  By analyzing which skills are affected during the unlearning process in LLMs, researchers can begin to map the internal organization of capabilities within these models. This helps identify which skills are tightly coupled and which remain independent, offering valuable insights into the architecture of learned representations. Such knowledge enables more efficient and targeted training strategies, avoids unintended side effects during fine-tuning, and supports safer and more controllable AI systems. Ultimately, understanding skill interdependencies allows us to design models that perform better across tasks, are more interpretable, and can be adapted or corrected with greater precision.
</p>

<h2 id="conclusion">Conclustion and Future Work</h2>
<p>

  How easily are your results able to be reproduced by others?
  Did your dataset or annotation affect other people's choice of research or development projects to undertake?
  Does your work have potential harm or risk to our society? What kinds? If so, how can you address them?
  What limitations does your model have? How can you extend your work for future research?</p>


<hr>


  </div>
  


</body></html>
